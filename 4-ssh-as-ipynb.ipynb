{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Collaborative Filtering\n",
    "This section contains the implementation of the NCF models.\n",
    "\n",
    "We start by imporing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>> IMPORTS\n",
    "import os\n",
    "import torch\n",
    "import importlib\n",
    "import functools\n",
    "import logging\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import Tuple, Any, Iterable, Union\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import utils.metrics as metric\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "importlib.reload(metric)  # To caputre changes in metrics module\n",
    "matplotlib.use(\"Agg\")  # Backend for SSH runs\n",
    "\n",
    "\n",
    "# <<<<< IMPORTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "We start by implementing necessary helper functions used in data cleaning and pre-processing stage of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>> PREPROCESSING\n",
    "def load_min_data(filename: Union[str, Iterable]):\n",
    "    \"\"\"Helper to load minimzed datasets with only 2000 samples, for testing\"\"\"\n",
    "    dfs = []\n",
    "    if isinstance(filename, str):\n",
    "        filename = [filename]\n",
    "    for fn in filename:\n",
    "        df = pd.read_csv(fn)\n",
    "        # All min-datasets have an index column which has to be dropped:\n",
    "        dfs.append(df.drop(df.columns[0], axis=1))\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def clean_customer_data(df: pd.DataFrame):\n",
    "    \"\"\"Helper to remove NAs from Age and use of consistent labeling\"\"\"\n",
    "    df = df.dropna(subset=[\"age\"])\n",
    "    df.loc[\n",
    "        ~df[\"fashion_news_frequency\"].isin([\"Regularly\", \"Monthly\"]),\n",
    "        \"fashion_news_frequency\",\n",
    "    ] = \"None\"\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_article_data(df):\n",
    "    \"\"\"Helper that maps index group number `26' to 0 for easier\n",
    "    label encoding\"\"\"\n",
    "    df.loc[df[\"index_group_no\"] == 26, \"index_group_no\"] = 0\n",
    "    return df\n",
    "\n",
    "\n",
    "# <<<<< PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "Now that pre-processing methods are defined, we can establish the main class containing the datasets. The main class, `Data_HM`, contains all necessary info regarding positive and negative samples, DataLoaders and training/validation sets.\n",
    "\n",
    "Below is a child class inherited from `Data_HM`. This specifically is used for generating an equivalent instance with a modified negative sampling algorithm with considerably lower run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>> DATA LOADING\n",
    "\n",
    "\n",
    "class Data_HM(Dataset):\n",
    "    \"\"\"This is the general HM Dataset class whose children are train-dataset\n",
    "    and validation-dataset\n",
    "\n",
    "    Args:\n",
    "        Dataset: Abstract Dataset class from pyTorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        total_cases: int,\n",
    "        portion_negatives: float,\n",
    "        df_transactions: pd.DataFrame,\n",
    "        batch_size: int,\n",
    "        train_portion: Union[float, None] = None,\n",
    "        test_portion: Union[float, None] = None,\n",
    "        transform: Any = None,\n",
    "        target_transform: Any = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if train_portion is None:\n",
    "            if test_portion is None:\n",
    "                raise ValueError(\n",
    "                    \"Both train portion and test portion cannot be None.\"\n",
    "                    )\n",
    "            self.train_portion = 1 - test_portion\n",
    "        self.batch_size = batch_size\n",
    "        self.df_id, self.le_cust, self.le_art = self.generate_dataset(\n",
    "            total_cases, portion_negatives, df_transactions\n",
    "        )\n",
    "        self.train_portion = train_portion\n",
    "        self.train, self.val = self.split_dataset()\n",
    "        self.transform, self.target_transform = transform, target_transform\n",
    "\n",
    "    def generate_dataset(\n",
    "        self, total_cases: int, portion_negatives: float,\n",
    "        df_transactions: pd.DataFrame\n",
    "    ) -> Tuple[pd.DataFrame, LabelEncoder, LabelEncoder]:\n",
    "        \"\"\"Produce DataFrames for positive labels and generated negatives\n",
    "\n",
    "        Args:\n",
    "            total_cases (int): Total number of transactions\n",
    "            portion_negatives (float): The portion of the `total_cases` that\n",
    "                                    should be negative. Balanced 0/1 when 0.5\n",
    "            df_transactions (pd.DataFrame): Transactions to pull\n",
    "                                    samples/generate samples from\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, LabelEncoder, LabelEncoder]:\n",
    "                * DataFrame on form (customer, article, label) where\n",
    "                    the IDs are label-encoded,\n",
    "                * Customer and article LabelEncoder objects\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            0 <= portion_negatives <= 1\n",
    "        ), r\"portion negatives must be a float between 0%=0.0 and 100%=1.0!\"\n",
    "        if total_cases is None:\n",
    "            # Defaults to using all cases\n",
    "            total_cases = len(df_transactions)\n",
    "        n_positive = round(total_cases * (1 - portion_negatives))\n",
    "        n_negative = total_cases - n_positive\n",
    "\n",
    "        df_positive = (\n",
    "            df_transactions\n",
    "            .sample(n=n_positive)\n",
    "            .reset_index(drop=True)\n",
    "            )\n",
    "        df_positive = df_positive[[\"customer_id\", \"article_id\"]]\n",
    "        df_positive[\"label\"] = 1\n",
    "\n",
    "        # Sampling negative labels:\n",
    "        #   We select a random combination of `customer_id`, `article_id`,\n",
    "        #   and ensure that this is not a true transaction.\n",
    "        #   Then we make a 2-column dataframe on same form as `df_positive`\n",
    "\n",
    "        df_np = df_transactions[[\"customer_id\", \"article_id\"]].to_numpy()\n",
    "        neg_np = np.empty((n_negative, df_np.shape[1]), dtype=\"<U64\")\n",
    "        for i in range(n_negative):\n",
    "            legit = False\n",
    "            while not legit:\n",
    "                sample = [\n",
    "                    np.random.choice(df_np[:, col]) for col in\n",
    "                    range(df_np.shape[1])\n",
    "                ]\n",
    "                # Checking if random sample actually exists:\n",
    "                legit = not (\n",
    "                    (df_np[:, 0] == sample[0]) & (df_np[:, 1] == sample[1])\n",
    "                ).any()\n",
    "            neg_np[i, :] = sample\n",
    "        neg_np = np.column_stack((neg_np, [0] * neg_np.shape[0]))\n",
    "        df_negative = pd.DataFrame(neg_np, columns=df_positive.columns)\n",
    "        # Return a shuffled concatenation of the two dataframes\n",
    "        full_data = (\n",
    "            pd.concat((df_positive, df_negative))\n",
    "            .sample(frac=1)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        # Make label encodings of the IDs\n",
    "        le_cust = LabelEncoder()\n",
    "        le_art = LabelEncoder()\n",
    "        le_cust.fit(full_data[\"customer_id\"])\n",
    "        le_art.fit(full_data[\"article_id\"])\n",
    "        cust_encode = le_cust.transform(full_data[\"customer_id\"])\n",
    "        art_encode = le_art.transform(full_data[\"article_id\"])\n",
    "        return (\n",
    "            pd.DataFrame(\n",
    "                data={\n",
    "                    \"customer_id\": cust_encode,\n",
    "                    \"article_id\": art_encode,\n",
    "                    \"label\": full_data[\"label\"].astype(np.uint8),\n",
    "                }\n",
    "            ),\n",
    "            le_cust,\n",
    "            le_art,\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_id.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row, label = self.df_id.iloc[idx, :-1].values, self.df_id.iloc[idx, -1]\n",
    "        label = int(label)  # Stored as str initially\n",
    "        if self.transform:\n",
    "            row = self.transform(row)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return row, label\n",
    "\n",
    "    def split_dataset(self):\n",
    "        \"\"\"Split full data to train and validation Subset-objects\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Subset, Subset]: Train and validation subsets\n",
    "        \"\"\"\n",
    "        length = len(self)\n",
    "        train_size = int(length * self.train_portion)\n",
    "        valid_size = length - train_size\n",
    "        train, val = torch.utils.data.random_split(\n",
    "            self, [train_size, valid_size]\n",
    "            )\n",
    "        return train, val\n",
    "\n",
    "    def get_data_from_subset(self, subset: torch.utils.data.Subset):\n",
    "        \"\"\"Retrieve data from Subset object directly\"\"\"\n",
    "        return subset.dataset.df_id.iloc[subset.indices]\n",
    "\n",
    "    def get_DataLoader(self, trainDL: bool = True):\n",
    "        \"\"\"Retrieve DataLoader object for either train or validation set.\n",
    "\n",
    "        Args:\n",
    "            trainDL (bool, optional): Flag to determine which set to load.\n",
    "            Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: Torch DataLoader\n",
    "        \"\"\"\n",
    "        subset = self.train if trainDL else self.val\n",
    "        return DataLoader(dataset=subset, batch_size=self.batch_size)\n",
    "\n",
    "\n",
    "class Data_HM_Complete(Data_HM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        total_cases: int,\n",
    "        portion_negatives: float,\n",
    "        df_transactions: pd.DataFrame,\n",
    "        batch_size: int,\n",
    "        train_portion: Union[float, None] = None,\n",
    "        test_portion: Union[float, None] = None,\n",
    "        transform: Any = None,\n",
    "        target_transform: Any = None,\n",
    "        random_seed: int = None,\n",
    "    ) -> None:\n",
    "        self.random_seed = random_seed\n",
    "        super().__init__(\n",
    "            total_cases,\n",
    "            portion_negatives,\n",
    "            df_transactions,\n",
    "            batch_size,\n",
    "            train_portion,\n",
    "            test_portion,\n",
    "            transform,\n",
    "            target_transform,\n",
    "        )\n",
    "\n",
    "    def generate_dataset(\n",
    "        self, total_cases: int, portion_negatives: float,\n",
    "        df_transactions: pd.DataFrame\n",
    "    ) -> Tuple[pd.DataFrame, LabelEncoder, LabelEncoder]:\n",
    "        logging.debug(\"Entered child `generate_dataset` method (expected)\")\n",
    "        # NB: Overrides parent method but returns the same info ...\n",
    "        if self.random_seed is not None:\n",
    "            np.random.seed(self.random_seed)\n",
    "            logging.debug(f\"Seed set to {self.random_seed}\")\n",
    "        df_transactions = df_transactions[\n",
    "            [\"customer_id\", \"article_id\"]\n",
    "        ]  # This object has the same behavior as df_positive\n",
    "        customers = df_transactions[\"customer_id\"].unique()\n",
    "        articles = df_transactions[\"article_id\"].unique()\n",
    "        n_positive = round(total_cases * (1 - portion_negatives))\n",
    "        n_negative = total_cases - n_positive  # mu inferred implicitly\n",
    "\n",
    "        logging.debug(f\"Sampling {n_positive} positives from dataframe\")\n",
    "        # Lazy evalution: if we have to sample 99% of the data,\n",
    "        # we just take everything\n",
    "        if n_positive < 0.99 * len(df_transactions):\n",
    "            positive_samples = df_transactions.sample(\n",
    "                n=n_positive, replace=False\n",
    "                )\n",
    "        else:\n",
    "            positive_samples = df_transactions\n",
    "\n",
    "        logging.debug(f\"Generating {n_negative} random negative samples\")\n",
    "        samples = pd.DataFrame(\n",
    "            {\n",
    "                \"customer_id\": np.random.choice(\n",
    "                    customers, size=n_negative, replace=True\n",
    "                ),\n",
    "                \"article_id\": np.random.choice(\n",
    "                    articles, size=n_negative, replace=True\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "        logging.debug(\"Merging samples in order to remove false negatives\")\n",
    "        samples = pd.merge(\n",
    "            samples,\n",
    "            df_transactions,\n",
    "            on=[\"customer_id\", \"article_id\"],\n",
    "            how=\"outer\",\n",
    "            indicator=True,\n",
    "        )\n",
    "        logging.debug(\"Indexing out false negatives\")\n",
    "        samples = samples.loc[samples[\"_merge\"] == \"left_only\"].drop(\n",
    "            \"_merge\", axis=1\n",
    "            )\n",
    "\n",
    "        positive_samples[\"label\"] = 1\n",
    "        samples[\"label\"] = 0\n",
    "\n",
    "        logging.debug(\n",
    "            \"Added labels. Now: concat of positive and negative plus a shuffle\\\n",
    "                of all data\"\n",
    "        )\n",
    "        samples = (\n",
    "            pd.concat((positive_samples, samples)).sample(frac=1).reset_index(\n",
    "                drop=True)\n",
    "        )\n",
    "\n",
    "        customers_in_sample = samples[\"customer_id\"].unique()\n",
    "        articles_in_sample = samples[\"article_id\"].unique()\n",
    "\n",
    "        le_cust = LabelEncoder()\n",
    "        le_art = LabelEncoder()\n",
    "        logging.debug(\"Fitting label encoders\")\n",
    "        le_cust.fit(customers_in_sample)\n",
    "        le_art.fit(articles_in_sample)\n",
    "\n",
    "        # Doing as much as possible in-place here:\n",
    "        logging.debug(\"Transforming IDs to label-encoded IDs\")\n",
    "        samples[\"customer_id\"] = le_cust.transform(samples[\"customer_id\"])\n",
    "        samples[\"article_id\"] = le_art.transform(samples[\"article_id\"])\n",
    "        samples[\"label\"] = samples[\"label\"].astype(\n",
    "            np.uint8\n",
    "        )  # Just to reproduce parent method...\n",
    "        logging.info(\"Done generating data!\")\n",
    "        return (\n",
    "            samples,\n",
    "            le_cust,\n",
    "            le_art,\n",
    "        )\n",
    "\n",
    "\n",
    "# <<<<< DATA LOADING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model specifications\n",
    "\n",
    "In this portion, we define the models used for training. `HM_model` is the baseline class which only uses transactional data. The extended model is called `HM_Extended`, and uses attributes from customers and articles to try to improve the prediction.\n",
    "\n",
    "We also define the dataclass `Hyperparameters`, working as a container of all possible hyperparameters to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>> MODEL DEFINITINON AND TRAINING\n",
    "\n",
    "\n",
    "class HM_model(torch.nn.Module):\n",
    "    \"\"\"Baseline HM model\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_customer,\n",
    "        num_articles,\n",
    "        embedding_size,\n",
    "        bias_nodes: bool = True,\n",
    "        sparse: bool = False,\n",
    "    ):\n",
    "        super(HM_model, self).__init__()\n",
    "        self.customer_embed = torch.nn.Embedding(\n",
    "            num_embeddings=num_customer, embedding_dim=embedding_size,\n",
    "            sparse=sparse\n",
    "        )\n",
    "        self.art_embed = torch.nn.Embedding(\n",
    "            num_embeddings=num_articles, embedding_dim=embedding_size,\n",
    "            sparse=sparse\n",
    "        )\n",
    "        if not bias_nodes:  # Default is that we DO have biases\n",
    "            # They're added lienarly so this should give no effect\n",
    "            self.customer_bias = lambda row: 0\n",
    "            self.article_bias = lambda row: 0\n",
    "        else:\n",
    "            self.customer_bias = torch.nn.Embedding(\n",
    "                num_customer, 1, sparse=sparse)\n",
    "            self.article_bias = torch.nn.Embedding(\n",
    "                num_articles, 1, sparse=sparse)\n",
    "\n",
    "    def forward(self, customer_row, article_row):\n",
    "        \"\"\"The forward pass used in model training (matrix factorization)\n",
    "\n",
    "        Args:\n",
    "            customer_row (Tensor): Tensor of (batch of) customer row(s)\n",
    "            article_row (Tensor): Tensor of (batch of) article row(s)\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Activation of U@V^T + B_u + B_v\n",
    "        \"\"\"\n",
    "        customer_embed = self.customer_embed(customer_row)\n",
    "\n",
    "        art_embed = self.art_embed(article_row)\n",
    "        # dot_prod_old = torch.sum(torch.mul(customer_embed, art_embed), 1)\n",
    "        dot_prod = (customer_embed * art_embed).sum(dim=1, keepdim=True)\n",
    "        # Add bias nodes to model:\n",
    "        dot_prod = (\n",
    "            dot_prod + self.customer_bias(customer_row) +\n",
    "            self.article_bias(article_row)\n",
    "        )\n",
    "        return torch.sigmoid(dot_prod)\n",
    "\n",
    "\n",
    "class HM_Extended(HM_model):\n",
    "    \"\"\"Model class extending the information used in learning process\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_customer,\n",
    "        num_articles,\n",
    "        num_age,\n",
    "        num_idxgroup,\n",
    "        num_garmentgroup,\n",
    "        embedding_size,\n",
    "        bias_nodes: bool = True,\n",
    "        sparse: bool = False,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            num_customer, num_articles, embedding_size, bias_nodes\n",
    "            )\n",
    "        del self.art_embed  # For model loading compatibility\n",
    "        self.article_embed = torch.nn.Embedding(\n",
    "            num_articles, embedding_size, sparse=sparse\n",
    "        )\n",
    "        self.age_embed = torch.nn.Embedding(num_age, embedding_size)\n",
    "        self.indexgroup_embed = torch.nn.Embedding(\n",
    "            num_idxgroup, embedding_size\n",
    "            )\n",
    "        self.garmentgroup_embed = torch.nn.Embedding(\n",
    "            num_garmentgroup, embedding_size\n",
    "            )\n",
    "\n",
    "        self.article_MLP = torch.nn.Linear(embedding_size * 3, embedding_size)\n",
    "        self.customer_MLP = torch.nn.Linear(embedding_size * 2, embedding_size)\n",
    "\n",
    "    def article_transform(self, article, garment_group, index_group):\n",
    "        \"\"\"Concatenates each article embedding through its linear layer,\n",
    "        activation is sigmoid\"\"\"\n",
    "        embeds = (\n",
    "            self.article_embed(article),\n",
    "            self.indexgroup_embed(index_group),\n",
    "            self.garmentgroup_embed(garment_group),\n",
    "        )\n",
    "        final_embedding = self.article_MLP(torch.cat(embeds, 1))\n",
    "        return torch.sigmoid(final_embedding)\n",
    "\n",
    "    def customer_transform(self, customer, age):\n",
    "        \"\"\"Concatenates each customer embedding through its linear layer,\n",
    "        activation is sigmoid\"\"\"\n",
    "        embeds = (self.customer_embed(customer), self.age_embed(age))\n",
    "        final_embedding = self.customer_MLP(torch.cat(embeds, 1))\n",
    "        return torch.sigmoid(final_embedding)\n",
    "\n",
    "    def forward(self, row):\n",
    "        \"\"\"Forward pass for extended model, overrides parent method\"\"\"\n",
    "        customer, article, age, garment_group, index_group = [\n",
    "            row[:, i] for i in range(5)\n",
    "        ]\n",
    "        # Manipulate IDs to be zero-indexed, thus we don't need label encoders.\n",
    "        age[age < 0] = 36  # Cast NaNs to average of all customers, 36\n",
    "        garment_group = garment_group - 1001\n",
    "        index_group = index_group - 1\n",
    "        age = age - 1\n",
    "        customer_matrix = self.customer_transform(customer, age)\n",
    "        article_matrix = self.article_transform(\n",
    "            article, garment_group, index_group\n",
    "            )\n",
    "        biases = self.customer_bias(customer), self.article_bias(article)\n",
    "        x = (customer_matrix * article_matrix).sum(1, keepdim=True)\n",
    "        x = x + biases[0] + biases[1]\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "# Class method to HM_Extended\n",
    "def _extend_row_data(\n",
    "    self: HM_Extended,\n",
    "    customer_rows: Iterable[str],\n",
    "    article_rows: Iterable[str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Adds the given customer/article rows to dataset-object (not in-place)\n",
    "\n",
    "    Args:\n",
    "        self (Data_HM): Main dataset object\n",
    "        customer_rows (Iterable[str]): List of column names of customer info\n",
    "        article_rows (Iterable[str]): List of column names of article info\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Modified self.df_id with the additional info\n",
    "    \"\"\"\n",
    "    customer_rows = [\"customer_id\"] + customer_rows\n",
    "    article_rows = [\"article_id\"] + article_rows\n",
    "\n",
    "    # Find original customer and article IDs present in dataset\n",
    "    df_decoded = self.df_id.copy()\n",
    "    df_decoded[\"article_id\"] = self.le_art.inverse_transform(\n",
    "        df_decoded[\"article_id\"]\n",
    "    )\n",
    "    df_decoded[\"customer_id\"] = self.le_cust.inverse_transform(\n",
    "        df_decoded[\"customer_id\"]\n",
    "    )\n",
    "    enc_customers, enc_articles = load_kaggle_assets(\n",
    "        to_download=[\"customers.csv\", \"articles.csv\"]\n",
    "    )\n",
    "    enc_customers = enc_customers[\n",
    "        enc_customers[\"customer_id\"].isin(df_decoded[\"customer_id\"])\n",
    "    ]\n",
    "    enc_articles = enc_articles[\n",
    "        enc_articles[\"article_id\"].isin(df_decoded[\"article_id\"])\n",
    "    ]\n",
    "\n",
    "    enc_customers[\"customer_id\"] = self.le_cust.transform(\n",
    "        enc_customers[\"customer_id\"]\n",
    "    )\n",
    "    enc_articles[\"article_id\"] = self.le_art.transform(\n",
    "        enc_articles[\"article_id\"]\n",
    "    )\n",
    "    df_ext = self.df_id.merge(enc_customers[customer_rows]).merge(\n",
    "        enc_articles[article_rows]\n",
    "    )\n",
    "    # Ensure that last column is the label\n",
    "    ordered_columns = df_ext.columns[df_ext.columns != \"label\"].append(\n",
    "        pd.Index([\"label\"])\n",
    "    )\n",
    "    logging.debug(\n",
    "        f\"df_id has been extended with\\\n",
    "            {', '.join(customer_rows[1:]+article_rows[1:])}\"\n",
    "    )\n",
    "    return df_ext[ordered_columns]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Hyperparameters:\n",
    "    lr_rate: float = 1e-3\n",
    "    weight_decay: str = 1e-4\n",
    "    epochs: int = 20\n",
    "    validation_frequency: int = 1\n",
    "    optimizer: Any = torch.optim.Adam\n",
    "    lossfnc: Any = torch.nn.BCELoss\n",
    "    embedding_size: int = 500\n",
    "    bias_nodes: bool = True\n",
    "    save_loss: Union[bool, str] = True\n",
    "    verbose: bool = False\n",
    "    min_lr: float = 0.0  # For LR scheduler\n",
    "\n",
    "    # These have no use if dataset is loaded from file\n",
    "    dataset_cases: int = 2000\n",
    "    dataset_portion_negatives: float = 0.9\n",
    "    dataset_train_portion: float = 0.7\n",
    "    dataset_batch_size: int = 5\n",
    "    dataset_full: bool = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Below is the implementations for training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model: HM_model,\n",
    "    data: Data_HM,\n",
    "    epoch_num: int,\n",
    "    optimizer,\n",
    "    loss,\n",
    "    verbose: bool = False,\n",
    "    baseline: bool = True,\n",
    "):\n",
    "    \"\"\"Trains a single epoch of Data_HM (or child) model\n",
    "\n",
    "    Args:\n",
    "        model (HM_model): HM_model class or child to HM_model.\n",
    "        data (Data_HM): Dataset class, either Data_HM or a child class\n",
    "        epoch_num (int): Which epoch this run is\n",
    "        optimizer (_type_): Optimizer type from main call, usually Adam\n",
    "        loss (_type_): Loss function, usually BCE\n",
    "        verbose (bool, optional): Verbose flag, deprecated. Defaults to False.\n",
    "        baseline (bool, optional): Baseline flag (training a little different\n",
    "                                        for extended moel). Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        float: Total loss for current epoch\n",
    "    \"\"\"\n",
    "    epoch_loss = 0\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    for item in data.get_DataLoader(trainDL=True):\n",
    "        item = tuple(t.to(device) for t in item)\n",
    "        row, label = item\n",
    "        if not baseline:\n",
    "            row = row.int()  # For ext. model\n",
    "        optimizer.zero_grad()\n",
    "        if not baseline:\n",
    "            pred = model(row)\n",
    "        else:\n",
    "            pred = model(row[:, 0], row[:, 1])\n",
    "        loss_value = loss(pred.view(-1), torch.FloatTensor(\n",
    "            label.tolist()\n",
    "        ).to(device))\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss_value\n",
    "    if verbose:\n",
    "        logging.info(f\"\\t| Training loss for epoch\\\n",
    "            {epoch_num+1}: {epoch_loss}\")\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def train(model, data, params, baseline: bool = True, plot_loss: bool = False):\n",
    "    \"\"\"Main training function for models\n",
    "\n",
    "    Args:\n",
    "        model (HM_model | HM_Extended): Model class to train\n",
    "        data (Data_HM | Data_HM_Complete): Dataset class to use\n",
    "        params (Hyperparameters): Hyperparameter container\n",
    "                (and also some other settings)\n",
    "        baseline (bool, optional): Baseline flag. Defaults to True.\n",
    "        plot_loss (bool, optional): Flag to produce loss plots to disk or not.\n",
    "                                    Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        float: Validation loss for the last (computed) epoch\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # Uses binary cross entropy at the moment.\n",
    "    loss_metric = params.lossfnc().to(device)\n",
    "    if params.optimizer == \"SparseAdam\":\n",
    "        # Does not have weight decay parameter\n",
    "        optimizer = torch.optim.SparseAdam(\n",
    "            model.parameters(), lr=params.lr_rate\n",
    "        )\n",
    "    else:\n",
    "        optimizer = params.optimizer(\n",
    "            model.parameters(), lr=params.lr_rate,\n",
    "            weight_decay=params.weight_decay\n",
    "        )\n",
    "\n",
    "    # Adjust lr once model stops improving using scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, min_lr=params.min_lr, verbose=params.verbose\n",
    "    )\n",
    "\n",
    "    save_loss = params.save_loss\n",
    "    if save_loss:\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        # `settings` just contains all hyperparameter info, to be able to\n",
    "        # distinguish which models produce the loss plots when\n",
    "        # running multiple at the same time\n",
    "        settings = \",\".join([str(v) for v in params.__dict__.values()])\n",
    "\n",
    "    for epoch in tqdm(range(params.epochs)):\n",
    "        model.train()\n",
    "        dataloader_train = data.get_DataLoader(trainDL=True)\n",
    "        dataloader_valid = data.get_DataLoader(trainDL=False)\n",
    "        epoch_loss = train_one_epoch(\n",
    "            model,\n",
    "            data,\n",
    "            epoch,\n",
    "            optimizer,\n",
    "            loss_metric,\n",
    "            params.verbose,\n",
    "            baseline,\n",
    "        )\n",
    "        if not epoch % params.validation_frequency:\n",
    "            # Validate step\n",
    "            model.eval()\n",
    "            valid_loss = 0.0\n",
    "            for item in dataloader_valid:\n",
    "                item = tuple(t.to(device) for t in item)\n",
    "                row, label = item\n",
    "                if not baseline:\n",
    "                    row = row.int()\n",
    "                    pred = model(row)\n",
    "                else:\n",
    "                    pred = model(row[:, 0], row[:, 1])\n",
    "                loss = loss_metric(\n",
    "                    pred.view(-1), torch.FloatTensor(label.tolist()).to(device)\n",
    "                )\n",
    "                valid_loss = loss.item() * row.size(0)\n",
    "\n",
    "            lr_scheduler.step(valid_loss)  # Update lr scheduler\n",
    "            if params.verbose:\n",
    "                logging.info(f\"Provisory results for epoch {epoch+1}:\")\n",
    "                logging.info(\n",
    "                    f\"Loss for training set\\t\\\n",
    "                        {epoch_loss.tolist() / len(dataloader_train)}\"\n",
    "                )\n",
    "                logging.info(\n",
    "                    f\"Loss for validation set\\t\\\n",
    "                        {valid_loss / len(dataloader_valid)}\"\n",
    "                )\n",
    "            if save_loss:\n",
    "                train_losses.append(\n",
    "                    epoch_loss.tolist() / len(dataloader_train)\n",
    "                )\n",
    "                valid_losses.append(\n",
    "                    valid_loss / len(dataloader_valid)\n",
    "                )\n",
    "    if save_loss:\n",
    "        fn_append = save_loss if isinstance(save_loss, str) else \"\"\n",
    "        save_dir = os.path.join(\"results\", datetime.today().strftime(\n",
    "            \"%Y.%m.%d.%H.%M\"\n",
    "            ))\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        filename = (\n",
    "            os.path.join(save_dir, f\"losses_{fn_append}.csv\")\n",
    "            .replace(\"<\", \"\")\n",
    "            .replace(\">\", \"\")\n",
    "        )\n",
    "        np.savetxt(\n",
    "            filename,\n",
    "            np.transpose([train_losses, valid_losses]),\n",
    "            delimiter=\",\",\n",
    "        )\n",
    "        logging.debug(f\"Saved losses for this run to {filename}\")\n",
    "        if plot_loss:\n",
    "            plot_loss_results(\n",
    "                filename,\n",
    "                save=f\"{'b' if baseline else 'e'}_{settings}.pdf\",\n",
    "                baseline=baseline,\n",
    "            )\n",
    "            # # If you want to remove the csv files of the losses,\n",
    "            # # you can call `os.remove(filename)` here\n",
    "            # os.remove(filename)\n",
    "    return valid_loss / len(dataloader_valid)\n",
    "\n",
    "\n",
    "def load_dataset_and_train(\n",
    "    use_min_dataset: bool = False,\n",
    "    persisted_dataset_path: Union[str, None] = None,\n",
    "    save_model: Union[str, bool] = False,\n",
    "    hyperparams=Hyperparameters(),\n",
    "    transactions_path: str = \"dataset/transactions_train.csv\",\n",
    "    baseline: bool = True,\n",
    "):\n",
    "    \"\"\"This model loads the data files, database object and\n",
    "    runs the train method. Will also save trained model\"\"\"\n",
    "\n",
    "    # Load data\n",
    "    if use_min_dataset:\n",
    "        df_c, df_a, df_t = load_min_data(\n",
    "            [\n",
    "                f\"dataset_sample/{n}_min.csv\"\n",
    "                for n in (\"customer\", \"articles\", \"transactions\")\n",
    "            ]\n",
    "        )\n",
    "    elif hyperparams.dataset_full:\n",
    "        logging.debug(\"Creating full dataset object\")\n",
    "        df_t = pd.read_pickle(transactions_path)\n",
    "        dataset_params = {\n",
    "            \"total_cases\": hyperparams.dataset_cases,\n",
    "            \"portion_negatives\": hyperparams.dataset_portion_negatives,\n",
    "            \"df_transactions\": df_t,\n",
    "            \"train_portion\": hyperparams.dataset_train_portion,\n",
    "            \"batch_size\": hyperparams.dataset_batch_size,\n",
    "        }\n",
    "        data = Data_HM_Complete(**dataset_params)\n",
    "        logging.debug(\"Full dataset object done!\")\n",
    "    elif persisted_dataset_path is None:\n",
    "        df_c = pd.read_csv(\"dataset/customers.csv\")\n",
    "        # Articles IDs all start with 0 which disappears if cast to a number\n",
    "        df_a = pd.read_csv(\"dataset/articles.csv\", dtype={\"article_id\": str})\n",
    "        df_t = pd.read_csv(transactions_path, dtype={\"article_id\": str})\n",
    "        df_c = clean_customer_data(df_c)\n",
    "        df_a = clean_article_data(df_a)\n",
    "\n",
    "        dataset_params = {\n",
    "            \"total_cases\": hyperparams.dataset_cases,\n",
    "            \"portion_negatives\": hyperparams.dataset_portion_negatives,\n",
    "            \"df_transactions\": df_t,\n",
    "            \"train_portion\": hyperparams.dataset_train_portion,\n",
    "            \"batch_size\": hyperparams.dataset_batch_size,\n",
    "        }\n",
    "        data = Data_HM(**dataset_params)\n",
    "        save_dataset_obj(\n",
    "            data,\n",
    "            \"object_storage/dataset-\" +\n",
    "            f\"{datetime.today().strftime('%Y.%m.%d.%H.%M')}.pckl\",\n",
    "        )\n",
    "    else:\n",
    "        data = read_dataset_obj(persisted_dataset_path)\n",
    "        logging.debug(\"Read dataset successfully\")\n",
    "\n",
    "    model = load_model(\n",
    "        baseline, data, hyperparams.embedding_size, hyperparams.bias_nodes\n",
    "    )\n",
    "    logging.debug(\"Created model sucessfully\")\n",
    "    last_valid_loss = train(model, data, hyperparams, baseline, plot_loss=True)\n",
    "    if save_model:\n",
    "        if isinstance(save_model, bool):\n",
    "            save_model = datetime.today().strftime(\"%Y.%m.%d.%H.%M\") + \".pth\"\n",
    "        if not os.path.isdir(\"models\"):\n",
    "            os.mkdir(\"models\")\n",
    "        torch.save(model.state_dict(), os.path.join(\"models\", save_model))\n",
    "    return last_valid_loss\n",
    "\n",
    "\n",
    "def get_k_most_purchased(\n",
    "    k: int,\n",
    "    transactions_path: Union[str, pd.DataFrame],\n",
    "    pass_as_df: bool = False,\n",
    "    by_index_group: Union[bool, int] = False,\n",
    "    bulk_index_group=None,\n",
    ") -> np.ndarray:\n",
    "    if isinstance(transactions_path, str):\n",
    "        if transactions_path.endswith(\"pckl\"):\n",
    "            df = pd.read_pickle(transactions_path)\n",
    "        else:\n",
    "            df = pd.read_csv(transactions_path, dtype={\"article_id\": str})\n",
    "    else:\n",
    "        df = transactions_path\n",
    "    if by_index_group:\n",
    "        logging.debug(f\"Accessing topk based on index group {by_index_group}\")\n",
    "        _, df_articles = load_kaggle_assets([\"customers.csv\", \"articles.csv\"])\n",
    "        if bulk_index_group is not None:\n",
    "            out = []\n",
    "            for index_group in bulk_index_group:\n",
    "                df_i = df[df[\"index_group_no\"] == index_group]\n",
    "                out.append(\n",
    "                    df_i.groupby(\"article_id\")\n",
    "                    .agg(\"customer_id\")\n",
    "                    .count()\n",
    "                    .sort_values(ascending=False)\n",
    "                    .head(k)\n",
    "                    .index.values\n",
    "                )\n",
    "            return out\n",
    "\n",
    "        df_articles[df_articles[\"index_group_no\"] == by_index_group]\n",
    "        df_articles = df_articles[[\"article_id\", \"index_group_no\"]]\n",
    "        df = df.merge(df_articles, on=\"article_id\")\n",
    "\n",
    "    return (\n",
    "        df.groupby(\"article_id\")\n",
    "        .agg(\"customer_id\")\n",
    "        .count()\n",
    "        .sort_values(ascending=False)\n",
    "        .head(k)\n",
    "        .index.values\n",
    "    )\n",
    "\n",
    "\n",
    "# <<<<< MODEL DEFINITION AND TRAINING\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "\n",
    "Below are a couple important utility functions.\n",
    "\n",
    "* `time_pd_vs_np` computes the runtime from the sampling methods available, used to show that the pandas method is really slow\n",
    "* `plot_negative_sampling` is also related to the same result, producing the plots\n",
    "* `save_dataset_obj` is technically a general method to serialize any object. If the path in argument does not exist, it will create it (safer)\n",
    "* `read_dataset_obj` similarly just reads a pickled object\n",
    "* `load_kaggle_assets` downloads tables from the competition site, loads to DataFrames and removes the files from disk.\n",
    "* `load_from_gdrive` downloads a file based on Google drive URL/ID\n",
    "* `load_model` will retrieve the correct object based on what model is passed as argument.\n",
    "* `plot_loss_results` shows the training and/or validation loss for a training result and stores to disk\n",
    "* `compare_hyperparameter_results` produces similar plots specialized to list of results for different hyperparameters\n",
    "\n",
    "A few of these are also not used at the moment:\n",
    "\n",
    "* `heuristic_embedding_size` is a result from FastAI indicating optimal choice for embedding size. Not actually used currently\n",
    "* `explore_hyperparameters` does a *naÃ¯ve* hyperparameter search, finding the optimal value by tuning one variable at a time, keeping the rest constant.\n",
    "* `alternative_hyperparam_exploration` checks *every* combination of variable choices, but due to runtime is not used\n",
    "* `split_test_set_file` is a method to make a 70-30 split in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>> UTILITIES\n",
    "\n",
    "\n",
    "def time_pd_vs_np(n_negative, df) -> Tuple[float, float]:\n",
    "    \"\"\"Compute time it takes to sample n_negative negative transactions\n",
    "\n",
    "    Args:\n",
    "        n_negative (int): Number of negative samples\n",
    "        df (pd.DataFrame): Dataframe to sample from, requires columns\n",
    "        'customer_id' and 'article_id'\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: Time taken using Pandas objects (first value) and\n",
    "        NumPy objects (second value)\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    start_pd = time.time()\n",
    "    num_written = 0\n",
    "    tmpStr = \"customer_id,article_id\\n\"\n",
    "    while num_written < n_negative:\n",
    "        # Choose random customer and article\n",
    "        selection = np.array(\n",
    "            [\n",
    "                df[\"customer_id\"].sample().values,\n",
    "                df[\"article_id\"].sample().values,\n",
    "            ]\n",
    "        ).flatten()\n",
    "        if not (\n",
    "            (df[\"customer_id\"] == selection[0]) & (\n",
    "                df[\"article_id\"] == selection[1]\n",
    "                )).any():\n",
    "            tmpStr += f\"{selection[0]}, {selection[1]}\\n\"\n",
    "            num_written += 1\n",
    "    with open(\"tmp.csv\", \"w\") as f:\n",
    "        f.write(tmpStr)\n",
    "    _ = pd.read_csv(\"tmp.csv\")\n",
    "    os.remove(\"tmp.csv\")\n",
    "    time_pd = time.time() - start_pd\n",
    "\n",
    "    # Numpy method\n",
    "    start_np = time.time()\n",
    "    df_np = df[[\"customer_id\", \"article_id\"]].to_numpy()\n",
    "    neg_np = np.empty((n_negative, df_np.shape[1]), dtype=\"<U64\")\n",
    "    for i in range(n_negative):\n",
    "        legit = False\n",
    "        while not legit:\n",
    "            sample = [np.random.choice(\n",
    "                df_np[:, col]\n",
    "            ) for col in range(df_np.shape[1])]\n",
    "            legit = not (\n",
    "                (df_np[:, 0] == sample[0]) & (df_np[:, 1] == sample[1])\n",
    "            ).any()\n",
    "        neg_np[i, :] = sample\n",
    "    time_np = time.time() - start_np\n",
    "\n",
    "    return time_pd, time_np\n",
    "\n",
    "\n",
    "def plot_negative_sampling(\n",
    "    start: int,\n",
    "    stop: int,\n",
    "    step: int = 1,\n",
    "    filename: Union[str, None] = None,\n",
    "    persist_data: bool = True,\n",
    "    cont_from_checkpoint: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"Plot the outputs of `time_pd_vs_np` for different ranges of n_negative\n",
    "\n",
    "    Args:\n",
    "        start (int): Range of n_negative (inclusive)\n",
    "        stop (int): Range of n_negative (exclusive)\n",
    "        step (int, optional): Step in range of n_negative. Defaults to 1.\n",
    "        filename (str | None, optional): Plot output file name, if None,\n",
    "                                    does not save file. Defaults to None.\n",
    "        persist_data (bool, optional): Serialization option to store each\n",
    "                                        iterate's result. Defaults to True.\n",
    "        cont_from_checkpoint (bool, optional): Reads previous runs and doesn't\n",
    "                                recompute if done before. Defaults to True.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from tqdm import tqdm\n",
    "    import pickle\n",
    "\n",
    "    xax = list(range(start, stop, step))\n",
    "\n",
    "    if cont_from_checkpoint:\n",
    "        with open(\"plotData.pckl\", \"rb\") as f:\n",
    "            plot_values = pickle.load(f)\n",
    "\n",
    "        # Add empty list for keys not covered by checkpoint:\n",
    "        computed = set([x_i for x_i in plot_values.keys()])\n",
    "        to_add = set(xax) - computed\n",
    "        for elem in to_add:\n",
    "            plot_values[elem] = []\n",
    "\n",
    "        # Skip those already computed\n",
    "        xax = [x for x in xax if x not in computed]\n",
    "\n",
    "    else:\n",
    "        plot_values = {x_i: [] for x_i in xax}\n",
    "\n",
    "    for n_negative in tqdm(xax):\n",
    "        time_pd, time_np = time_pd_vs_np(n_negative)\n",
    "        plot_values[n_negative].extend([time_pd, time_np])\n",
    "\n",
    "        if persist_data:\n",
    "            with open(\"plotData.pckl\", \"wb\") as f:\n",
    "                pickle.dump(plot_values, f)\n",
    "\n",
    "    plt.plot(\n",
    "        plot_values.keys(),\n",
    "        plot_values.values(),\n",
    "        label=[\n",
    "            \"pandas.DataFrame.sample implementation\",\n",
    "            \"NumPy.random.choice implementation\",\n",
    "        ],\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Number of negative (generated) samples\")\n",
    "    plt.ylabel(\"Time [s]\")\n",
    "    plt.title(\"Comparison between sampling methods time\")\n",
    "    if filename is not None:\n",
    "        plt.savefig(f\"{filename}.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_dataset_obj(data: HM_model, dst: str) -> None:\n",
    "\n",
    "    if not os.path.isdir(os.path.dirname(dst)):\n",
    "        os.makedirs(os.path.dirname(dst))\n",
    "    with open(dst, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "def read_dataset_obj(src: str) -> Any:\n",
    "\n",
    "    with open(src, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_kaggle_assets(\n",
    "    to_download: Union[Iterable[str], None] = None\n",
    ") -> Iterable[pd.DataFrame]:\n",
    "    \"\"\"Downloads data from Kaggle competition, loads to Dataframes and deletes\n",
    "    original files. Made due to low disk quota on server\n",
    "\n",
    "    Args:\n",
    "        to_download (list[str] | None, optional): List of files to download,\n",
    "        if None will download all csv files. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        list[pd.DataFrame]: Dataframes from downloaded files\n",
    "    \"\"\"\n",
    "    logging.debug(\"Loading assets from Kaggle to Markov ...\")\n",
    "    from zipfile import ZipFile\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    if to_download is None:\n",
    "        to_download = [\n",
    "            \"customers.csv\", \"transactions_train.csv\", \"articles.csv\"\n",
    "        ]\n",
    "    pd_objects = []\n",
    "\n",
    "    for i, file in enumerate(to_download):\n",
    "        logging.debug(f\"Downlodaing file {file}\")\n",
    "        api.competition_download_file(\n",
    "            competition=\"h-and-m-personalized-fashion-recommendations\",\n",
    "            file_name=file\n",
    "        )\n",
    "\n",
    "        zf = ZipFile(f\"{file}.zip\")\n",
    "        zf.extractall()\n",
    "        zf.close()\n",
    "\n",
    "        # Customers-file does not require dtype specification, the others do\n",
    "        if i == 0:\n",
    "            df = pd.read_csv(file)\n",
    "        else:\n",
    "            df = pd.read_csv(file, dtype={\"article_id\": str})\n",
    "        pd_objects.append(df)\n",
    "        # Remove files created from download now that it's loaded to memory\n",
    "        os.remove(file)\n",
    "        os.remove(f\"{file}.zip\")\n",
    "        logging.debug(f\"Sucessfully loaded and removed {file}\")\n",
    "    return pd_objects\n",
    "\n",
    "\n",
    "def load_from_gdrive(gd_id: str, outpath: str = \"tmp_model.pth\") -> None:\n",
    "    \"\"\"Assumes you want to download a trained model from google drive (big!)\"\"\"\n",
    "    import gdown\n",
    "\n",
    "    gdown.download(id=gd_id, output=outpath)\n",
    "    assert os.path.isfile(outpath), \"Unable to fine downloaded file!\"\n",
    "    logging.debug(\n",
    "        \"Sucessfully downloaded pth file. Make sure to remove after loading\" +\n",
    "        \" to memory\"\n",
    "    )\n",
    "    return outpath\n",
    "\n",
    "\n",
    "def load_model(\n",
    "    baseline: bool,\n",
    "    data: Data_HM,\n",
    "    emb_sz: int = 500,\n",
    "    bias: bool = True,\n",
    "    sparse: bool = False,\n",
    ") -> object:\n",
    "    \"\"\"Based on baseline flag, retrieves model and ensures\n",
    "    column types are correct.\n",
    "\n",
    "    Args:\n",
    "        baseline (bool): Baseline flag\n",
    "        data (Data_HM): Original Dataset object (NB self.df_id can be\n",
    "        modified when passed).\n",
    "        emb_sz (int, optional): Embedding size, must match size in `data`.\n",
    "        Defaults to 500.\n",
    "        bias (bool, optional): Bias flags (include bias nodes or not).\n",
    "        Defaults to True.\n",
    "        sparse (bool, optional): Sparse property to model embeddings.\n",
    "        Defaults to False\n",
    "\n",
    "    Returns:\n",
    "        object: HM_model or HM_Extended\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    n_cust, n_art, *_ = data.df_id.nunique()\n",
    "    if baseline:\n",
    "        model = HM_model(\n",
    "            n_cust, n_art, embedding_size=emb_sz, bias_nodes=bias,\n",
    "            sparse=sparse\n",
    "        ).to(device)\n",
    "    else:\n",
    "        additional_columns = {\"age\", \"garment_group_no\", \"index_group_no\"}\n",
    "        if additional_columns == additional_columns.intersection(\n",
    "            set(data.df_id.columns)\n",
    "        ):\n",
    "            logging.debug(\n",
    "                \"Dataset object is already extended, no need to download anew\"\n",
    "            )\n",
    "        else:\n",
    "            logging.debug(\"Transforming df_id to work for extended model\")\n",
    "            data.df_id = _extend_row_data(\n",
    "                data, [\"age\"], [\"garment_group_no\", \"index_group_no\"]\n",
    "            )\n",
    "        # age/idxgroup/ggroup have to be the max instead of nunique\n",
    "        n_age, n_idxgroup, n_garmentgroup = (\n",
    "            data.df_id.max()[2:5].values.astype(int)\n",
    "        )\n",
    "\n",
    "        model = HM_Extended(\n",
    "            num_customer=n_cust,\n",
    "            num_articles=n_art,\n",
    "            num_age=n_age,\n",
    "            num_idxgroup=n_idxgroup,\n",
    "            num_garmentgroup=n_garmentgroup,\n",
    "            embedding_size=emb_sz,\n",
    "            bias_nodes=bias,\n",
    "        ).to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "def plot_loss_results(\n",
    "    lossfile,\n",
    "    which: str = \"all\",\n",
    "    plot_title: bool = True,\n",
    "    save: Union[str, None] = None,\n",
    "    baseline: bool = True,\n",
    "):\n",
    "    \"\"\"Generate plot of training and/or validation loss across trained epochs\n",
    "\n",
    "    Args:\n",
    "        lossfile (str): Path to csv file containing training\n",
    "            and validation losses\n",
    "        which (str, optional): Which plot to make (training, validation, all).\n",
    "            Defaults to \"all\".\n",
    "        plot_title (bool, optional): Title flag. Won't use any titles if False.\n",
    "            Defaults to True.\n",
    "        save (Union[str, None], optional): Destination path for plot.\n",
    "            Won't save to disk of None. Defaults to None.\n",
    "        baseline (bool, optional): Baseline flag\n",
    "            (assumes model=='extended' if False). Defaults to True.\n",
    "    \"\"\"\n",
    "    res = pd.read_csv(lossfile, header=None)\n",
    "    plt.figure()\n",
    "    if which != \"validation\":\n",
    "        plt.plot(res[0], label=\"Training loss\")\n",
    "    if which != \"training\":\n",
    "        plt.plot(res[1], label=\"Validation loss\")\n",
    "    plt.xlabel(\"Epoch number\")\n",
    "    plt.ylabel(\"Loss value\")\n",
    "    plt.legend()\n",
    "    title = \"Training and validation\" if which == \"all\" else which.capitalize()\n",
    "    title += f\" loss for {'baseline' if baseline else 'extended'} model\"\n",
    "    if plot_title:\n",
    "        plt.title(title)\n",
    "    if save is not None:\n",
    "        plt.savefig(save)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def compare_hyperparameter_results(data: Iterable[str]):\n",
    "    \"\"\"Helper to plot the training loss for each run specified in `data`\"\"\"\n",
    "    plt_calls = {}\n",
    "    for results_fn in data:\n",
    "        plt.xlabel(\"Epoch number\")\n",
    "        plt.ylabel(\"Loss value\")\n",
    "        res = pd.read_csv(results_fn, header=None)\n",
    "        setting = results_fn[\n",
    "            results_fn.find(\"_\") + 1:\n",
    "        ]  # Retrieves filename more or less\n",
    "        variable = setting.split(\"=\")[0]\n",
    "        # Here we just iterate through Hyperparam. choices and\n",
    "        # store the function call for later use\n",
    "        if variable in plt_calls:\n",
    "            plt_calls[variable].append(\n",
    "                functools.partial(plt.plot, res[0], label=setting)\n",
    "            )\n",
    "        else:\n",
    "            plt_calls[variable] = [\n",
    "                functools.partial(plt.plot, res[0], label=setting)\n",
    "            ]\n",
    "\n",
    "    # Iterate through previously established function calls and plots\n",
    "    for var, cmd_list in plt_calls.items():\n",
    "        for cmd in cmd_list:\n",
    "            cmd()  # plt.plot(...) for specific variable\n",
    "        plt.legend()\n",
    "        plt.yscale(\"log\")\n",
    "        plt.title(f\"Training loss loss for different choices of {var}\")\n",
    "        plt.savefig(f\"hyperparams_train_{var}.pdf\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def explore_hyperparameters():\n",
    "    \"\"\"Runs model for different choices of weight decay,\n",
    "    embedding size and learn rate\"\"\"\n",
    "    weight_decays = (1e-4, 1e-3, 1e-2, 1e-1)\n",
    "    embedding_size = (10, 100, 500, 1000, int(1e4))\n",
    "    lr_rates = (1e-5, 1e-4, 1e-3, 1e-2)\n",
    "    print(\"Testing EMBEDDING SIZE\")\n",
    "    for emb_size in embedding_size:\n",
    "        testing_param = f\"{emb_size = }\"\n",
    "        print(testing_param)\n",
    "        hparams = Hyperparameters(\n",
    "            embedding_size=emb_size,\n",
    "            save_loss=testing_param\n",
    "        )\n",
    "        load_dataset_and_train(\n",
    "            persisted_dataset_path=\"object_storage/HM_data.pckl\",\n",
    "            hyperparams=hparams\n",
    "        )\n",
    "    print(\"Testing WEIGHT DECAYS\")\n",
    "    for wd in weight_decays:\n",
    "        testing_param = f\"{wd = }\"\n",
    "        print(testing_param)\n",
    "        hparams = Hyperparameters(weight_decay=wd, save_loss=testing_param)\n",
    "        load_dataset_and_train(\n",
    "            persisted_dataset_path=\"object_storage/HM_data.pckl\",\n",
    "            hyperparams=hparams\n",
    "        )\n",
    "    for lr in lr_rates:\n",
    "        testing_param = f\"{lr = }\"\n",
    "        print(testing_param)\n",
    "        hparams = Hyperparameters(lr_rate=lr, save_loss=testing_param)\n",
    "        load_dataset_and_train(\n",
    "            persisted_dataset_path=\"object_storage/HM_data.pckl\",\n",
    "            hyperparams=hparams\n",
    "        )\n",
    "\n",
    "\n",
    "# Functions below are currently not in use, but can be practical\n",
    "\n",
    "\n",
    "def heuristic_embedding_size(cardinality):\n",
    "    # https://github.com/fastai/fastai/blob/master/fastai/tabular/model.py#L12\n",
    "    return min(600, round(1.6 * cardinality**0.56))\n",
    "\n",
    "\n",
    "def alternative_hyperparam_exploration(\n",
    "    wds: Iterable,\n",
    "    embszs: Iterable,\n",
    "    lrs: Iterable,\n",
    "    dataset_path=None,\n",
    "    baseline: bool = True,\n",
    ") -> dict:\n",
    "    \"\"\"Checks each combination of choices and prints out the one\n",
    "    with best validation loss\n",
    "\n",
    "    Args:\n",
    "        wds (Iterable): List of weight decay\n",
    "        embszs (Iterable): List of embedding sizes\n",
    "        lrs (Iterable): List of learning rates\n",
    "        dataset_path (str|None, optional): Path to dataset instance,\n",
    "            if None will generate a new each time. Defaults to None.\n",
    "        baseline (bool, optional): Flag if model to test is the\n",
    "            baseline or extended. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary of parameter choices corresponding to best model\n",
    "    \"\"\"\n",
    "    best_model = {}\n",
    "    best_loss = np.inf\n",
    "    for emb_size in embszs:\n",
    "        for wd in wds:\n",
    "            for lr in lrs:\n",
    "                params = {\n",
    "                    \"embedding_size\": emb_size,\n",
    "                    \"epochs\": 20,\n",
    "                    \"save_loss\": True,\n",
    "                    \"weight_decay\": wd,\n",
    "                    \"lr_rate\": lr,\n",
    "                    \"verbose\": True,\n",
    "                    \"validation_frequency\": 1,\n",
    "                }\n",
    "                logging.debug(\n",
    "                    \"Training 20 epochs of \" +\n",
    "                    f\"{'baseline' if baseline else 'extended'} model.\" +\n",
    "                    f\"Settings: {lr = }, {wd = }, {emb_size = }\"\n",
    "                )\n",
    "                loss = load_dataset_and_train(\n",
    "                    persisted_dataset_path=dataset_path,\n",
    "                    hyperparams=Hyperparameters(**params),\n",
    "                    baseline=baseline,\n",
    "                    save_model=True,\n",
    "                )\n",
    "                logging.debug(f\"Last epoch loss: {loss}\")\n",
    "                if loss < best_loss:\n",
    "                    best_model = params\n",
    "                    best_loss = loss\n",
    "                    logging.debug(f\"Found better model:\\n{best_model}\\n--\")\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def split_test_set_file():\n",
    "    \"\"\"Method to make a 70-30 split in dataset, not in use\"\"\"\n",
    "    full_set = pd.read_csv(\n",
    "        \"dataset/transactions_train.csv\", dtype={\"article_id\": str}\n",
    "    )\n",
    "    num_train = int(len(full_set) * 0.7)\n",
    "    num_test = len(full_set) - num_train\n",
    "    test_idx = np.random.randint(0, len(full_set), size=num_test)\n",
    "    full_set.iloc[test_idx].to_csv(\"dataset/tr_test.csv\")\n",
    "    full_set.drop(test_idx).to_csv(\"dataset/tr_train.csv\")\n",
    "\n",
    "\n",
    "# <<<<< UTILITIES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Below we implement the methods for computing the MAP@12 metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>> INFERENCE\n",
    "\n",
    "\n",
    "def predictions_aggregator(data: Data_HM, by_index: bool = True):\n",
    "    \"\"\"Get k top predictions for customer only based on\n",
    "    what index group it has the most purchases in\"\"\"\n",
    "    if not by_index:\n",
    "        df = data.df_id[data.df_id[\"label\"] == 1].drop(\"label\", axis=1)\n",
    "        top_predictions = get_k_most_purchased(12, df).tolist()\n",
    "        return {\n",
    "            customer: [top_predictions] for customer in df[\n",
    "                \"customer_id\"\n",
    "            ].unique()\n",
    "            }\n",
    "\n",
    "    df = _extend_row_data(data, [], [\"index_group_no\"])  # df_id\n",
    "    train = data.get_data_from_subset(data.train)  # train set of df_id\n",
    "    df = df.merge(train)\n",
    "    df = df[df[\"label\"] == 1].drop(\"label\", axis=1)\n",
    "    logging.debug(f\"DF has now rows {df.columns} and is of length {len(df)}.\")\n",
    "    all_index_groups = df[\"index_group_no\"].unique()\n",
    "    all_top_predictions = get_k_most_purchased(\n",
    "        k=12,\n",
    "        transactions_path=df,\n",
    "        pass_as_df=True,  # Note that this is currently unsued\n",
    "        by_index_group=True,\n",
    "        bulk_index_group=all_index_groups,\n",
    "    )\n",
    "    top_predictions = {\n",
    "        index_group: value\n",
    "        for index_group, value in zip(all_index_groups, all_top_predictions)\n",
    "    }\n",
    "    logging.debug(top_predictions)\n",
    "    best_preds = {}\n",
    "    group = (\n",
    "        df\n",
    "        .groupby(\"customer_id\")\n",
    "        .agg({\"index_group_no\": list})\n",
    "        .reset_index()\n",
    "    )\n",
    "    group[\"index_group_no\"] = group[\"index_group_no\"].apply(\n",
    "        lambda lst: max(set(lst), key=lst.count)\n",
    "    )\n",
    "    for idx, row in group.iterrows():\n",
    "        best_preds[row.customer_id] = [\n",
    "            top_predictions[row.index_group_no].tolist()\n",
    "        ]\n",
    "\n",
    "    return best_preds\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def topk_for_all_customers(\n",
    "    model_path,\n",
    "    test_data,\n",
    "    k: int = 12,\n",
    "    baseline: bool = False,\n",
    "    n_customer_threshold: int = 100,\n",
    "    remove_pth_file_after_load: bool = False,\n",
    ") -> dict:\n",
    "    \"\"\"Make dictionary on form {customer: [art_1, ..., art_k]}\n",
    "    for k highest predicted articles, for each customer and article\n",
    "    in the *validation set*\"\"\"\n",
    "\n",
    "    def _update_out_dict(all_preds: defaultdict):\n",
    "        \"\"\"Helper to add predictions to out_dict\"\"\"\n",
    "        for customer_dict in all_preds:\n",
    "            k_i = min(k, len(all_preds[customer_dict]))\n",
    "            keys_i = np.array(list(all_preds[customer_dict].keys()))\n",
    "            values_i = list(all_preds[customer_dict].values())\n",
    "            top_ind = np.argpartition(values_i, -k_i)[-k_i:]\n",
    "            out_dict[customer_dict] = [keys_i[top_ind].tolist()]\n",
    "\n",
    "    def iterate_through_extended_model(all_preds: defaultdict):\n",
    "        \"\"\"Internal method if predictions are not for baseline\"\"\"\n",
    "        logging.debug(\"Entered extended iterating method\")\n",
    "        customer_count = 0\n",
    "        for customer_row in tqdm(valid_customers):\n",
    "            customer_i = customer_row[0].item()  # Don't include age data\n",
    "            customer_count += 1\n",
    "            for article_rows in valid_articles:\n",
    "                bz = article_rows.shape[0]\n",
    "                customer_rows = customer_row.expand(bz, 2)\n",
    "                row = torch.concat(\n",
    "                    (\n",
    "                        customer_rows[:, 0].reshape((bz, 1)),\n",
    "                        article_rows[:, 0].reshape((bz, 1)),\n",
    "                        customer_rows[:, 1].reshape((bz, 1)),\n",
    "                        article_rows[:, 1:].reshape((bz, 2)),\n",
    "                    ),\n",
    "                    dim=1,\n",
    "                )\n",
    "                pred = model(row).view(-1)\n",
    "                for i, pred_i in enumerate(pred):\n",
    "                    article_i = row[i, 1].item()\n",
    "                    all_preds[customer_i][article_i] = pred_i\n",
    "            if customer_count % n_customer_threshold == 0:\n",
    "                # Clears all predictions again to save memory\n",
    "                logging.debug(\"(new) Sending batch to out dict...\")\n",
    "                _update_out_dict(all_preds)\n",
    "                # Free memory and re-initiate defaultdict\n",
    "                del all_preds\n",
    "                all_preds = defaultdict(dict)\n",
    "\n",
    "    # Initialize model and dataset\n",
    "    all_preds = defaultdict(\n",
    "        dict\n",
    "    )  # Essentially a 2D-dict, see `collections.defaultdict`\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = load_model(baseline, test_data, emb_sz=500)\n",
    "    model.load_state_dict(torch.load(\n",
    "        model_path, map_location=torch.device(device)\n",
    "    ))\n",
    "    if remove_pth_file_after_load:\n",
    "        logging.info(f\"Removing pth-file {model_path}\")\n",
    "        os.remove(model_path)\n",
    "    model.eval()\n",
    "\n",
    "    # Access validation data\n",
    "    data = test_data.get_data_from_subset(test_data.val).to_numpy()\n",
    "    if not baseline:\n",
    "        _, customer_idx = np.unique(data[:, 0], return_index=True)\n",
    "        valid_customers = data[customer_idx, :]\n",
    "        valid_customers = valid_customers[:, [0, 2]]  # customer id and age\n",
    "        logging.debug(f\"Customers with age component,\\n{valid_customers}\")\n",
    "        _, article_idx = np.unique(data[:, 1], return_index=True)\n",
    "        valid_articles = data[article_idx, :]\n",
    "        valid_articles = valid_articles[:, [1, 3, 4]]  # article data\n",
    "    else:\n",
    "        valid_customers = np.unique(data[:, 0])\n",
    "        valid_articles = np.unique(data[:, 1])\n",
    "    valid_customers, valid_articles = torch.IntTensor(valid_customers).to(\n",
    "        device\n",
    "    ), torch.IntTensor(valid_articles).to(device)\n",
    "    from math import ceil\n",
    "\n",
    "    # Create batches of articles similar to the model batch size\n",
    "    valid_articles = np.array_split(\n",
    "        valid_articles, ceil(valid_articles.shape[0] / test_data.batch_size)\n",
    "    )\n",
    "    out_dict = dict()\n",
    "    logging.info(\"Computing predictions for customers ...\")\n",
    "    if not baseline:\n",
    "        iterate_through_extended_model(\n",
    "            all_preds\n",
    "        )  # model,valid_customers,test_data,n_customer_threshold,all_preds)\n",
    "    else:\n",
    "        logging.debug(\"Entered iteration through baseline data\")\n",
    "        customer_count = 0\n",
    "        for customer in tqdm(valid_customers):\n",
    "            customer_count += 1\n",
    "            for article in valid_articles:\n",
    "                # For customer 3, we pass model([3, ..., 3], [a1, ..., an])\n",
    "                customer_exp = customer.expand(article.shape[0])\n",
    "                pred = model(customer_exp, article).view(-1)\n",
    "                for i, pred_i in enumerate(pred):\n",
    "                    all_preds[customer.item()][article[i]] = pred_i\n",
    "            if customer_count % n_customer_threshold == 0:\n",
    "                # Clears all predictions again to save memory\n",
    "                logging.debug(\"Sending batch to out dict...\")\n",
    "                _update_out_dict(all_preds)\n",
    "                # Free memory and re-initiate defaultdict\n",
    "                del all_preds\n",
    "                all_preds = defaultdict(dict)\n",
    "\n",
    "    # Call this once more for the last n<n_customer_threshold customers\n",
    "    _update_out_dict(all_preds)\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def compute_map(\n",
    "    best_preds: dict,\n",
    "    test_data: Data_HM,\n",
    "    k: int = 12,\n",
    "    use_all_data_as_ground_truth: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute average precision @k for predicted entries\n",
    "    `out_dict` from previous function\n",
    "\n",
    "    Args:\n",
    "        best_preds (dict): Output dictionary from\n",
    "                topk_from_all_customers function\n",
    "        test_data (Data_HM): Dataset object\n",
    "        k (int, optional): Cutoff. Defaults to 12.\n",
    "        use_all_data_as_ground_truth (bool, optional): Bool flag, if False\n",
    "                will only consider validation set. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Average precision for all customers such that its\n",
    "        .mean() is the MAP@k value\n",
    "    \"\"\"\n",
    "    if use_all_data_as_ground_truth:\n",
    "        logging.debug(\"Returning best predicted values back to true IDs ...\")\n",
    "        decoded_preds = dict()\n",
    "        for key, v in best_preds.items():\n",
    "            unenc_k = test_data.le_cust.inverse_transform([key])[0]\n",
    "            unenc_v = test_data.le_art.inverse_transform(v[0])\n",
    "            decoded_preds[unenc_k] = [unenc_v]\n",
    "        best_preds = decoded_preds  # Overwrites best_preds\n",
    "\n",
    "        # Ground truth from entire database:\n",
    "        logging.debug(\"Reading ground truth ...\")\n",
    "        ground_truth = pd.read_pickle(\"object_storage/transactions.pckl\")\n",
    "        # # Alternative method if csv is stored to disk.\n",
    "        # ground_truth = pd.read_csv(\n",
    "        #     \"dataset/transactions_train.csv\",\n",
    "        #     dtype={\"article_id\": str},\n",
    "        #     usecols=[\"customer_id\", \"article_id\"],\n",
    "        # )\n",
    "\n",
    "        ground_truth = ground_truth[\n",
    "            ground_truth[\"customer_id\"].isin(list(best_preds.keys()))\n",
    "        ]\n",
    "        logging.debug(\"Grouping ground truth by customer\")\n",
    "        ground_truth = (\n",
    "            ground_truth\n",
    "            .groupby(\"customer_id\")\n",
    "            .agg({\"article_id\": list})\n",
    "            .reset_index()\n",
    "        )\n",
    "        logging.debug(f\"Head of ground_truth\\n{ground_truth.head()}\")\n",
    "    else:\n",
    "        logging.debug(\"Loading only data in validation set\")\n",
    "        # This only checks the true values of the (training and) validation set\n",
    "        ground_truth = (\n",
    "            test_data.df_id[test_data.df_id[\"label\"] == 1]\n",
    "            .groupby(\"customer_id\")\n",
    "            .agg({\"article_id\": list})\n",
    "            .reset_index()\n",
    "        )\n",
    "    # Load model predictions to dataframe\n",
    "    logging.debug(\"Converting best_preds to dataframe\")\n",
    "    preds = pd.DataFrame.from_dict(best_preds, orient=\"index\").reset_index()\n",
    "    preds.columns = [\"customer_id\", \"est_article_id\"]\n",
    "    logging.debug(f\"Head of preds\\n{preds.head()}\")\n",
    "\n",
    "    # Remove duplicates (i.e. where U bought V several times)\n",
    "    ground_truth[\"article_id\"] = ground_truth[\"article_id\"].apply(\n",
    "        lambda c: list(set(c))\n",
    "    )\n",
    "    logging.debug(\"Merging ground truth with preds\")\n",
    "    merged = ground_truth.merge(preds)\n",
    "    # Just having a look making sure everything looks good with the new thing\n",
    "    logging.debug(f\"Columns of merged DF: {merged.columns}\")\n",
    "    logging.debug(\n",
    "        f\"Merged has length {len(merged)} and preds had {len(preds)}.\"\n",
    "    )\n",
    "\n",
    "    logging.debug(\"Computing MAP ...\")\n",
    "    # Compute average precision here\n",
    "    average_precision = np.zeros(merged.shape[0])\n",
    "    for row_num, row in tqdm(merged.iterrows()):\n",
    "        ap = 0\n",
    "        truth = np.array(row.article_id)\n",
    "        preds = np.array(row.est_article_id)\n",
    "        for i in range(1, k + 1):\n",
    "            if preds[i - 1] in truth:\n",
    "                ap = ap + len(np.intersect1d(preds[:i], truth)) / i\n",
    "        average_precision[row_num] = ap / min(\n",
    "            k, len(row.est_article_id), len(row.article_id)\n",
    "        )\n",
    "\n",
    "    # Returns average precision for each customer instead of taking the mean,\n",
    "    # which gives us MAP@k\n",
    "    return average_precision"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main method\n",
    "\n",
    "Finally, we make some wrappers for easier use, and define different actions that may be of interest. We requrie every call to include an `-action` flag, referencing different functions of interest. These are wrapped with a custom decorator ensuring to log any exception to the log file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_except_action(action):\n",
    "    \"\"\"Customer that encapsulates (void) function in try-except block\"\"\"\n",
    "\n",
    "    def wrapper():\n",
    "        action_name = action.__name__\n",
    "        try:\n",
    "            action()\n",
    "        except Exception as e:\n",
    "            logging.exception(f\"Exception in {action_name}!\\n{e}\")\n",
    "        else:\n",
    "            logging.deug(\n",
    "                f\"Successfully done with action {action_name}. Exiting ...\"\n",
    "                )\n",
    "        finally:\n",
    "            return None\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@try_except_action\n",
    "def action_hyperparams():\n",
    "    logging.debug(\"Starting param exploration for extended model\")\n",
    "\n",
    "    alternative_hyperparam_exploration(\n",
    "        wds=(1e-4, 1e-3, 1e-1),\n",
    "        embszs=(500, 100, 1000),\n",
    "        lrs=(1e-2, 1e-3, 1e-4),\n",
    "        dataset_path=\"object_storage/dataset-2022.11.26.12.04.pckl\",\n",
    "        baseline=False,\n",
    "    )\n",
    "    logging.debug(\"Starting hyperparameter exploration of baseline\")\n",
    "    alternative_hyperparam_exploration(\n",
    "        wds=(1e-4, 1e-3, 1e-1),\n",
    "        embszs=(500, 100, 1000),\n",
    "        lrs=(1e-2, 1e-3, 1e-4),\n",
    "        dataset_path=\"object_storage/dataset-2022.11.26.12.04.pckl\",\n",
    "        baseline=True,\n",
    "    )\n",
    "\n",
    "\n",
    "@try_except_action\n",
    "def action_map():\n",
    "    logging.debug(\"Computing MAP for model\")\n",
    "    data = read_dataset_obj(\n",
    "        \"object_storage/dataset-2022.11.26.12.04.pckl\"\n",
    "    )  # 200k samples\n",
    "    # # Computes MAP for aggregation predictions first.\n",
    "    # First: aggregation not index group-based\n",
    "    res_simple_agg_full = compute_map(\n",
    "        predictions_aggregator(data, by_index=False), data, 12, True\n",
    "    )\n",
    "    res_simple_agg_valid = compute_map(\n",
    "        predictions_aggregator(data, by_index=False), data, 12, False\n",
    "    )\n",
    "    logging.info(\n",
    "        f\"With all data: {res_simple_agg_full.mean()}, with validation data:\" +\n",
    "        f\" {res_simple_agg_valid.mean()}.\"\n",
    "    )\n",
    "\n",
    "    res = compute_map(predictions_aggregator(data), data, 12, True)\n",
    "    logging.info(f\"With all data: {res.mean()}\")\n",
    "    res = compute_map(predictions_aggregator(data), data, 12, False)\n",
    "    logging.info(f\"With validation only: {res.mean()}\")\n",
    "\n",
    "    for name, model in zip(\n",
    "        [\"baseline\", \"extended\"],\n",
    "        [\n",
    "            \"1L8VmsQ3dRedgheUIAREvLBuH0PFxwTG8\",\n",
    "            \"1-t_jh7ajCUZRSm2EwUK4cLqAcZ2-Q6zK\"\n",
    "        ],\n",
    "    ):\n",
    "        baseline_bool = name == \"baseline\"\n",
    "        logging.debug(f\"Downloading {name} model from drive...\")\n",
    "        mod_weights_path = load_from_gdrive(gd_id=model)\n",
    "        from time import perf_counter\n",
    "\n",
    "        time_start = perf_counter()\n",
    "        predictions_dict = topk_for_all_customers(\n",
    "            mod_weights_path,\n",
    "            test_data=data,\n",
    "            n_customer_threshold=200,\n",
    "            remove_pth_file_after_load=True,\n",
    "            baseline=baseline_bool,\n",
    "        )\n",
    "        logging.info(\n",
    "            \"Time spent finding the top k for \" +\n",
    "            f\"{'baseline' if baseline_bool else 'extended'} model: \" +\n",
    "            f\"{perf_counter() - time_start} seconds.\"\n",
    "        )\n",
    "        # Seemed to be OS-related issues to storing this, hence the try block\n",
    "        try:\n",
    "            save_dataset_obj(\n",
    "                predictions_dict, \"object_storage/preds-dec18.pckl\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.exception(\n",
    "                f\"Unable to store predictions dict for some reason!\\n{e}\"\n",
    "            )\n",
    "            logging.exception(\"Using fallback which is to paste all data:\")\n",
    "            logging.debug(f\"\\n{predictions_dict}\")\n",
    "        logging.debug(\"Finished making predictions, now computing MAP\")\n",
    "        average_precision = compute_map(\n",
    "            predictions_dict, data, use_all_data_as_ground_truth=False\n",
    "        )\n",
    "        logging.info(f\"Computed MAP: {average_precision.mean()}\")\n",
    "        average_precision = compute_map(\n",
    "            predictions_dict, data, use_all_data_as_ground_truth=True\n",
    "        )\n",
    "        logging.info(\n",
    "            f\"Now with full data: Computed MAP: {average_precision.mean()}\"\n",
    "        )\n",
    "\n",
    "\n",
    "@try_except_action\n",
    "def action_savemodel():\n",
    "    logging.debug(\"Training model to save\")\n",
    "    hyperparams = Hyperparameters(\n",
    "        lr_rate=1e-2,\n",
    "        weight_decay=1e-4,\n",
    "        embedding_size=500,\n",
    "        save_loss=\"baseline200k\",\n",
    "        verbose=True,\n",
    "    )\n",
    "    logging.debug(f\"With params {hyperparams.__dict__}\")\n",
    "    load_dataset_and_train(\n",
    "        persisted_dataset_path=\"object_storage/dataset-2022.11.26.12.04.pckl\",\n",
    "        save_model=True,\n",
    "        hyperparams=hyperparams,\n",
    "        baseline=True,\n",
    "    )\n",
    "\n",
    "\n",
    "@try_except_action\n",
    "def action_fulldata():\n",
    "    # Baseline with full data - a couple of different settings\n",
    "    load_dataset_and_train(\n",
    "        transactions_path=\"object_storage/transactions.pckl\",\n",
    "        hyperparams=Hyperparameters(\n",
    "            lr_rate=1e-2,\n",
    "            min_lr=1e-3,\n",
    "            epochs=10,\n",
    "            weight_decay=0,\n",
    "            dataset_cases=1_000_000,  # 2 * 31_788_324,\n",
    "            dataset_portion_negatives=0.5,\n",
    "            dataset_train_portion=0.7,\n",
    "            dataset_batch_size=128,\n",
    "            dataset_full=True,\n",
    "            bias_nodes=False,\n",
    "            save_loss=\"NO_BIAS\",\n",
    "        ),\n",
    "    )\n",
    "    load_dataset_and_train(\n",
    "        transactions_path=\"object_storage/transactions.pckl\",\n",
    "        hyperparams=Hyperparameters(\n",
    "            lr_rate=1e-2,\n",
    "            min_lr=1e-3,\n",
    "            epochs=10,\n",
    "            weight_decay=0,\n",
    "            dataset_cases=1_000_000,  # 2 * 31_788_324,\n",
    "            dataset_portion_negatives=0.5,\n",
    "            dataset_train_portion=0.7,\n",
    "            dataset_batch_size=128,\n",
    "            dataset_full=True,\n",
    "            bias_nodes=True,\n",
    "            save_loss=\"WITH_BIAS\",\n",
    "        ),\n",
    "    )\n",
    "    load_dataset_and_train(\n",
    "        transactions_path=\"object_storage/transactions.pckl\",\n",
    "        hyperparams=Hyperparameters(\n",
    "            lr_rate=1e-4,\n",
    "            min_lr=1e-3,\n",
    "            epochs=10,\n",
    "            weight_decay=0,\n",
    "            dataset_cases=1_000_000,  # 2 * 31_788_324,\n",
    "            dataset_portion_negatives=0.5,\n",
    "            dataset_train_portion=0.7,\n",
    "            dataset_batch_size=128,\n",
    "            dataset_full=True,\n",
    "            bias_nodes=False,\n",
    "        ),\n",
    "    )\n",
    "    load_dataset_and_train(\n",
    "        transactions_path=\"object_storage/transactions.pckl\",\n",
    "        hyperparams=Hyperparameters(\n",
    "            lr_rate=1e-5,\n",
    "            min_lr=1e-3,\n",
    "            epochs=10,\n",
    "            weight_decay=0,\n",
    "            dataset_cases=1_000_000,  # 2 * 31_788_324,\n",
    "            dataset_portion_negatives=0.5,\n",
    "            dataset_train_portion=0.7,\n",
    "            dataset_batch_size=128,\n",
    "            dataset_full=True,\n",
    "            bias_nodes=False,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "@try_except_action\n",
    "def action_baseline():\n",
    "    logging.debug(\"Starting training of baseline - with some weight analysis\")\n",
    "    df0 = pd.read_pickle(\"object_storage/transactions.pckl\")\n",
    "    # Current data loads full dataset\n",
    "    data = Data_HM_Complete(\n",
    "        total_cases=2 * 31_788_324,\n",
    "        portion_negatives=0.5,\n",
    "        df_transactions=df0,\n",
    "        batch_size=128,\n",
    "        train_portion=0.7,\n",
    "    )\n",
    "    # Baseline model with \\gamma = 0.01, \\lambda = 0, n_emb = 500,\n",
    "    # no bias nodes, embeddings sparse, sparseAdam optimizer\n",
    "    model = load_model(\n",
    "        baseline=True, data=data, emb_sz=500, bias=False, sparse=True\n",
    "    )\n",
    "    hyperparams = Hyperparameters(\n",
    "        lr_rate=0.01,\n",
    "        epochs=5,\n",
    "        optimizer=\"SparseAdam\",\n",
    "        lossfnc=torch.nn.MSELoss,\n",
    "        weight_decay=0,\n",
    "        embedding_size=500,\n",
    "        save_loss=\"sparseAdam\",\n",
    "        bias_nodes=False,\n",
    "        verbose=True,\n",
    "        dataset_full=True,\n",
    "        min_lr=0.01,  # So that LR don't decrement dynamically\n",
    "    )\n",
    "    logging.debug(\n",
    "        f\"Starting training of model with parameters {hyperparams.__dict__}\"\n",
    "        )\n",
    "    last_validation_loss = train(\n",
    "        model, data, hyperparams, baseline=True, plot_loss=True\n",
    "    )\n",
    "    logging.debug(\n",
    "        \"Model done with training (and loss plots stored to disk). \" +\n",
    "        f\"Last validation loss {last_validation_loss}\"\n",
    "    )\n",
    "    # Prints info on the weights to log\n",
    "    for parameter in model.named_parameters():\n",
    "        max_value = parameter[1].detach().numpy().max()\n",
    "        logging.info(f\"Parameter {parameter[0]} has max value {max_value}\")\n",
    "\n",
    "    del model  # Ensuring the weights aren't part of new model\n",
    "    model = load_model(baseline=True, data=data, emb_sz=500, bias=False)\n",
    "    hyperparams = Hyperparameters(\n",
    "        lr_rate=1e-2,\n",
    "        weight_decay=0,\n",
    "        embedding_size=500,\n",
    "        save_loss=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    logging.debug(\n",
    "        f\"Starting training of model with parameters {hyperparams.__dict__}\"\n",
    "        )\n",
    "    last_validation_loss = train(\n",
    "        model, data, hyperparams, baseline=True, plot_loss=True\n",
    "    )\n",
    "    logging.debug(\n",
    "        \"Second model done with training (and loss plots stored to disk)\"\n",
    "        )\n",
    "\n",
    "    # Prints info on the weights to log\n",
    "    for parameter in model.named_parameters():\n",
    "        max_value = parameter[1].detach().numpy().max()\n",
    "        logging.info(f\"Parameter {parameter[0]} has max value {max_value}\")\n",
    "\n",
    "    logging.debug(\"Finished action sucessfully. Exiting ...\")\n",
    "\n",
    "\n",
    "@try_except_action\n",
    "def action_predvalues():\n",
    "    \"\"\"We load the model and predictions, and re-evaluate the model\n",
    "    for the best predicitons to AFAP find the model's confidence\"\"\"\n",
    "    data = read_dataset_obj(\"object_storage/dataset-2022.11.26.12.04.pckl\")\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    baseline = True\n",
    "    model = load_model(baseline=baseline, data=data, emb_sz=500, bias=True)\n",
    "    mod_weights_path = load_from_gdrive(\n",
    "        gd_id=\"1L8VmsQ3dRedgheUIAREvLBuH0PFxwTG8\"\n",
    "        )\n",
    "    model.load_state_dict(\n",
    "        torch.load(mod_weights_path, map_location=torch.device(device))\n",
    "    )\n",
    "    os.remove(mod_weights_path)\n",
    "    with open(\"object_storage/preds-dec6.pckl\", \"rb\") as f:\n",
    "        best_predictions = pickle.load(f)\n",
    "    prediction_values = {}\n",
    "    logging.debug(\"All stuff has been loaded sucessfully.\")\n",
    "\n",
    "    model.eval()\n",
    "    for customer_id, article_ids in best_predictions.items():\n",
    "        article_tensor = torch.IntTensor(article_ids[0])  # Flatten list\n",
    "        customer_tensor = torch.IntTensor([customer_id]).expand(\n",
    "            article_tensor.shape[0]\n",
    "        )\n",
    "        prediction = model(customer_tensor, article_tensor)\n",
    "        # Make 12-length list\n",
    "        prediction_values[customer_id] = prediction.detach().numpy()\n",
    "    logging.debug(\n",
    "        \"Done with finding predictions. Creating plot of confidence.\"\n",
    "        )\n",
    "\n",
    "    averages = []\n",
    "    maxes = []\n",
    "    for predictions in prediction_values.values():\n",
    "        averages.append(predictions.mean())\n",
    "        maxes.append(predictions.max())\n",
    "    plt.plot(averages, \"o\", color=\"grey\", markersize=1, label=\"Average value\")\n",
    "    plt.plot(maxes, \"ro\", markersize=1, label=\"Max value\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Customer index ID\")\n",
    "    plt.ylabel(\"Confidence (0%-100%)\")\n",
    "    save_path = \"average_prediction_confidence.png\"\n",
    "    plt.savefig(save_path)\n",
    "    logging.debug(f\"Saved plot to {save_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    logging.basicConfig(\n",
    "        filename=\"ssh_run.log\",\n",
    "        # encoding=\"utf-8\", # Not present in Python 3.8 so commented out\n",
    "        level=logging.DEBUG, # Change this to not get all debug messages\n",
    "        format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "    )\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-action\", help=\"What type of action to run...\")\n",
    "    args = parser.parse_args()\n",
    "    action = args.action\n",
    "    logging.debug(f\"4-shh-compatible called with action flag: {action}\")\n",
    "    possible_actions = (\n",
    "        \"(hyperparams, map, savemodel, baseline, fulldata, predvalues)\"\n",
    "    )\n",
    "\n",
    "    if action == \"hyperparams\":\n",
    "        action_hyperparams()\n",
    "    elif action == \"map\":\n",
    "        action_map()\n",
    "    elif action == \"savemodel\":\n",
    "        action_savemodel()\n",
    "    elif action == \"baseline\":\n",
    "        action_baseline()\n",
    "    elif action == \"fulldata\":\n",
    "        action_fulldata()\n",
    "    elif action == \"predvalues\":\n",
    "        action_predvalues()\n",
    "    elif action is None:\n",
    "        print(f\"Please provide -action flag when running {possible_actions}\")\n",
    "    else:\n",
    "        print(f\"Action not recognized. Possible actions: {possible_actions}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1 (tags/v3.10.1:2cd268a, Dec  6 2021, 19:10:37) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04a4c663c1c9b43728e49bf64ea34e4d1fe986a9327a4efda5dcdc8b739aba35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
