{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext jupyter_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaways from the dataset**\n",
    "\n",
    "* Some articles have no image\n",
    "* Some customers don't buy anything\n",
    "* The complete transaction data has 31 788 325 rows, just short of 32 million (!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import Union, Tuple\n",
    "from types import NoneType\n",
    "import random, shutil, os, itertools, black, jupyter_black\n",
    "\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling methods\n",
    "\n",
    "We need to be able to pull out realistic samples of the dataset. To do this, we first sample $n$ customers at random and include every transaction that they have done - these are the positive labels. In addition, we want to obtain additional transactions that are not related to the customers in the sample, working as a negative label. We implement this by saying that $k$% of the data are true labels, defaulting $k=10$%. Lastly, we pull out the article IDs in all the transactions and obtain the images for said article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_csv_sampler(\n",
    "    csv_path: str,\n",
    "    sample_size: int,\n",
    "    num_records: int | NoneType = None,\n",
    "    header: str | NoneType = \"infer\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Read samples of rows from csv file\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to file including file extensions\n",
    "        sample_size (int): Number of rows to sample\n",
    "        num_records (int | NoneType, optional): Total records in file, defaults to None. If None, the file will be scanned (costly)\n",
    "        header (str | NoneType, optional): 'header'-parameter for pandas, defaults to 'infer'. Set to None if file has no header.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with sampled entries (and potentially header)\n",
    "    \"\"\"\n",
    "    if num_records is None:\n",
    "        num_records = newlines_in_csv(csv_path)\n",
    "    indices_skip = sorted(\n",
    "        random.sample(range(1, num_records + 1), num_records - sample_size)\n",
    "    )\n",
    "    return pd.read_csv(csv_path, skiprows=indices_skip, header=header)\n",
    "\n",
    "\n",
    "def newlines_in_csv(csv_path: str, chunk_size: int = 1024) -> int:\n",
    "    \"\"\"Counts number of newlines in csv file without loading entire file to memory.\n",
    "    The number of newlines is the same as number of rows assuming,\n",
    "        * EITHER csv has a header and last entry does not end with newline\n",
    "        * OR csv does not have a header, but last entry ends with newline\n",
    "        * ALWAYS data does not have any nested newline madness\n",
    "    Originally from orlp, https://stackoverflow.com/a/64744699\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path of csv file\n",
    "        chunk_size (int, optional): How many KB to process at at a time. Defaults to 1024 = 1 MB.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of newlines\n",
    "    \"\"\"\n",
    "    chunk = chunk_size**2\n",
    "    f = np.memmap(csv_path)\n",
    "    number_newlines = sum(\n",
    "        np.sum(f[i : i + chunk] == ord(\"\\n\")) for i in range(0, len(f), chunk)\n",
    "    )\n",
    "    del f\n",
    "    return number_newlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_img_from_article(df: pd.DataFrame, outpath):\n",
    "    for id in df['article_id']:\n",
    "        id0 = \"0\" + str(id)\n",
    "        img_path = f\"./dataset/images/{id0[:3]}/{id0}.jpg\"\n",
    "        if not os.path.isfile(img_path):\n",
    "            continue # ID has no image (happens for some cases)\n",
    "        out_dir = f\"./{outpath}/images/{id0[:3]}/\"\n",
    "        if not os.path.isdir(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "            shutil.copy(img_path, out_dir)\n",
    "copy_img_from_article(df_art, \"dataset_sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up the datasets\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "def load_min_data(filename: str | Iterable):\n",
    "    dfs = []\n",
    "    if isinstance(filename, str):\n",
    "        filename = [filename]\n",
    "    for fn in filename:\n",
    "        df = pd.read_csv(fn)\n",
    "        # All min-datasets have an index column which has to be dropped:\n",
    "        dfs.append(df.drop(df.columns[0], axis=1))\n",
    "    return dfs\n",
    "\n",
    "def clean_customer_data(df):\n",
    "    # df = df.drop(\"FN\", axis=1) # I they're not exactly equal\n",
    "    df.loc[\n",
    "        ~df[\"fashion_news_frequency\"].isin([\"Regularly\", \"Monthly\"]),\n",
    "        \"fashion_news_frequency\",\n",
    "    ] = \"None\"\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New data loading principle\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class Data_HM(Dataset):\n",
    "    \"\"\"This is the general HM Dataset class whose children are train-dataset and validation-dataset\n",
    "\n",
    "    Args:\n",
    "        Dataset: Abstract Dataset class from pyTorch\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        total_cases: int,\n",
    "        portion_negatives: float,\n",
    "        df_transactions: pd.DataFrame,\n",
    "        df_articles: pd.DataFrame,\n",
    "        df_customers: pd.DataFrame,\n",
    "        train_portion: float | None = None,\n",
    "        test_portion: float | None = None,\n",
    "    ) -> None:\n",
    "        super().__init__()  # TODO not sure if we need this\n",
    "        self.pos, self.neg = self.generate_dataset(\n",
    "            total_cases, portion_negatives, df_transactions\n",
    "        )\n",
    "        self.df = pd.concat(\n",
    "            [\n",
    "                self.merge_dfs_add_label(\n",
    "                    self.pos,\n",
    "                    df_articles,\n",
    "                    df_customers,\n",
    "                    positive=True,\n",
    "                ),\n",
    "                self.merge_dfs_add_label(\n",
    "                    self.neg,\n",
    "                    df_articles,\n",
    "                    df_customers,\n",
    "                    positive=False,\n",
    "                ),\n",
    "            ]\n",
    "        ).reset_index(drop=True)\n",
    "        self.train, self.test = self.split(train_portion, test_portion)\n",
    "\n",
    "    def generate_dataset(\n",
    "        self, total_cases: int, portion_negatives: float, df_transactions: pd.DataFrame\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Produce DataFrames for positive labels and generated negative samples\n",
    "\n",
    "        Args:\n",
    "            total_cases (int): Total number of transactions\n",
    "            portion_negatives (float): The portion of the `total_cases` that should be negative. Balanced 0/1 when 0.5\n",
    "            df_transactions (pd.DataFrame): Transactions to pull samples/generate samples from\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: _description_\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            0 <= portion_negatives <= 1\n",
    "        ), r\"portion negatives must be a float between 0%=0.0 and 100%=1.0!\"\n",
    "        n_positive = int(total_cases * (1 - portion_negatives))\n",
    "        n_negative = int(total_cases * portion_negatives)\n",
    "        df_positive = df_transactions.sample(n=n_positive).reset_index(drop=True)\n",
    "        df_positive = df_positive[[\"customer_id\", \"article_id\"]]\n",
    "\n",
    "        \n",
    "        # Sampling negative labels:\n",
    "        #   We select a random combination of `customer_id`, `article_id`, and ensure that this is not a true transaction.\n",
    "        #   Then we write this tuple to a csv which is transformed into a DataFrame similar to `df_positive`\n",
    "\n",
    "        num_written = 0\n",
    "        tmpStr = \"customer_id,article_id\\n\"\n",
    "        while num_written < n_negative:\n",
    "            # Choose random customer and article\n",
    "            selection = np.array(  # TODO this can probably be optimized further\n",
    "                [\n",
    "                    df_transactions[\"customer_id\"].sample().values,\n",
    "                    df_transactions[\"article_id\"].sample().values,\n",
    "                ]\n",
    "            ).flatten()\n",
    "            if not (\n",
    "                (df_transactions[\"customer_id\"] == selection[0])\n",
    "                & (df_transactions[\"article_id\"] == selection[1])\n",
    "            ).any():\n",
    "                tmpStr += f\"{selection[0]}, {selection[1]}\\n\"\n",
    "                num_written += 1\n",
    "        with open(\"tmp.csv\", \"w\") as f:\n",
    "            f.write(tmpStr)\n",
    "        df_negative = pd.read_csv(\"tmp.csv\")\n",
    "        os.remove(\"tmp.csv\")\n",
    "        return df_positive, df_negative\n",
    "\n",
    "    def merge_dfs_add_label(\n",
    "        self, df_transactions: pd.DataFrame, df_articles: pd.DataFrame, df_customers: pd.DataFrame, positive: bool = False\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Merge customer and article data to the sampled data `df_transactions`, excluding customer/article IDs\n",
    "\n",
    "        Args:\n",
    "            df_transactions (pd.DataFrame): DataFrame from `generate_dataset`\n",
    "            df_articles (pd.DataFrame): Articles DataFrame\n",
    "            df_customers (pd.DataFrame): Customers DataFrame\n",
    "            positive (bool, optional): Wether or not df_transactions represent positive labels. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DF with all columns included\n",
    "        \"\"\"\n",
    "        columns_articles = [\n",
    "            \"article_id\",\n",
    "            \"prod_name\",\n",
    "            \"product_type_name\",\n",
    "            \"product_group_name\",\n",
    "            \"graphical_appearance_name\",\n",
    "            \"colour_group_name\",\n",
    "            \"perceived_colour_value_name\",\n",
    "            \"perceived_colour_master_name\",\n",
    "            \"department_name\",\n",
    "            \"index_name\",\n",
    "            \"index_group_name\",\n",
    "            \"section_name\",\n",
    "            \"garment_group_name\",\n",
    "            \"detail_desc\",\n",
    "        ]\n",
    "        # TODO consider storing blacklisted cols instead of whitelisted\n",
    "\n",
    "        df_articles = df_articles[columns_articles]\n",
    "\n",
    "        df = pd.merge(\n",
    "            df_transactions, df_customers, how=\"inner\", on=[\"customer_id\"]\n",
    "        ).drop([\"customer_id\"], axis=1)\n",
    "        df = pd.merge(df, df_articles, how=\"inner\", on=[\"article_id\"]).drop(\n",
    "            [\"article_id\"], axis=1\n",
    "        )\n",
    "        df[\"label\"] = 1 if positive else 0\n",
    "        return df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row, label = self.df.iloc[idx, :-1], self.df.iloc[idx, -1]\n",
    "        if self.transform:\n",
    "            row = self.transform(row)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return row, label\n",
    "\n",
    "    def split(\n",
    "        self, train_portion: float | None = None, test_portion: float | None = None\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Split full dataset into training and validation set. Note that only one of train_portion or\n",
    "            test_portion are required (test_portion = 100% - test_portion)\n",
    "\n",
    "        Args:\n",
    "            train_portion (float | None, optional): Percentage of rows assigned to training set. Defaults to None.\n",
    "            test_portion (float | None, optional): Percentage of rows assigned to validation set. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: Train-set and validation-set\n",
    "        \"\"\"\n",
    "        assert any(\n",
    "            [train_portion, test_portion]\n",
    "        ), \"At least one of train or test portion must be float\"\n",
    "        if train_portion is None:\n",
    "            train_portion = 1-test_portion\n",
    "        train = self.df.sample(frac=train_portion)\n",
    "        test = (\n",
    "            pd.merge(self.df, train, indicator=True, how=\"outer\")\n",
    "            .query('_merge==\"left_only\"')\n",
    "            .drop(\"_merge\", axis=1)\n",
    "        )\n",
    "        return train.reset_index(drop=True), test.reset_index(drop=True)\n",
    "\n",
    "\n",
    "class HM_train(Data_HM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        total_cases,\n",
    "        portion_negatives,\n",
    "        df_transactions,\n",
    "        df_articles,\n",
    "        df_customers: pd.DataFrame,\n",
    "        train_portion=None,\n",
    "        test_portion=None,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            total_cases,\n",
    "            portion_negatives,\n",
    "            df_transactions,\n",
    "            df_articles,\n",
    "            df_customers,\n",
    "            train_portion,\n",
    "            test_portion,\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row, label = self.train.iloc[idx, :-1], self.train.iloc[idx, -1]\n",
    "        if self.transform:\n",
    "            row = self.transform(row)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return row, label\n",
    "\n",
    "\n",
    "class HM_val(Data_HM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        total_cases,\n",
    "        portion_negatives,\n",
    "        df_transactions,\n",
    "        df_articles,\n",
    "        df_customers: pd.DataFrame,\n",
    "        train_portion=None,\n",
    "        test_portion=None,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            total_cases,\n",
    "            portion_negatives,\n",
    "            df_transactions,\n",
    "            df_articles,\n",
    "            df_customers,\n",
    "            train_portion,\n",
    "            test_portion,\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row, label = self.test.iloc[idx, :-1], self.test.iloc[idx, -1]\n",
    "        if self.transform:\n",
    "            row = self.transform(row)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return row, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FN</th>\n",
       "      <th>Active</th>\n",
       "      <th>club_member_status</th>\n",
       "      <th>fashion_news_frequency</th>\n",
       "      <th>age</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>prod_name</th>\n",
       "      <th>product_type_name</th>\n",
       "      <th>product_group_name</th>\n",
       "      <th>graphical_appearance_name</th>\n",
       "      <th>colour_group_name</th>\n",
       "      <th>perceived_colour_value_name</th>\n",
       "      <th>perceived_colour_master_name</th>\n",
       "      <th>department_name</th>\n",
       "      <th>index_name</th>\n",
       "      <th>index_group_name</th>\n",
       "      <th>section_name</th>\n",
       "      <th>garment_group_name</th>\n",
       "      <th>detail_desc</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>None</td>\n",
       "      <td>47.0</td>\n",
       "      <td>e28a643283c2b517d3e63d17291ae4b339d79827e32dd3...</td>\n",
       "      <td>Tilly</td>\n",
       "      <td>T-shirt</td>\n",
       "      <td>Garment Upper body</td>\n",
       "      <td>All over pattern</td>\n",
       "      <td>Off White</td>\n",
       "      <td>Light</td>\n",
       "      <td>White</td>\n",
       "      <td>Jersey Basic</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Womens Everyday Basics</td>\n",
       "      <td>Jersey Basic</td>\n",
       "      <td>T-shirt in lightweight jersey with a rounded h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>Regularly</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2887d1d82d975bf335461f8c87ce835677844b5cba34c5...</td>\n",
       "      <td>Magaluf lace detail dress</td>\n",
       "      <td>Dress</td>\n",
       "      <td>Garment Full body</td>\n",
       "      <td>Solid</td>\n",
       "      <td>Black</td>\n",
       "      <td>Dark</td>\n",
       "      <td>Black</td>\n",
       "      <td>Dress</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Womens Casual</td>\n",
       "      <td>Dresses Ladies</td>\n",
       "      <td>Knee-length dress in a viscose weave with embr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>Regularly</td>\n",
       "      <td>34.0</td>\n",
       "      <td>fe814f9d1b8a657f20a30666c230cba0ad41a3172d5a6d...</td>\n",
       "      <td>Everlacing Love Top</td>\n",
       "      <td>Bikini top</td>\n",
       "      <td>Swimwear</td>\n",
       "      <td>Stripe</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Swimwear</td>\n",
       "      <td>Lingeries/Tights</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Womens Swimwear, beachwear</td>\n",
       "      <td>Swimwear</td>\n",
       "      <td>Fully lined bikini top with lacing at the fron...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>Regularly</td>\n",
       "      <td>31.0</td>\n",
       "      <td>9c97073caea2e274c9ef34e0d90e43fb40078bcb9d15db...</td>\n",
       "      <td>SUPREME RW tights</td>\n",
       "      <td>Leggings/Tights</td>\n",
       "      <td>Garment Lower body</td>\n",
       "      <td>Colour blocking</td>\n",
       "      <td>Grey</td>\n",
       "      <td>Dark</td>\n",
       "      <td>Grey</td>\n",
       "      <td>Ladies Sport Bottoms</td>\n",
       "      <td>Sport</td>\n",
       "      <td>Sport</td>\n",
       "      <td>Ladies H&amp;M Sport</td>\n",
       "      <td>Jersey Fancy</td>\n",
       "      <td>Sports tights in fast-drying functional fabric...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>None</td>\n",
       "      <td>31.0</td>\n",
       "      <td>71b88711bd37db08ac1549d73432852f664cb48be8d893...</td>\n",
       "      <td>Trudy Cardigan</td>\n",
       "      <td>Cardigan</td>\n",
       "      <td>Garment Upper body</td>\n",
       "      <td>Solid</td>\n",
       "      <td>Light Green</td>\n",
       "      <td>Dusty Light</td>\n",
       "      <td>Green</td>\n",
       "      <td>Tops Knitwear</td>\n",
       "      <td>Divided</td>\n",
       "      <td>Divided</td>\n",
       "      <td>Divided Collection</td>\n",
       "      <td>Knitwear</td>\n",
       "      <td>Short, fitted cardigan in a fine knit with a V...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>None</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0f23f9b1e451204de97aca27b98e60c089c8f7ed13e6d8...</td>\n",
       "      <td>Lee (1)</td>\n",
       "      <td>Top</td>\n",
       "      <td>Garment Upper body</td>\n",
       "      <td>Melange</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Medium Dusty</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Jersey Basic</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Womens Everyday Basics</td>\n",
       "      <td>Jersey Basic</td>\n",
       "      <td>Long-sleeved top in soft jersey.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    FN  Active club_member_status fashion_news_frequency   age  \\\n",
       "0  1.0     1.0             ACTIVE                   None  47.0   \n",
       "1  1.0     1.0             ACTIVE              Regularly  22.0   \n",
       "2  1.0     1.0             ACTIVE              Regularly  34.0   \n",
       "3  1.0     1.0             ACTIVE              Regularly  31.0   \n",
       "4  NaN     NaN             ACTIVE                   None  31.0   \n",
       "5  NaN     NaN             ACTIVE                   None  48.0   \n",
       "\n",
       "                                         postal_code  \\\n",
       "0  e28a643283c2b517d3e63d17291ae4b339d79827e32dd3...   \n",
       "1  2887d1d82d975bf335461f8c87ce835677844b5cba34c5...   \n",
       "2  fe814f9d1b8a657f20a30666c230cba0ad41a3172d5a6d...   \n",
       "3  9c97073caea2e274c9ef34e0d90e43fb40078bcb9d15db...   \n",
       "4  71b88711bd37db08ac1549d73432852f664cb48be8d893...   \n",
       "5  0f23f9b1e451204de97aca27b98e60c089c8f7ed13e6d8...   \n",
       "\n",
       "                   prod_name product_type_name  product_group_name  \\\n",
       "0                      Tilly           T-shirt  Garment Upper body   \n",
       "1  Magaluf lace detail dress             Dress   Garment Full body   \n",
       "2        Everlacing Love Top        Bikini top            Swimwear   \n",
       "3          SUPREME RW tights   Leggings/Tights  Garment Lower body   \n",
       "4             Trudy Cardigan          Cardigan  Garment Upper body   \n",
       "5                    Lee (1)               Top  Garment Upper body   \n",
       "\n",
       "  graphical_appearance_name colour_group_name perceived_colour_value_name  \\\n",
       "0          All over pattern         Off White                       Light   \n",
       "1                     Solid             Black                        Dark   \n",
       "2                    Stripe              Blue                      Medium   \n",
       "3           Colour blocking              Grey                        Dark   \n",
       "4                     Solid       Light Green                 Dusty Light   \n",
       "5                   Melange              Blue                Medium Dusty   \n",
       "\n",
       "  perceived_colour_master_name       department_name        index_name  \\\n",
       "0                        White          Jersey Basic        Ladieswear   \n",
       "1                        Black                 Dress        Ladieswear   \n",
       "2                         Blue              Swimwear  Lingeries/Tights   \n",
       "3                         Grey  Ladies Sport Bottoms             Sport   \n",
       "4                        Green         Tops Knitwear           Divided   \n",
       "5                         Blue          Jersey Basic        Ladieswear   \n",
       "\n",
       "  index_group_name                section_name garment_group_name  \\\n",
       "0       Ladieswear      Womens Everyday Basics       Jersey Basic   \n",
       "1       Ladieswear               Womens Casual     Dresses Ladies   \n",
       "2       Ladieswear  Womens Swimwear, beachwear           Swimwear   \n",
       "3            Sport            Ladies H&M Sport       Jersey Fancy   \n",
       "4          Divided          Divided Collection           Knitwear   \n",
       "5       Ladieswear      Womens Everyday Basics       Jersey Basic   \n",
       "\n",
       "                                         detail_desc  label  \n",
       "0  T-shirt in lightweight jersey with a rounded h...      1  \n",
       "1  Knee-length dress in a viscose weave with embr...      0  \n",
       "2  Fully lined bikini top with lacing at the fron...      0  \n",
       "3  Sports tights in fast-drying functional fabric...      0  \n",
       "4  Short, fitted cardigan in a fine knit with a V...      0  \n",
       "5                   Long-sleeved top in soft jersey.      0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset = Data_HM(20, 0.8, df_t, df_a, df_c, train_portion=0.7)\n",
    "my_test = HM_val(20, 0.8, df_t, df_a, df_c, train_portion=0.7)\n",
    "my_test.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = naive_csv_sampler(\"dataset/transactions_train.csv\", sample_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_dat</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>aaf7a4cf881cc71b8cf97cd8e9c88ce300eb4fe2a279de...</td>\n",
       "      <td>649445003</td>\n",
       "      <td>0.059305</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-09-21</td>\n",
       "      <td>31287b3d29b025cf00822b66b462a415e9c58d65385627...</td>\n",
       "      <td>620337036</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-09-23</td>\n",
       "      <td>04ebf0daa6de941f870109b5536bc226f574264bd13b25...</td>\n",
       "      <td>637673005</td>\n",
       "      <td>0.033881</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-09-28</td>\n",
       "      <td>2e25374e1dd6141985ef534edabbe3ff436b395d1ce8d1...</td>\n",
       "      <td>672498003</td>\n",
       "      <td>0.025407</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-10-12</td>\n",
       "      <td>93cb3a871d8997d85f8d765d37d5526b2eabab693919e4...</td>\n",
       "      <td>677219003</td>\n",
       "      <td>0.033881</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>c6ae7c8e763d1127d6991e86a37d4e6fef69742ef2661c...</td>\n",
       "      <td>907527001</td>\n",
       "      <td>0.041441</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>2020-08-31</td>\n",
       "      <td>3296834ebcbd763dbd8d854f0883998bcf397cc02e6abb...</td>\n",
       "      <td>805947003</td>\n",
       "      <td>0.042356</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>2020-09-06</td>\n",
       "      <td>cfdc06ef05cf8e982bad3ce856bdcdbf4b141b35c2e1ad...</td>\n",
       "      <td>570189003</td>\n",
       "      <td>0.025407</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>2020-09-20</td>\n",
       "      <td>d4003b0349e30d5569547bb11ccd69669cdc9db6463c81...</td>\n",
       "      <td>715828028</td>\n",
       "      <td>0.033881</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>77c31afb6e9ae436227d84c955d23c27ac1013392870cf...</td>\n",
       "      <td>888908001</td>\n",
       "      <td>0.082475</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          t_dat                                        customer_id  \\\n",
       "0    2018-09-20  aaf7a4cf881cc71b8cf97cd8e9c88ce300eb4fe2a279de...   \n",
       "1    2018-09-21  31287b3d29b025cf00822b66b462a415e9c58d65385627...   \n",
       "2    2018-09-23  04ebf0daa6de941f870109b5536bc226f574264bd13b25...   \n",
       "3    2018-09-28  2e25374e1dd6141985ef534edabbe3ff436b395d1ce8d1...   \n",
       "4    2018-10-12  93cb3a871d8997d85f8d765d37d5526b2eabab693919e4...   \n",
       "..          ...                                                ...   \n",
       "195  2020-08-30  c6ae7c8e763d1127d6991e86a37d4e6fef69742ef2661c...   \n",
       "196  2020-08-31  3296834ebcbd763dbd8d854f0883998bcf397cc02e6abb...   \n",
       "197  2020-09-06  cfdc06ef05cf8e982bad3ce856bdcdbf4b141b35c2e1ad...   \n",
       "198  2020-09-20  d4003b0349e30d5569547bb11ccd69669cdc9db6463c81...   \n",
       "199  2020-09-22  77c31afb6e9ae436227d84c955d23c27ac1013392870cf...   \n",
       "\n",
       "     article_id     price  sales_channel_id  \n",
       "0     649445003  0.059305                 1  \n",
       "1     620337036  0.016932                 2  \n",
       "2     637673005  0.033881                 2  \n",
       "3     672498003  0.025407                 2  \n",
       "4     677219003  0.033881                 2  \n",
       "..          ...       ...               ...  \n",
       "195   907527001  0.041441                 2  \n",
       "196   805947003  0.042356                 2  \n",
       "197   570189003  0.025407                 2  \n",
       "198   715828028  0.033881                 1  \n",
       "199   888908001  0.082475                 2  \n",
       "\n",
       "[200 rows x 5 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of rows in complete transactions csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31788325"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newlines_in_csv(\"dataset/transactions_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Trying out user-user collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Psudeocode\n",
    "* Create customer profiles for all customers in (sampled) dataset\n",
    "    * i.e. each customer ID has a vector r_ID whose elements represent items purchased\n",
    "* Compute Jaccard similarity between all r_IDs, independent of position\n",
    "* For a given customer x, choose the k customers closest to x\n",
    "* For an article i, wether or not to recommend is based on the recommendation score\n",
    "    r(x, i) = mean( [rel(y, i) for y in top k] )\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def position_indep_jaccard(x: list | set, y: list | set) -> float:\n",
    "    # Position-independent jaccard-similarity\n",
    "    x, y = set(x), set(y)\n",
    "    return len(x.intersection(y)) / len(x.union(y))\n",
    "\n",
    "\n",
    "\n",
    "def find_customer_similarity(\n",
    "    df_customer: pd.DataFrame, df_transactions: pd.DataFrame\n",
    ") -> Tuple[pd.DataFrame, dict]:\n",
    "    articles_dict = {}\n",
    "    for cust_ID in df_customer[\"customer_id\"]:\n",
    "        articles_dict[cust_ID] = df_transactions[\"article_id\"][\n",
    "            df_transactions[\"customer_id\"] == cust_ID\n",
    "        ].to_list()\n",
    "        # Pop customers without purchase history\n",
    "        if len(articles_dict[cust_ID]) == 0:\n",
    "            articles_dict.pop(cust_ID)\n",
    "    num_customers = len(df_customer)\n",
    "    print(f\"{num_customers = }\")\n",
    "    similarity_matrix = np.zeros((num_customers, num_customers))\n",
    "    # Iterate over customers:\n",
    "    for r, cust in enumerate(articles_dict.keys()):\n",
    "        for c, second in enumerate(articles_dict.keys()):\n",
    "            sim = position_indep_jaccard(articles_dict[cust], articles_dict[second])\n",
    "            similarity_matrix[r, c] = sim\n",
    "\n",
    "    return (\n",
    "        pd.DataFrame(\n",
    "            similarity_matrix, index=articles_dict.keys(), columns=articles_dict.keys()\n",
    "        ),\n",
    "        articles_dict,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_recommendation(\n",
    "    similarity_matrix: pd.DataFrame,\n",
    "    articles_dict: dict,\n",
    "    customer_ID: str,\n",
    "    article_ID: int,\n",
    "    k: int,\n",
    ") -> float:\n",
    "    \"\"\"Produce recommendation score of an item based on its k closest customer behaviors\n",
    "\n",
    "    Args:\n",
    "        similarity_matrix (pd.DataFrame): nxn matrix of similarities between customers\n",
    "        articles_dict (dict): Dictionary of customer purchases on form {customer_id: [item1, item2, ...]}\n",
    "        customer_ID (str): The customer the recommendation score is based on\n",
    "        article_ID (int): The article the score is based on\n",
    "        k (int): How many (closest) customer-neighbors to include in computation.\n",
    "\n",
    "    Returns:\n",
    "        float: Measure of how well the item would fit the customer in question, between [0,1]\n",
    "    \"\"\"\n",
    "    # The k most similar customers IDs:\n",
    "    closest_customers = (\n",
    "        similarity_matrix[customer_ID].sort_values(ascending=False)[:k].index\n",
    "    )\n",
    "    return (\n",
    "        sum(1 if article_ID in articles_dict[cust] else 0 for cust in closest_customers)\n",
    "        / k\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_customers = 200\n"
     ]
    }
   ],
   "source": [
    "# Load sample data\n",
    "df_cust = pd.read_csv(\"dataset_sample/customer_min.csv\")\n",
    "df_tr = pd.read_csv(\"dataset_sample/transactions_min.csv\")\n",
    "df_art = pd.read_csv(\"dataset_sample/articles_min.csv\")\n",
    "sim_matr, art_dict = find_customer_similarity(df_cust, df_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_recommendations(\n",
    "    n: int,\n",
    "    similarity_matrix: pd.DataFrame,\n",
    "    articles_dict: dict,\n",
    "    customer_ID: str,\n",
    "    k: int,\n",
    "    ignore_purchased: bool = True,\n",
    ") -> list:\n",
    "    \"\"\"Get the n 'best' recommended items for a specific customer ID\n",
    "\n",
    "    Args:\n",
    "        n (int): How many items to recommend\n",
    "        similarity_matrix (pd.DataFrame): Customer similarity matrix\n",
    "        articles_dict (dict): Dictionary of customer purchases on form {customer_id: [item1, item2, ...]}\n",
    "        customer_ID (str): _description_\n",
    "        k (int): _description_\n",
    "        ignore_purchased (bool, optional): _description_. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        list: _description_\n",
    "    \"\"\"\n",
    "    # Get rec. score for all cases and choose n with highest score\n",
    "    # ignore_purchased to ignore those articles customer has already bought\n",
    "    blacklisted_articles = (\n",
    "        set(articles_dict[customer_ID]) if ignore_purchased else set()\n",
    "    )\n",
    "    art_IDs = set(itertools.chain(*articles_dict.values())) - blacklisted_articles\n",
    "    score_dict = {\n",
    "        art_ID: get_recommendation(\n",
    "            similarity_matrix, articles_dict, customer_ID, art_ID, k\n",
    "        )\n",
    "        for art_ID in art_IDs\n",
    "    }\n",
    "    n_best_items = {\n",
    "        k: v for k, v in sorted(score_dict.items(), key=lambda el: el[1], reverse=True)\n",
    "    }\n",
    "    # Return entire dict for debug purposes, but otherwise just the article IDs (not scores)\n",
    "    return list(itertools.islice(n_best_items.items(), n))\n",
    "    return list(n_best_items.keys())[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(770851001, 0.2),\n",
       " (806388003, 0.2),\n",
       " (615154002, 0.2),\n",
       " (830702001, 0.2),\n",
       " (677561001, 0.2)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_n_recommendations(\n",
    "    n=5,\n",
    "    similarity_matrix=sim_matr,\n",
    "    articles_dict=art_dict,\n",
    "    customer_ID=\"008068b49b6bdd622ed406e30c8603270770174ebf300dbac0f5beac522921e0\",\n",
    "    k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "get_recommendation(\n",
    "    similarity_matrix=sim_matr,\n",
    "    articles_dict=art_dict,\n",
    "    customer_ID='008068b49b6bdd622ed406e30c8603270770174ebf300dbac0f5beac522921e0',\n",
    "    article_ID=556255001,\n",
    "    k=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, two of the $k$ closest customers (including the customer itself) has bought the article in question. Thus we get a score of $\\frac25=0.4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Methods for metric evaluation (MAP@12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prec(k: int, preds: np.ndarray, true: np.ndarray) -> float:\n",
    "    \"\"\"Precision function with cutoff (k). Used for MAP@12 metric.\n",
    "\n",
    "    Args:\n",
    "        k (int): Cutoff point for prediction array\n",
    "        preds (np.ndarray): Prediction array\n",
    "        true (np.ndarray): Ground truth\n",
    "\n",
    "    Returns:\n",
    "        float: Precision, i.e. portion of correctly predicted values\n",
    "\n",
    "    \"\"\"\n",
    "    # Assumes that preds and true are 1d arrays ['a','b',...]\n",
    "    return len(np.intersect1d(preds[:k], true))/k\n",
    "\n",
    "def rel(k: int, preds: np.ndarray, true: np.ndarray) -> int:\n",
    "    assert 0 < k <= len(preds), \"k must be able to index preds!\"\n",
    "    return int(preds[k-1] in true)\n",
    "\n",
    "def MAPk(k, preds, true) -> float:\n",
    "    return np.mean([\n",
    "        np.sum([prec(i,p,t)*rel(i,p,t) for i in range(1,k+1)])/\\\n",
    "            min(k, len(true))\\\n",
    "                for t, p in zip(true, preds)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_mapk (__main__.TestMetricFunctions) ... ok\n",
      "test_prec (__main__.TestMetricFunctions) ... ok\n",
      "test_rel (__main__.TestMetricFunctions) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.003s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1bd1ab12110>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tests\n",
    "import unittest\n",
    "class TestMetricFunctions(unittest.TestCase):\n",
    "    def __init__(self, methodName: str = 'runTest') -> None:\n",
    "        self.gt = np.array(['a', 'b', 'c', 'd', 'e'])\n",
    "        self.preds1 = np.array(['b', 'c', 'a', 'd', 'e'])\n",
    "        self.preds2 = np.array(['a', 'b', 'c', 'd', 'e'])\n",
    "        self.preds3 = np.array(['f', 'b', 'c', 'd', 'e'])\n",
    "        self.preds4 = np.array(['a', 'f', 'e', 'g', 'b'])\n",
    "        self.preds5 = np.array(['a', 'f', 'c', 'g', 'b'])\n",
    "        self.preds6 = np.array(['d', 'c', 'b', 'a', 'e'])\n",
    "        super().__init__(methodName)\n",
    "\n",
    "    def test_prec(self):\n",
    "        self.assertAlmostEqual(prec(1, self.preds1, self.gt), 1.0)\n",
    "        self.assertAlmostEqual(prec(1, self.preds2, self.gt), 1.0)\n",
    "        self.assertAlmostEqual(prec(1, self.preds3, self.gt), 0.0)\n",
    "        self.assertAlmostEqual(prec(2, self.preds4, self.gt), 0.5)\n",
    "        self.assertAlmostEqual(prec(3, self.preds5, self.gt), 2/3)\n",
    "        self.assertAlmostEqual(prec(3, self.preds6, self.gt), 1.0)\n",
    "    \n",
    "    def test_rel(self):\n",
    "        self.assertAlmostEqual(rel(1, self.preds1, self.gt), 1.0)\n",
    "        self.assertAlmostEqual(rel(1, self.preds2, self.gt), 1.0)\n",
    "        self.assertAlmostEqual(rel(1, self.preds3, self.gt), 0.0)\n",
    "        self.assertAlmostEqual(rel(2, self.preds4, self.gt), 0.0)\n",
    "        self.assertAlmostEqual(rel(3, self.preds5, self.gt), 1.0)\n",
    "        self.assertAlmostEqual(rel(3, self.preds6, self.gt), 1.0)\n",
    "    \n",
    "    def test_mapk(self):\n",
    "        all_true = np.array([self.gt for i in range(6)])\n",
    "        all_pred = np.array([self.preds1, self.preds2, self.preds3,\\\n",
    "                            self.preds4, self.preds5, self.preds6])\n",
    "        self.assertAlmostEqual(MAPk(k=4, preds=all_pred, true=all_true), 0.71875)\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Playing around with Torch embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4272"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First network: let the labels determine if a customer has purchased it or not. Ignore also images for now\n",
    "import torch, os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class Dataset_HM(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        customer_id,\n",
    "        transactions_file,\n",
    "        customers_file,\n",
    "        articles_file,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "    ) -> None:\n",
    "        # I guess we need the customer ID to get the labels for all articles...\n",
    "            # Alternatively we need to repeat that process for each customer ID in dataset\n",
    "        self.df_articles = pd.read_csv(articles_file)\n",
    "        self.df_customers = pd.read_csv(customers_file)\n",
    "        self.df_transactions = pd.read_csv(transactions_file)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # for c_id in self.df_customers['customer_id']:\n",
    "        #     purchased_train = self.df_transactions[self.df_transactions[\"customer_id\"] == c_id][\n",
    "        #         \"article_id\" # Articles bought by customer with ID `id`\n",
    "        #     ]\n",
    "\n",
    "        #     with open(\"tmp.csv\", \"a\") as f:\n",
    "        #         for a_id in self.df_articles[\"article_id\"]:\n",
    "        #             f.write(f\"{c_id}, {a_id}, {1 if a_id in purchased_train.values else 0}\\n\")\n",
    "\n",
    "        # self.labels_train = pd.read_csv(\"tmp.csv\")\n",
    "        # os.remove(\"tmp.csv\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df_article.iloc[idx]  # Here we only use the info in the articles lol\n",
    "        label = self.labels_train.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            row = self.transform(row)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return row, label\n",
    "\n",
    "    def get_loader(self, test: bool = False):\n",
    "        data = self.test_data if test else self.train_data\n",
    "        return DataLoader(data, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "data_testing = Dataset_HM(\n",
    "    \"a301a140b47463f6daf3d9ca358729889e407581b02ce98ce05771d1028d75a3\",\n",
    "    \"dataset_sample/transactions_min.csv\",\n",
    "    \"dataset_sample/articles_min.csv\",\n",
    ")\n",
    "len(data_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torchvision.io import read_image\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Model_HM(torch.nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_size):\n",
    "        super(Model_HM, self).__init__()\n",
    "        self.user_embeddings = torch.nn.Embedding(\n",
    "            num_embeddings=num_users, embedding_dim=embedding_size\n",
    "        )\n",
    "        self.article_embeddings = torch.nn.Embedding(\n",
    "            num_embeddings=num_items, embedding_dim=embedding_size\n",
    "        )\n",
    "\n",
    "    def forward(self, users, items):\n",
    "        user_embeddings = self.user_embeddings(users)\n",
    "        article_embeddings = self.article_embeddings(items)\n",
    "        dot_prod = torch.sum(torch.mul(user_embeddings, article_embeddings), 1)\n",
    "        return torch.sigmoid(dot_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset_HM' object has no attribute 'train_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17752\\833856275.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;34m\"dataset_sample/articles_min.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m )\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdl_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_testing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mdl_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_testing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17752\\3576107508.py\u001b[0m in \u001b[0;36mget_loader\u001b[1;34m(self, test)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_data\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Dataset_HM' object has no attribute 'train_data'"
     ]
    }
   ],
   "source": [
    "data_testing = Dataset_HM(\n",
    "    \"a301a140b47463f6daf3d9ca358729889e407581b02ce98ce05771d1028d75a3\",\n",
    "    \"dataset_sample/transactions_min.csv\",\n",
    "    \"dataset_sample/articles_min.csv\",\n",
    ")\n",
    "dl_train = data_testing.get_loader()\n",
    "dl_test = data_testing.get_loader(test=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('Proj': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "402b62e985bc5250c3cc67917d34a79ef392b62d1143c3e95347f6ab24b3a3fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
