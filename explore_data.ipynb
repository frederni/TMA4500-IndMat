{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext jupyter_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaways from the dataset**\n",
    "\n",
    "* Some articles have no image\n",
    "* Some customers don't buy anything\n",
    "* The complete transaction data has 31 788 325 rows, just short of 32 million (!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import Union, Tuple\n",
    "from types import NoneType\n",
    "import random, shutil, os, itertools, black, jupyter_black\n",
    "\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sampling methods**\n",
    "\n",
    "We need to be able to pull out realistic samples of the dataset. To do this, we first sample $n$ customers at random and include every transaction that they have done - these are the positive labels. In addition, we want to obtain additional transactions that are not related to the customers in the sample, working as a negative label. We implement this by saying that $k$% of the data are true labels, defaulting $k=10$%. Lastly, we pull out the article IDs in all the transactions and obtain the images for said article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_csv_sampler(\n",
    "    csv_path: str,\n",
    "    sample_size: int,\n",
    "    num_records: int | NoneType = None,\n",
    "    header: str | NoneType = \"infer\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Read samples of rows from csv file\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to file including file extensions\n",
    "        sample_size (int): Number of rows to sample\n",
    "        num_records (int | NoneType, optional): Total records in file, defaults to None. If None, the file will be scanned (costly)\n",
    "        header (str | NoneType, optional): 'header'-parameter for pandas, defaults to 'infer'. Set to None if file has no header.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with sampled entries (and potentially header)\n",
    "    \"\"\"\n",
    "    if num_records is None:\n",
    "        num_records = newlines_in_csv(csv_path)\n",
    "    indices_skip = sorted(\n",
    "        random.sample(range(1, num_records + 1), num_records - sample_size)\n",
    "    )\n",
    "    return pd.read_csv(csv_path, skiprows=indices_skip, header=header)\n",
    "\n",
    "\n",
    "def newlines_in_csv(csv_path: str, chunk_size: int = 1024) -> int:\n",
    "    \"\"\"Counts number of newlines in csv file without loading entire file to memory.\n",
    "    The number of newlines is the same as number of rows assuming,\n",
    "        * EITHER csv has a header and last entry does not end with newline\n",
    "        * OR csv does not have a header, but last entry ends with newline\n",
    "        * ALWAYS data does not have any nested newline madness\n",
    "    Originally from orlp, https://stackoverflow.com/a/64744699\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path of csv file\n",
    "        chunk_size (int, optional): How many KB to process at at a time. Defaults to 1024 = 1 MB.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of newlines\n",
    "    \"\"\"\n",
    "    chunk = chunk_size**2\n",
    "    f = np.memmap(csv_path)\n",
    "    number_newlines = sum(\n",
    "        np.sum(f[i : i + chunk] == ord(\"\\n\")) for i in range(0, len(f), chunk)\n",
    "    )\n",
    "    del f\n",
    "    return number_newlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up the datasets\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "def load_min_data(filename: str | Iterable):\n",
    "    dfs = []\n",
    "    if isinstance(filename, str):\n",
    "        filename = [filename]\n",
    "    for fn in filename:\n",
    "        df = pd.read_csv(fn)\n",
    "        # All min-datasets have an index column which has to be dropped:\n",
    "        dfs.append(df.drop(df.columns[0], axis=1))\n",
    "    return dfs\n",
    "\n",
    "\n",
    "df_t, df_c, df_a = load_min_data(\n",
    "    [\n",
    "        f\"dataset_sample/{name}_min.csv\"\n",
    "        for name in (\"transactions\", \"customer\", \"articles\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def clean_customer_data(df):\n",
    "    # df = df.drop(\"FN\", axis=1) # I they're not exactly equal\n",
    "    df.loc[\n",
    "        ~df[\"fashion_news_frequency\"].isin([\"Regularly\", \"Monthly\"]),\n",
    "        \"fashion_news_frequency\",\n",
    "    ] = \"None\"\n",
    "    return df\n",
    "\n",
    "\n",
    "df_c = clean_customer_data(df_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New data loading principle\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class Data_HM(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        total_cases,\n",
    "        portion_negatives,\n",
    "        df_transactions,\n",
    "        df_articles,\n",
    "        df_customers: pd.DataFrame,\n",
    "        train_portion=None,\n",
    "        test_portion=None,\n",
    "    ) -> None:\n",
    "        super().__init__()  # TODO not sure if we need this\n",
    "        self.pos, self.neg = self.generate_dataset(\n",
    "            total_cases, portion_negatives, df_transactions\n",
    "        )\n",
    "        self.df = pd.concat(\n",
    "            [\n",
    "                self.merge_dfs_add_label(\n",
    "                    self.pos,\n",
    "                    df_articles,\n",
    "                    df_customers,\n",
    "                    positive=True,\n",
    "                ),\n",
    "                self.merge_dfs_add_label(\n",
    "                    self.neg,\n",
    "                    df_articles,\n",
    "                    df_customers,\n",
    "                    positive=False,\n",
    "                ),\n",
    "            ]\n",
    "        ).reset_index(drop=True)\n",
    "        self.train, self.test = self.split(train_portion, test_portion)\n",
    "\n",
    "    def generate_dataset(\n",
    "        self, total_cases, portion_negatives, df_transactions: pd.DataFrame\n",
    "    ):\n",
    "        assert (\n",
    "            0 <= portion_negatives <= 1\n",
    "        ), r\"portion negatives must be a percentage between 0% and 100%!\"\n",
    "        n_positive = int(total_cases * (1 - portion_negatives))\n",
    "        n_negative = int(total_cases * portion_negatives)\n",
    "        df_positive = df_transactions.sample(n=n_positive).reset_index(drop=True)\n",
    "        df_positive = df_positive[[\"customer_id\", \"article_id\"]]\n",
    "\n",
    "        num_written = 0\n",
    "        tmpStr = \"customer_id,article_id\\n\"\n",
    "        while num_written < n_negative:\n",
    "            # Choose random customer and article\n",
    "            selection = np.array(  # TODO this can probably be optimized further\n",
    "                [\n",
    "                    df_transactions[\"customer_id\"].sample().values,\n",
    "                    df_transactions[\"article_id\"].sample().values,\n",
    "                ]\n",
    "            ).flatten()\n",
    "            if not (\n",
    "                (df_transactions[\"customer_id\"] == selection[0])\n",
    "                & (df_transactions[\"article_id\"] == selection[1])\n",
    "            ).any():\n",
    "                tmpStr += f\"{selection[0]}, {selection[1]}\\n\"\n",
    "                num_written += 1\n",
    "        with open(\"tmp.csv\", \"w\") as f:\n",
    "            f.write(tmpStr)\n",
    "        df_negative = pd.read_csv(\"tmp.csv\")\n",
    "        os.remove(\"tmp.csv\")\n",
    "        return df_positive, df_negative\n",
    "\n",
    "    def merge_dfs_add_label(\n",
    "        self, df_transactions, df_articles, df_customers, positive: bool = False\n",
    "    ):\n",
    "        columns_articles = [\n",
    "            \"article_id\",\n",
    "            \"prod_name\",\n",
    "            \"product_type_name\",\n",
    "            \"product_group_name\",\n",
    "            \"graphical_appearance_name\",\n",
    "            \"colour_group_name\",\n",
    "            \"perceived_colour_value_name\",\n",
    "            \"perceived_colour_master_name\",\n",
    "            \"department_name\",\n",
    "            \"index_name\",\n",
    "            \"index_group_name\",\n",
    "            \"section_name\",\n",
    "            \"garment_group_name\",\n",
    "            \"detail_desc\",\n",
    "        ]\n",
    "        # TODO consider storing blacklisted cols instead of whitelisted\n",
    "\n",
    "        df_articles = df_articles[columns_articles]\n",
    "\n",
    "        # print(df_customers)\n",
    "        df = pd.merge(\n",
    "            df_transactions, df_customers, how=\"inner\", on=[\"customer_id\"]\n",
    "        ).drop([\"customer_id\"], axis=1)\n",
    "        df = pd.merge(df, df_articles, how=\"inner\", on=[\"article_id\"]).drop(\n",
    "            [\"article_id\"], axis=1\n",
    "        )\n",
    "        df[\"label\"] = 1 if positive else 0\n",
    "        return df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row, label = self.df.iloc[idx, :-1], self.df.iloc[idx, -1]\n",
    "        if self.transform:\n",
    "            row = self.transform(row)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return row, label\n",
    "\n",
    "    def split(\n",
    "        self, train_portion: float | None = None, test_portion: float | None = None\n",
    "    ):\n",
    "        assert any(\n",
    "            [train_portion, test_portion]\n",
    "        ), \"At least one of train or test portion must be float\"\n",
    "        train = self.df.sample(frac=train_portion)\n",
    "        test = (\n",
    "            pd.merge(self.df, train, indicator=True, how=\"outer\")\n",
    "            .query('_merge==\"left_only\"')\n",
    "            .drop(\"_merge\", axis=1)\n",
    "        )\n",
    "        return train, test\n",
    "\n",
    "\n",
    "class HM_train(Data_HM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        total_cases,\n",
    "        portion_negatives,\n",
    "        df_transactions,\n",
    "        df_articles,\n",
    "        df_customers: pd.DataFrame,\n",
    "        train_portion=None,\n",
    "        test_portion=None,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            total_cases,\n",
    "            portion_negatives,\n",
    "            df_transactions,\n",
    "            df_articles,\n",
    "            df_customers,\n",
    "            train_portion,\n",
    "            test_portion,\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row, label = self.train.iloc[idx, :-1], self.train.iloc[idx, -1]\n",
    "        if self.transform:\n",
    "            row = self.transform(row)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return row, label\n",
    "\n",
    "\n",
    "class HM_test(Data_HM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        total_cases,\n",
    "        portion_negatives,\n",
    "        df_transactions,\n",
    "        df_articles,\n",
    "        df_customers: pd.DataFrame,\n",
    "        train_portion=None,\n",
    "        test_portion=None,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            total_cases,\n",
    "            portion_negatives,\n",
    "            df_transactions,\n",
    "            df_articles,\n",
    "            df_customers,\n",
    "            train_portion,\n",
    "            test_portion,\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row, label = self.test.iloc[idx, :-1], self.test.iloc[idx, -1]\n",
    "        if self.transform:\n",
    "            row = self.transform(row)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return row, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_portion = 0.3\n",
    "test_portion = 0.7\n",
    "all([isinstance(param, float) for param in (train_portion, test_portion)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FN</th>\n",
       "      <th>Active</th>\n",
       "      <th>club_member_status</th>\n",
       "      <th>fashion_news_frequency</th>\n",
       "      <th>age</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>prod_name</th>\n",
       "      <th>product_type_name</th>\n",
       "      <th>product_group_name</th>\n",
       "      <th>graphical_appearance_name</th>\n",
       "      <th>colour_group_name</th>\n",
       "      <th>perceived_colour_value_name</th>\n",
       "      <th>perceived_colour_master_name</th>\n",
       "      <th>department_name</th>\n",
       "      <th>index_name</th>\n",
       "      <th>index_group_name</th>\n",
       "      <th>section_name</th>\n",
       "      <th>garment_group_name</th>\n",
       "      <th>detail_desc</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>Regularly</td>\n",
       "      <td>25.0</td>\n",
       "      <td>775b31c72dd3176829dc56c8c740b2247b073533918e4e...</td>\n",
       "      <td>C Jackpot Swimsuit</td>\n",
       "      <td>Swimsuit</td>\n",
       "      <td>Swimwear</td>\n",
       "      <td>Metallic</td>\n",
       "      <td>Beige</td>\n",
       "      <td>Dusty Light</td>\n",
       "      <td>Beige</td>\n",
       "      <td>Swimwear</td>\n",
       "      <td>Lingeries/Tights</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Womens Swimwear, beachwear</td>\n",
       "      <td>Swimwear</td>\n",
       "      <td>Swimsuit with a deep V-neck, lined cups with r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>None</td>\n",
       "      <td>22.0</td>\n",
       "      <td>c12edba42d280702a54a11dd9d754b7fbe9c8756b52e05...</td>\n",
       "      <td>Hazel</td>\n",
       "      <td>T-shirt</td>\n",
       "      <td>Garment Upper body</td>\n",
       "      <td>Solid</td>\n",
       "      <td>Black</td>\n",
       "      <td>Dark</td>\n",
       "      <td>Black</td>\n",
       "      <td>Jersey Basic</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Womens Everyday Basics</td>\n",
       "      <td>Jersey Basic</td>\n",
       "      <td>T-shirt in soft cotton jersey with a slightly ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>None</td>\n",
       "      <td>30.0</td>\n",
       "      <td>791b977b18ecf5914c8f78eae44b471cc2001433d8b082...</td>\n",
       "      <td>Falling star trouser</td>\n",
       "      <td>Trousers</td>\n",
       "      <td>Garment Lower body</td>\n",
       "      <td>Solid</td>\n",
       "      <td>Light Beige</td>\n",
       "      <td>Dusty Light</td>\n",
       "      <td>White</td>\n",
       "      <td>Trouser</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Womens Tailoring</td>\n",
       "      <td>Trousers</td>\n",
       "      <td>Full-length trousers in woven fabric. High wai...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>None</td>\n",
       "      <td>29.0</td>\n",
       "      <td>72d01d90afd0c226d32ca53e764d544b0238563941cef7...</td>\n",
       "      <td>Long leggings 2-pack</td>\n",
       "      <td>Leggings/Tights</td>\n",
       "      <td>Garment Lower body</td>\n",
       "      <td>Solid</td>\n",
       "      <td>Black</td>\n",
       "      <td>Dark</td>\n",
       "      <td>Black</td>\n",
       "      <td>Jersey Basic</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Womens Everyday Basics</td>\n",
       "      <td>Jersey Basic</td>\n",
       "      <td>Jersey leggings with an elasticated waist.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>Regularly</td>\n",
       "      <td>32.0</td>\n",
       "      <td>8f27f69db4ece33fa186d3e5599cc7d4dbafcbf342aaf8...</td>\n",
       "      <td>SIMPLE KNIT DRESS</td>\n",
       "      <td>Dress</td>\n",
       "      <td>Garment Full body</td>\n",
       "      <td>Solid</td>\n",
       "      <td>Black</td>\n",
       "      <td>Dark</td>\n",
       "      <td>Black</td>\n",
       "      <td>Campaigns</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Womens Everyday Collection</td>\n",
       "      <td>Special Offers</td>\n",
       "      <td>Short, fitted dress in a fine knit with a smal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>Regularly</td>\n",
       "      <td>32.0</td>\n",
       "      <td>8f27f69db4ece33fa186d3e5599cc7d4dbafcbf342aaf8...</td>\n",
       "      <td>Class Line earring</td>\n",
       "      <td>Earring</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Solid</td>\n",
       "      <td>Gold</td>\n",
       "      <td>Bright</td>\n",
       "      <td>Metal</td>\n",
       "      <td>Jewellery</td>\n",
       "      <td>Ladies Accessories</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Womens Small accessories</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Metal earrings with round discs and hoop penda...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>Regularly</td>\n",
       "      <td>27.0</td>\n",
       "      <td>76c7d8747a83d1c09d66e311b142d77c0fa1d8fb646c5a...</td>\n",
       "      <td>Perrie Slim Mom Denim TRS</td>\n",
       "      <td>Trousers</td>\n",
       "      <td>Garment Lower body</td>\n",
       "      <td>Solid</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Light</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Trousers</td>\n",
       "      <td>Divided</td>\n",
       "      <td>Divided</td>\n",
       "      <td>Divided Collection</td>\n",
       "      <td>Trousers</td>\n",
       "      <td>5-pocket, ankle-length jeans in washed, sturdy...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>None</td>\n",
       "      <td>28.0</td>\n",
       "      <td>c8d07b479e8afa2e915daae81820a56b1f907cc97fd2cb...</td>\n",
       "      <td>Buenos Brief</td>\n",
       "      <td>Swimwear bottom</td>\n",
       "      <td>Swimwear</td>\n",
       "      <td>Solid</td>\n",
       "      <td>Black</td>\n",
       "      <td>Dark</td>\n",
       "      <td>Black</td>\n",
       "      <td>Divided Swimwear</td>\n",
       "      <td>Divided</td>\n",
       "      <td>Divided</td>\n",
       "      <td>Divided Basics</td>\n",
       "      <td>Swimwear</td>\n",
       "      <td>Fully lined bikini bottoms with a mid waist, w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>None</td>\n",
       "      <td>47.0</td>\n",
       "      <td>e28a643283c2b517d3e63d17291ae4b339d79827e32dd3...</td>\n",
       "      <td>Durham joggers(1)</td>\n",
       "      <td>Trousers</td>\n",
       "      <td>Garment Lower body</td>\n",
       "      <td>Melange</td>\n",
       "      <td>Grey</td>\n",
       "      <td>Dusty Light</td>\n",
       "      <td>Grey</td>\n",
       "      <td>Loungewear</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Womens Nightwear, Socks &amp; Tigh</td>\n",
       "      <td>Under-, Nightwear</td>\n",
       "      <td>Joggers in a soft, fine knit with a high, elas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>Regularly</td>\n",
       "      <td>57.0</td>\n",
       "      <td>52e267d94d5929007300d2777c2f4f8944c12c4d92229a...</td>\n",
       "      <td>PE BRITTA BO</td>\n",
       "      <td>Top</td>\n",
       "      <td>Garment Upper body</td>\n",
       "      <td>Solid</td>\n",
       "      <td>Dark Green</td>\n",
       "      <td>Dark</td>\n",
       "      <td>Green</td>\n",
       "      <td>Take Care External</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Collaborations</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Long-sleeved jersey top in a silk blend with s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>Regularly</td>\n",
       "      <td>21.0</td>\n",
       "      <td>00b7efd47eeb50702752f1b9ffd8ebd953a54124aead10...</td>\n",
       "      <td>Ford dress</td>\n",
       "      <td>Dress</td>\n",
       "      <td>Garment Full body</td>\n",
       "      <td>Solid</td>\n",
       "      <td>Pink</td>\n",
       "      <td>Bright</td>\n",
       "      <td>Pink</td>\n",
       "      <td>Jersey</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Womens Trend</td>\n",
       "      <td>Jersey Fancy</td>\n",
       "      <td>Short, fitted dress in jersey crêpe with a V-n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>Regularly</td>\n",
       "      <td>27.0</td>\n",
       "      <td>76c7d8747a83d1c09d66e311b142d77c0fa1d8fb646c5a...</td>\n",
       "      <td>ARTICHOKE SLIM FIT POLO</td>\n",
       "      <td>Polo shirt</td>\n",
       "      <td>Garment Upper body</td>\n",
       "      <td>All over pattern</td>\n",
       "      <td>Dark Blue</td>\n",
       "      <td>Dark</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Jersey Fancy</td>\n",
       "      <td>Menswear</td>\n",
       "      <td>Menswear</td>\n",
       "      <td>Contemporary Smart</td>\n",
       "      <td>Jersey Fancy</td>\n",
       "      <td>Short-sleeved polo shirt in stretch cotton jer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>Regularly</td>\n",
       "      <td>33.0</td>\n",
       "      <td>d10c43c82243f4706d28a2598f20656fa20af61fe180c3...</td>\n",
       "      <td>SPEED Timmy dress</td>\n",
       "      <td>Dress</td>\n",
       "      <td>Garment Full body</td>\n",
       "      <td>Lace</td>\n",
       "      <td>Dark Blue</td>\n",
       "      <td>Dark</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Jersey</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Womens Tailoring</td>\n",
       "      <td>Jersey Fancy</td>\n",
       "      <td>Short, sleeveless dress in sturdy, textured je...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     FN  Active club_member_status fashion_news_frequency   age  \\\n",
       "3   1.0     NaN             ACTIVE              Regularly  25.0   \n",
       "2   NaN     NaN             ACTIVE                   None  22.0   \n",
       "18  NaN     NaN             ACTIVE                   None  30.0   \n",
       "7   NaN     NaN             ACTIVE                   None  29.0   \n",
       "13  1.0     1.0             ACTIVE              Regularly  32.0   \n",
       "0   1.0     1.0             ACTIVE              Regularly  32.0   \n",
       "11  1.0     1.0             ACTIVE              Regularly  27.0   \n",
       "14  NaN     NaN             ACTIVE                   None  28.0   \n",
       "12  1.0     1.0             ACTIVE                   None  47.0   \n",
       "8   1.0     1.0             ACTIVE              Regularly  57.0   \n",
       "5   1.0     1.0             ACTIVE              Regularly  21.0   \n",
       "1   1.0     1.0             ACTIVE              Regularly  27.0   \n",
       "6   1.0     1.0             ACTIVE              Regularly  33.0   \n",
       "\n",
       "                                          postal_code  \\\n",
       "3   775b31c72dd3176829dc56c8c740b2247b073533918e4e...   \n",
       "2   c12edba42d280702a54a11dd9d754b7fbe9c8756b52e05...   \n",
       "18  791b977b18ecf5914c8f78eae44b471cc2001433d8b082...   \n",
       "7   72d01d90afd0c226d32ca53e764d544b0238563941cef7...   \n",
       "13  8f27f69db4ece33fa186d3e5599cc7d4dbafcbf342aaf8...   \n",
       "0   8f27f69db4ece33fa186d3e5599cc7d4dbafcbf342aaf8...   \n",
       "11  76c7d8747a83d1c09d66e311b142d77c0fa1d8fb646c5a...   \n",
       "14  c8d07b479e8afa2e915daae81820a56b1f907cc97fd2cb...   \n",
       "12  e28a643283c2b517d3e63d17291ae4b339d79827e32dd3...   \n",
       "8   52e267d94d5929007300d2777c2f4f8944c12c4d92229a...   \n",
       "5   00b7efd47eeb50702752f1b9ffd8ebd953a54124aead10...   \n",
       "1   76c7d8747a83d1c09d66e311b142d77c0fa1d8fb646c5a...   \n",
       "6   d10c43c82243f4706d28a2598f20656fa20af61fe180c3...   \n",
       "\n",
       "                    prod_name product_type_name  product_group_name  \\\n",
       "3          C Jackpot Swimsuit          Swimsuit            Swimwear   \n",
       "2                       Hazel           T-shirt  Garment Upper body   \n",
       "18       Falling star trouser          Trousers  Garment Lower body   \n",
       "7        Long leggings 2-pack   Leggings/Tights  Garment Lower body   \n",
       "13          SIMPLE KNIT DRESS             Dress   Garment Full body   \n",
       "0          Class Line earring           Earring         Accessories   \n",
       "11  Perrie Slim Mom Denim TRS          Trousers  Garment Lower body   \n",
       "14               Buenos Brief   Swimwear bottom            Swimwear   \n",
       "12          Durham joggers(1)          Trousers  Garment Lower body   \n",
       "8                PE BRITTA BO               Top  Garment Upper body   \n",
       "5                  Ford dress             Dress   Garment Full body   \n",
       "1     ARTICHOKE SLIM FIT POLO        Polo shirt  Garment Upper body   \n",
       "6           SPEED Timmy dress             Dress   Garment Full body   \n",
       "\n",
       "   graphical_appearance_name colour_group_name perceived_colour_value_name  \\\n",
       "3                   Metallic             Beige                 Dusty Light   \n",
       "2                      Solid             Black                        Dark   \n",
       "18                     Solid       Light Beige                 Dusty Light   \n",
       "7                      Solid             Black                        Dark   \n",
       "13                     Solid             Black                        Dark   \n",
       "0                      Solid              Gold                      Bright   \n",
       "11                     Solid              Blue                       Light   \n",
       "14                     Solid             Black                        Dark   \n",
       "12                   Melange              Grey                 Dusty Light   \n",
       "8                      Solid        Dark Green                        Dark   \n",
       "5                      Solid              Pink                      Bright   \n",
       "1           All over pattern         Dark Blue                        Dark   \n",
       "6                       Lace         Dark Blue                        Dark   \n",
       "\n",
       "   perceived_colour_master_name     department_name          index_name  \\\n",
       "3                         Beige            Swimwear    Lingeries/Tights   \n",
       "2                         Black        Jersey Basic          Ladieswear   \n",
       "18                        White             Trouser          Ladieswear   \n",
       "7                         Black        Jersey Basic          Ladieswear   \n",
       "13                        Black           Campaigns          Ladieswear   \n",
       "0                         Metal           Jewellery  Ladies Accessories   \n",
       "11                         Blue            Trousers             Divided   \n",
       "14                        Black    Divided Swimwear             Divided   \n",
       "12                         Grey          Loungewear          Ladieswear   \n",
       "8                         Green  Take Care External          Ladieswear   \n",
       "5                          Pink              Jersey          Ladieswear   \n",
       "1                          Blue        Jersey Fancy            Menswear   \n",
       "6                          Blue              Jersey          Ladieswear   \n",
       "\n",
       "   index_group_name                    section_name garment_group_name  \\\n",
       "3        Ladieswear      Womens Swimwear, beachwear           Swimwear   \n",
       "2        Ladieswear          Womens Everyday Basics       Jersey Basic   \n",
       "18       Ladieswear                Womens Tailoring           Trousers   \n",
       "7        Ladieswear          Womens Everyday Basics       Jersey Basic   \n",
       "13       Ladieswear      Womens Everyday Collection     Special Offers   \n",
       "0        Ladieswear        Womens Small accessories        Accessories   \n",
       "11          Divided              Divided Collection           Trousers   \n",
       "14          Divided                  Divided Basics           Swimwear   \n",
       "12       Ladieswear  Womens Nightwear, Socks & Tigh  Under-, Nightwear   \n",
       "8        Ladieswear                  Collaborations            Unknown   \n",
       "5        Ladieswear                    Womens Trend       Jersey Fancy   \n",
       "1          Menswear              Contemporary Smart       Jersey Fancy   \n",
       "6        Ladieswear                Womens Tailoring       Jersey Fancy   \n",
       "\n",
       "                                          detail_desc  label  \n",
       "3   Swimsuit with a deep V-neck, lined cups with r...      0  \n",
       "2   T-shirt in soft cotton jersey with a slightly ...      1  \n",
       "18  Full-length trousers in woven fabric. High wai...      0  \n",
       "7          Jersey leggings with an elasticated waist.      0  \n",
       "13  Short, fitted dress in a fine knit with a smal...      0  \n",
       "0   Metal earrings with round discs and hoop penda...      1  \n",
       "11  5-pocket, ankle-length jeans in washed, sturdy...      0  \n",
       "14  Fully lined bikini bottoms with a mid waist, w...      0  \n",
       "12  Joggers in a soft, fine knit with a high, elas...      0  \n",
       "8   Long-sleeved jersey top in a silk blend with s...      0  \n",
       "5   Short, fitted dress in jersey crêpe with a V-n...      0  \n",
       "1   Short-sleeved polo shirt in stretch cotton jer...      1  \n",
       "6   Short, sleeveless dress in sturdy, textured je...      0  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset = Data_HM(20, 0.8, df_t, df_a, df_c, train_portion=0.7)\n",
    "my_test = HM_test(20, 0.8, df_t, df_a, df_c, train_portion=0.7)\n",
    "my_test.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_img_from_article(df: pd.DataFrame, outpath):\n",
    "    for id in df['article_id']:\n",
    "        id0 = \"0\" + str(id)\n",
    "        img_path = f\"./dataset/images/{id0[:3]}/{id0}.jpg\"\n",
    "        if not os.path.isfile(img_path):\n",
    "            continue # ID has no image (happens for some cases)\n",
    "        out_dir = f\"./{outpath}/images/{id0[:3]}/\"\n",
    "        if not os.path.isdir(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "            shutil.copy(img_path, out_dir)\n",
    "copy_img_from_article(df_art, \"dataset_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = naive_csv_sampler(\"dataset/transactions_train.csv\", sample_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_dat</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>aaf7a4cf881cc71b8cf97cd8e9c88ce300eb4fe2a279de...</td>\n",
       "      <td>649445003</td>\n",
       "      <td>0.059305</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-09-21</td>\n",
       "      <td>31287b3d29b025cf00822b66b462a415e9c58d65385627...</td>\n",
       "      <td>620337036</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-09-23</td>\n",
       "      <td>04ebf0daa6de941f870109b5536bc226f574264bd13b25...</td>\n",
       "      <td>637673005</td>\n",
       "      <td>0.033881</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-09-28</td>\n",
       "      <td>2e25374e1dd6141985ef534edabbe3ff436b395d1ce8d1...</td>\n",
       "      <td>672498003</td>\n",
       "      <td>0.025407</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-10-12</td>\n",
       "      <td>93cb3a871d8997d85f8d765d37d5526b2eabab693919e4...</td>\n",
       "      <td>677219003</td>\n",
       "      <td>0.033881</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>c6ae7c8e763d1127d6991e86a37d4e6fef69742ef2661c...</td>\n",
       "      <td>907527001</td>\n",
       "      <td>0.041441</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>2020-08-31</td>\n",
       "      <td>3296834ebcbd763dbd8d854f0883998bcf397cc02e6abb...</td>\n",
       "      <td>805947003</td>\n",
       "      <td>0.042356</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>2020-09-06</td>\n",
       "      <td>cfdc06ef05cf8e982bad3ce856bdcdbf4b141b35c2e1ad...</td>\n",
       "      <td>570189003</td>\n",
       "      <td>0.025407</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>2020-09-20</td>\n",
       "      <td>d4003b0349e30d5569547bb11ccd69669cdc9db6463c81...</td>\n",
       "      <td>715828028</td>\n",
       "      <td>0.033881</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>77c31afb6e9ae436227d84c955d23c27ac1013392870cf...</td>\n",
       "      <td>888908001</td>\n",
       "      <td>0.082475</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          t_dat                                        customer_id  \\\n",
       "0    2018-09-20  aaf7a4cf881cc71b8cf97cd8e9c88ce300eb4fe2a279de...   \n",
       "1    2018-09-21  31287b3d29b025cf00822b66b462a415e9c58d65385627...   \n",
       "2    2018-09-23  04ebf0daa6de941f870109b5536bc226f574264bd13b25...   \n",
       "3    2018-09-28  2e25374e1dd6141985ef534edabbe3ff436b395d1ce8d1...   \n",
       "4    2018-10-12  93cb3a871d8997d85f8d765d37d5526b2eabab693919e4...   \n",
       "..          ...                                                ...   \n",
       "195  2020-08-30  c6ae7c8e763d1127d6991e86a37d4e6fef69742ef2661c...   \n",
       "196  2020-08-31  3296834ebcbd763dbd8d854f0883998bcf397cc02e6abb...   \n",
       "197  2020-09-06  cfdc06ef05cf8e982bad3ce856bdcdbf4b141b35c2e1ad...   \n",
       "198  2020-09-20  d4003b0349e30d5569547bb11ccd69669cdc9db6463c81...   \n",
       "199  2020-09-22  77c31afb6e9ae436227d84c955d23c27ac1013392870cf...   \n",
       "\n",
       "     article_id     price  sales_channel_id  \n",
       "0     649445003  0.059305                 1  \n",
       "1     620337036  0.016932                 2  \n",
       "2     637673005  0.033881                 2  \n",
       "3     672498003  0.025407                 2  \n",
       "4     677219003  0.033881                 2  \n",
       "..          ...       ...               ...  \n",
       "195   907527001  0.041441                 2  \n",
       "196   805947003  0.042356                 2  \n",
       "197   570189003  0.025407                 2  \n",
       "198   715828028  0.033881                 1  \n",
       "199   888908001  0.082475                 2  \n",
       "\n",
       "[200 rows x 5 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of rows in complete transactions csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31788325"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newlines_in_csv(\"dataset/transactions_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Trying out user-user collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Psudeocode\n",
    "* Create customer profiles for all customers in (sampled) dataset\n",
    "    * i.e. each customer ID has a vector r_ID whose elements represent items purchased\n",
    "* Compute Jaccard similarity between all r_IDs, independent of position\n",
    "* For a given customer x, choose the k customers closest to x\n",
    "* For an article i, wether or not to recommend is based on the recommendation score\n",
    "    r(x, i) = mean( [rel(y, i) for y in top k] )\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def position_indep_jaccard(x: list | set, y: list | set) -> float:\n",
    "    # Position-independent jaccard-similarity\n",
    "    x, y = set(x), set(y)\n",
    "    return len(x.intersection(y)) / len(x.union(y))\n",
    "\n",
    "\n",
    "\n",
    "def find_customer_similarity(\n",
    "    df_customer: pd.DataFrame, df_transactions: pd.DataFrame\n",
    ") -> Tuple[pd.DataFrame, dict]:\n",
    "    articles_dict = {}\n",
    "    for cust_ID in df_customer[\"customer_id\"]:\n",
    "        articles_dict[cust_ID] = df_transactions[\"article_id\"][\n",
    "            df_transactions[\"customer_id\"] == cust_ID\n",
    "        ].to_list()\n",
    "        # Pop customers without purchase history\n",
    "        if len(articles_dict[cust_ID]) == 0:\n",
    "            articles_dict.pop(cust_ID)\n",
    "    num_customers = len(df_customer)\n",
    "    print(f\"{num_customers = }\")\n",
    "    similarity_matrix = np.zeros((num_customers, num_customers))\n",
    "    # Iterate over customers:\n",
    "    for r, cust in enumerate(articles_dict.keys()):\n",
    "        for c, second in enumerate(articles_dict.keys()):\n",
    "            sim = position_indep_jaccard(articles_dict[cust], articles_dict[second])\n",
    "            similarity_matrix[r, c] = sim\n",
    "\n",
    "    return (\n",
    "        pd.DataFrame(\n",
    "            similarity_matrix, index=articles_dict.keys(), columns=articles_dict.keys()\n",
    "        ),\n",
    "        articles_dict,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_recommendation(\n",
    "    similarity_matrix: pd.DataFrame,\n",
    "    articles_dict: dict,\n",
    "    customer_ID: str,\n",
    "    article_ID: int,\n",
    "    k: int,\n",
    ") -> float:\n",
    "    \"\"\"Produce recommendation score of an item based on its k closest customer behaviors\n",
    "\n",
    "    Args:\n",
    "        similarity_matrix (pd.DataFrame): nxn matrix of similarities between customers\n",
    "        articles_dict (dict): Dictionary of customer purchases on form {customer_id: [item1, item2, ...]}\n",
    "        customer_ID (str): The customer the recommendation score is based on\n",
    "        article_ID (int): The article the score is based on\n",
    "        k (int): How many (closest) customer-neighbors to include in computation.\n",
    "\n",
    "    Returns:\n",
    "        float: Measure of how well the item would fit the customer in question, between [0,1]\n",
    "    \"\"\"\n",
    "    # The k most similar customers IDs:\n",
    "    closest_customers = (\n",
    "        similarity_matrix[customer_ID].sort_values(ascending=False)[:k].index\n",
    "    )\n",
    "    return (\n",
    "        sum(1 if article_ID in articles_dict[cust] else 0 for cust in closest_customers)\n",
    "        / k\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_customers = 200\n"
     ]
    }
   ],
   "source": [
    "# Load sample data\n",
    "df_cust = pd.read_csv(\"dataset_sample/customer_min.csv\")\n",
    "df_tr = pd.read_csv(\"dataset_sample/transactions_min.csv\")\n",
    "df_art = pd.read_csv(\"dataset_sample/articles_min.csv\")\n",
    "sim_matr, art_dict = find_customer_similarity(df_cust, df_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_recommendations(\n",
    "    n: int,\n",
    "    similarity_matrix: pd.DataFrame,\n",
    "    articles_dict: dict,\n",
    "    customer_ID: str,\n",
    "    k: int,\n",
    "    ignore_purchased: bool = True,\n",
    ") -> list:\n",
    "    \"\"\"Get the n 'best' recommended items for a specific customer ID\n",
    "\n",
    "    Args:\n",
    "        n (int): How many items to recommend\n",
    "        similarity_matrix (pd.DataFrame): Customer similarity matrix\n",
    "        articles_dict (dict): Dictionary of customer purchases on form {customer_id: [item1, item2, ...]}\n",
    "        customer_ID (str): _description_\n",
    "        k (int): _description_\n",
    "        ignore_purchased (bool, optional): _description_. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        list: _description_\n",
    "    \"\"\"\n",
    "    # Get rec. score for all cases and choose n with highest score\n",
    "    # ignore_purchased to ignore those articles customer has already bought\n",
    "    blacklisted_articles = (\n",
    "        set(articles_dict[customer_ID]) if ignore_purchased else set()\n",
    "    )\n",
    "    art_IDs = set(itertools.chain(*articles_dict.values())) - blacklisted_articles\n",
    "    score_dict = {\n",
    "        art_ID: get_recommendation(\n",
    "            similarity_matrix, articles_dict, customer_ID, art_ID, k\n",
    "        )\n",
    "        for art_ID in art_IDs\n",
    "    }\n",
    "    n_best_items = {\n",
    "        k: v for k, v in sorted(score_dict.items(), key=lambda el: el[1], reverse=True)\n",
    "    }\n",
    "    # Return entire dict for debug purposes, but otherwise just the article IDs (not scores)\n",
    "    return list(itertools.islice(n_best_items.items(), n))\n",
    "    return list(n_best_items.keys())[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(770851001, 0.2),\n",
       " (806388003, 0.2),\n",
       " (615154002, 0.2),\n",
       " (830702001, 0.2),\n",
       " (677561001, 0.2)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_n_recommendations(\n",
    "    n=5,\n",
    "    similarity_matrix=sim_matr,\n",
    "    articles_dict=art_dict,\n",
    "    customer_ID=\"008068b49b6bdd622ed406e30c8603270770174ebf300dbac0f5beac522921e0\",\n",
    "    k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "get_recommendation(\n",
    "    similarity_matrix=sim_matr,\n",
    "    articles_dict=art_dict,\n",
    "    customer_ID='008068b49b6bdd622ed406e30c8603270770174ebf300dbac0f5beac522921e0',\n",
    "    article_ID=556255001,\n",
    "    k=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, two of the $k$ closest customers (including the customer itself) has bought the article in question. Thus we get a score of $\\frac25=0.4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Methods for metric evaluation (MAP@12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prec(k: int, preds: np.ndarray, true: np.ndarray) -> float:\n",
    "    \"\"\"Precision function with cutoff (k). Used for MAP@12 metric.\n",
    "\n",
    "    Args:\n",
    "        k (int): Cutoff point for prediction array\n",
    "        preds (np.ndarray): Prediction array\n",
    "        true (np.ndarray): Ground truth\n",
    "\n",
    "    Returns:\n",
    "        float: Precision, i.e. portion of correctly predicted values\n",
    "\n",
    "    \"\"\"\n",
    "    # Assumes that preds and true are 1d arrays ['a','b',...]\n",
    "    return len(np.intersect1d(preds[:k], true))/k\n",
    "\n",
    "def rel(k: int, preds: np.ndarray, true: np.ndarray) -> int:\n",
    "    assert 0 < k <= len(preds), \"k must be able to index preds!\"\n",
    "    return int(preds[k-1] in true)\n",
    "\n",
    "def MAPk(k, preds, true) -> float:\n",
    "    return np.mean([\n",
    "        np.sum([prec(i,p,t)*rel(i,p,t) for i in range(1,k+1)])/\\\n",
    "            min(k, len(true))\\\n",
    "                for t, p in zip(true, preds)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_mapk (__main__.TestMetricFunctions) ... ok\n",
      "test_prec (__main__.TestMetricFunctions) ... ok\n",
      "test_rel (__main__.TestMetricFunctions) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.003s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1bd1ab12110>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tests\n",
    "import unittest\n",
    "class TestMetricFunctions(unittest.TestCase):\n",
    "    def __init__(self, methodName: str = 'runTest') -> None:\n",
    "        self.gt = np.array(['a', 'b', 'c', 'd', 'e'])\n",
    "        self.preds1 = np.array(['b', 'c', 'a', 'd', 'e'])\n",
    "        self.preds2 = np.array(['a', 'b', 'c', 'd', 'e'])\n",
    "        self.preds3 = np.array(['f', 'b', 'c', 'd', 'e'])\n",
    "        self.preds4 = np.array(['a', 'f', 'e', 'g', 'b'])\n",
    "        self.preds5 = np.array(['a', 'f', 'c', 'g', 'b'])\n",
    "        self.preds6 = np.array(['d', 'c', 'b', 'a', 'e'])\n",
    "        super().__init__(methodName)\n",
    "\n",
    "    def test_prec(self):\n",
    "        self.assertAlmostEqual(prec(1, self.preds1, self.gt), 1.0)\n",
    "        self.assertAlmostEqual(prec(1, self.preds2, self.gt), 1.0)\n",
    "        self.assertAlmostEqual(prec(1, self.preds3, self.gt), 0.0)\n",
    "        self.assertAlmostEqual(prec(2, self.preds4, self.gt), 0.5)\n",
    "        self.assertAlmostEqual(prec(3, self.preds5, self.gt), 2/3)\n",
    "        self.assertAlmostEqual(prec(3, self.preds6, self.gt), 1.0)\n",
    "    \n",
    "    def test_rel(self):\n",
    "        self.assertAlmostEqual(rel(1, self.preds1, self.gt), 1.0)\n",
    "        self.assertAlmostEqual(rel(1, self.preds2, self.gt), 1.0)\n",
    "        self.assertAlmostEqual(rel(1, self.preds3, self.gt), 0.0)\n",
    "        self.assertAlmostEqual(rel(2, self.preds4, self.gt), 0.0)\n",
    "        self.assertAlmostEqual(rel(3, self.preds5, self.gt), 1.0)\n",
    "        self.assertAlmostEqual(rel(3, self.preds6, self.gt), 1.0)\n",
    "    \n",
    "    def test_mapk(self):\n",
    "        all_true = np.array([self.gt for i in range(6)])\n",
    "        all_pred = np.array([self.preds1, self.preds2, self.preds3,\\\n",
    "                            self.preds4, self.preds5, self.preds6])\n",
    "        self.assertAlmostEqual(MAPk(k=4, preds=all_pred, true=all_true), 0.71875)\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Playing around with Torch embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4272"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First network: let the labels determine if a customer has purchased it or not. Ignore also images for now\n",
    "import torch, os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class Dataset_HM(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        customer_id,\n",
    "        transactions_file,\n",
    "        customers_file,\n",
    "        articles_file,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "    ) -> None:\n",
    "        # I guess we need the customer ID to get the labels for all articles...\n",
    "            # Alternatively we need to repeat that process for each customer ID in dataset\n",
    "        self.df_articles = pd.read_csv(articles_file)\n",
    "        self.df_customers = pd.read_csv(customers_file)\n",
    "        self.df_transactions = pd.read_csv(transactions_file)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # for c_id in self.df_customers['customer_id']:\n",
    "        #     purchased_train = self.df_transactions[self.df_transactions[\"customer_id\"] == c_id][\n",
    "        #         \"article_id\" # Articles bought by customer with ID `id`\n",
    "        #     ]\n",
    "\n",
    "        #     with open(\"tmp.csv\", \"a\") as f:\n",
    "        #         for a_id in self.df_articles[\"article_id\"]:\n",
    "        #             f.write(f\"{c_id}, {a_id}, {1 if a_id in purchased_train.values else 0}\\n\")\n",
    "\n",
    "        # self.labels_train = pd.read_csv(\"tmp.csv\")\n",
    "        # os.remove(\"tmp.csv\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df_article.iloc[idx]  # Here we only use the info in the articles lol\n",
    "        label = self.labels_train.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            row = self.transform(row)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return row, label\n",
    "\n",
    "    def get_loader(self, test: bool = False):\n",
    "        data = self.test_data if test else self.train_data\n",
    "        return DataLoader(data, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "data_testing = Dataset_HM(\n",
    "    \"a301a140b47463f6daf3d9ca358729889e407581b02ce98ce05771d1028d75a3\",\n",
    "    \"dataset_sample/transactions_min.csv\",\n",
    "    \"dataset_sample/articles_min.csv\",\n",
    ")\n",
    "len(data_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torchvision.io import read_image\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Model_HM(torch.nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_size):\n",
    "        super(Model_HM, self).__init__()\n",
    "        self.user_embeddings = torch.nn.Embedding(\n",
    "            num_embeddings=num_users, embedding_dim=embedding_size\n",
    "        )\n",
    "        self.article_embeddings = torch.nn.Embedding(\n",
    "            num_embeddings=num_items, embedding_dim=embedding_size\n",
    "        )\n",
    "\n",
    "    def forward(self, users, items):\n",
    "        user_embeddings = self.user_embeddings(users)\n",
    "        article_embeddings = self.article_embeddings(items)\n",
    "        dot_prod = torch.sum(torch.mul(user_embeddings, article_embeddings), 1)\n",
    "        return torch.sigmoid(dot_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset_HM' object has no attribute 'train_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17752\\833856275.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;34m\"dataset_sample/articles_min.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m )\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdl_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_testing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mdl_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_testing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17752\\3576107508.py\u001b[0m in \u001b[0;36mget_loader\u001b[1;34m(self, test)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_data\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Dataset_HM' object has no attribute 'train_data'"
     ]
    }
   ],
   "source": [
    "data_testing = Dataset_HM(\n",
    "    \"a301a140b47463f6daf3d9ca358729889e407581b02ce98ce05771d1028d75a3\",\n",
    "    \"dataset_sample/transactions_min.csv\",\n",
    "    \"dataset_sample/articles_min.csv\",\n",
    ")\n",
    "dl_train = data_testing.get_loader()\n",
    "dl_test = data_testing.get_loader(test=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TMA4500-IndMat",
   "language": "python",
   "name": "tma4500-indmat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "402b62e985bc5250c3cc67917d34a79ef392b62d1143c3e95347f6ab24b3a3fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
