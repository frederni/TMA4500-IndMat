{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext jupyter_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaways from the dataset**\n",
    "\n",
    "* Some articles have no image\n",
    "* Some customers don't buy anything\n",
    "* The complete transaction data has 31 788 325 rows, just short of 32 million (!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import Union, Tuple\n",
    "from types import NoneType\n",
    "import random, shutil, os, itertools, black, jupyter_black\n",
    "\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling methods\n",
    "\n",
    "We need to be able to pull out realistic samples of the dataset. To do this, we first sample $n$ customers at random and include every transaction that they have done - these are the positive labels. In addition, we want to obtain additional transactions that are not related to the customers in the sample, working as a negative label. We implement this by saying that $k$% of the data are true labels, defaulting $k=10$%. Lastly, we pull out the article IDs in all the transactions and obtain the images for said article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_csv_sampler(\n",
    "    csv_path: str,\n",
    "    sample_size: int,\n",
    "    num_records: int | NoneType = None,\n",
    "    header: str | NoneType = \"infer\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Read samples of rows from csv file\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to file including file extensions\n",
    "        sample_size (int): Number of rows to sample\n",
    "        num_records (int | NoneType, optional): Total records in file, defaults to None. If None, the file will be scanned (costly)\n",
    "        header (str | NoneType, optional): 'header'-parameter for pandas, defaults to 'infer'. Set to None if file has no header.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with sampled entries (and potentially header)\n",
    "    \"\"\"\n",
    "    if num_records is None:\n",
    "        num_records = newlines_in_csv(csv_path)\n",
    "    indices_skip = sorted(\n",
    "        random.sample(range(1, num_records + 1), num_records - sample_size)\n",
    "    )\n",
    "    return pd.read_csv(csv_path, skiprows=indices_skip, header=header)\n",
    "\n",
    "\n",
    "def newlines_in_csv(csv_path: str, chunk_size: int = 1024) -> int:\n",
    "    \"\"\"Counts number of newlines in csv file without loading entire file to memory.\n",
    "    The number of newlines is the same as number of rows assuming,\n",
    "        * EITHER csv has a header and last entry does not end with newline\n",
    "        * OR csv does not have a header, but last entry ends with newline\n",
    "        * ALWAYS data does not have any nested newline madness\n",
    "    Originally from orlp, https://stackoverflow.com/a/64744699\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path of csv file\n",
    "        chunk_size (int, optional): How many KB to process at at a time. Defaults to 1024 = 1 MB.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of newlines\n",
    "    \"\"\"\n",
    "    chunk = chunk_size**2\n",
    "    f = np.memmap(csv_path)\n",
    "    number_newlines = sum(\n",
    "        np.sum(f[i : i + chunk] == ord(\"\\n\")) for i in range(0, len(f), chunk)\n",
    "    )\n",
    "    del f\n",
    "    return number_newlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_img_from_article(df: pd.DataFrame, outpath):\n",
    "    for id in df['article_id']:\n",
    "        id0 = \"0\" + str(id)\n",
    "        img_path = f\"./dataset/images/{id0[:3]}/{id0}.jpg\"\n",
    "        if not os.path.isfile(img_path):\n",
    "            continue # ID has no image (happens for some cases)\n",
    "        out_dir = f\"./{outpath}/images/{id0[:3]}/\"\n",
    "        if not os.path.isdir(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "            shutil.copy(img_path, out_dir)\n",
    "copy_img_from_article(df_art, \"dataset_sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up the datasets\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "def load_min_data(filename: str | Iterable):\n",
    "    dfs = []\n",
    "    if isinstance(filename, str):\n",
    "        filename = [filename]\n",
    "    for fn in filename:\n",
    "        df = pd.read_csv(fn)\n",
    "        # All min-datasets have an index column which has to be dropped:\n",
    "        dfs.append(df.drop(df.columns[0], axis=1))\n",
    "    return dfs\n",
    "\n",
    "def clean_customer_data(df):\n",
    "    # df = df.drop(\"FN\", axis=1) # I they're not exactly equal\n",
    "    df.loc[\n",
    "        ~df[\"fashion_news_frequency\"].isin([\"Regularly\", \"Monthly\"]),\n",
    "        \"fashion_news_frequency\",\n",
    "    ] = \"None\"\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New data loading principle\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class Data_HM(Dataset):\n",
    "    \"\"\"This is the general HM Dataset class whose children are train-dataset and validation-dataset\n",
    "\n",
    "    Args:\n",
    "        Dataset: Abstract Dataset class from pyTorch\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        total_cases: int,\n",
    "        portion_negatives: float,\n",
    "        df_transactions: pd.DataFrame,\n",
    "        df_articles: pd.DataFrame,\n",
    "        df_customers: pd.DataFrame,\n",
    "        train_portion: float | None = None,\n",
    "        test_portion: float | None = None,\n",
    "    ) -> None:\n",
    "        super().__init__()  # TODO not sure if we need this\n",
    "        self.pos, self.neg = self.generate_dataset(\n",
    "            total_cases, portion_negatives, df_transactions\n",
    "        )\n",
    "        self.df = pd.concat(\n",
    "            [\n",
    "                self.merge_dfs_add_label(\n",
    "                    self.pos,\n",
    "                    df_articles,\n",
    "                    df_customers,\n",
    "                    positive=True,\n",
    "                ),\n",
    "                self.merge_dfs_add_label(\n",
    "                    self.neg,\n",
    "                    df_articles,\n",
    "                    df_customers,\n",
    "                    positive=False,\n",
    "                ),\n",
    "            ]\n",
    "        ).reset_index(drop=True)\n",
    "        self.train, self.test = self.split(train_portion, test_portion)\n",
    "\n",
    "    def generate_dataset(\n",
    "        self, total_cases: int, portion_negatives: float, df_transactions: pd.DataFrame\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Produce DataFrames for positive labels and generated negative samples\n",
    "\n",
    "        Args:\n",
    "            total_cases (int): Total number of transactions\n",
    "            portion_negatives (float): The portion of the `total_cases` that should be negative. Balanced 0/1 when 0.5\n",
    "            df_transactions (pd.DataFrame): Transactions to pull samples/generate samples from\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: _description_\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            0 <= portion_negatives <= 1\n",
    "        ), r\"portion negatives must be a float between 0%=0.0 and 100%=1.0!\"\n",
    "        n_positive = int(total_cases * (1 - portion_negatives))\n",
    "        n_negative = int(total_cases * portion_negatives)\n",
    "        df_positive = df_transactions.sample(n=n_positive).reset_index(drop=True)\n",
    "        df_positive = df_positive[[\"customer_id\", \"article_id\"]]\n",
    "\n",
    "        \n",
    "        # Sampling negative labels:\n",
    "        #   We select a random combination of `customer_id`, `article_id`, and ensure that this is not a true transaction.\n",
    "        #   Then we write this tuple to a csv which is transformed into a DataFrame similar to `df_positive`\n",
    "\n",
    "        num_written = 0\n",
    "        tmpStr = \"customer_id,article_id\\n\"\n",
    "        while num_written < n_negative:\n",
    "            # Choose random customer and article\n",
    "            selection = np.array(  # TODO this can probably be optimized further\n",
    "                [\n",
    "                    df_transactions[\"customer_id\"].sample().values,\n",
    "                    df_transactions[\"article_id\"].sample().values,\n",
    "                ]\n",
    "            ).flatten()\n",
    "            if not (\n",
    "                (df_transactions[\"customer_id\"] == selection[0])\n",
    "                & (df_transactions[\"article_id\"] == selection[1])\n",
    "            ).any():\n",
    "                tmpStr += f\"{selection[0]}, {selection[1]}\\n\"\n",
    "                num_written += 1\n",
    "        with open(\"tmp.csv\", \"w\") as f:\n",
    "            f.write(tmpStr)\n",
    "        df_negative = pd.read_csv(\"tmp.csv\")\n",
    "        os.remove(\"tmp.csv\")\n",
    "        return df_positive, df_negative\n",
    "\n",
    "    def merge_dfs_add_label(\n",
    "        self, df_transactions: pd.DataFrame, df_articles: pd.DataFrame, df_customers: pd.DataFrame, positive: bool = False\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Merge customer and article data to the sampled data `df_transactions`, excluding customer/article IDs\n",
    "\n",
    "        Args:\n",
    "            df_transactions (pd.DataFrame): DataFrame from `generate_dataset`\n",
    "            df_articles (pd.DataFrame): Articles DataFrame\n",
    "            df_customers (pd.DataFrame): Customers DataFrame\n",
    "            positive (bool, optional): Wether or not df_transactions represent positive labels. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DF with all columns included\n",
    "        \"\"\"\n",
    "        columns_articles = [\n",
    "            \"article_id\",\n",
    "            \"prod_name\",\n",
    "            \"product_type_name\",\n",
    "            \"product_group_name\",\n",
    "            \"graphical_appearance_name\",\n",
    "            \"colour_group_name\",\n",
    "            \"perceived_colour_value_name\",\n",
    "            \"perceived_colour_master_name\",\n",
    "            \"department_name\",\n",
    "            \"index_name\",\n",
    "            \"index_group_name\",\n",
    "            \"section_name\",\n",
    "            \"garment_group_name\",\n",
    "            \"detail_desc\",\n",
    "        ]\n",
    "        # TODO consider storing blacklisted cols instead of whitelisted\n",
    "\n",
    "        df_articles = df_articles[columns_articles]\n",
    "\n",
    "        df = pd.merge(\n",
    "            df_transactions, df_customers, how=\"inner\", on=[\"customer_id\"]\n",
    "        ).drop([\"customer_id\"], axis=1)\n",
    "        df = pd.merge(df, df_articles, how=\"inner\", on=[\"article_id\"]).drop(\n",
    "            [\"article_id\"], axis=1\n",
    "        )\n",
    "        df[\"label\"] = 1 if positive else 0\n",
    "        return df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row, label = self.df.iloc[idx, :-1], self.df.iloc[idx, -1]\n",
    "        if self.transform:\n",
    "            row = self.transform(row)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return row, label\n",
    "\n",
    "    def split(\n",
    "        self, train_portion: float | None = None, test_portion: float | None = None\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Split full dataset into training and validation set. Note that only one of train_portion or\n",
    "            test_portion are required (test_portion = 100% - test_portion)\n",
    "\n",
    "        Args:\n",
    "            train_portion (float | None, optional): Percentage of rows assigned to training set. Defaults to None.\n",
    "            test_portion (float | None, optional): Percentage of rows assigned to validation set. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: Train-set and validation-set\n",
    "        \"\"\"\n",
    "        assert any(\n",
    "            [train_portion, test_portion]\n",
    "        ), \"At least one of train or test portion must be float\"\n",
    "        if train_portion is None:\n",
    "            train_portion = 1-test_portion\n",
    "        train = self.df.sample(frac=train_portion)\n",
    "        test = (\n",
    "            pd.merge(self.df, train, indicator=True, how=\"outer\")\n",
    "            .query('_merge==\"left_only\"')\n",
    "            .drop(\"_merge\", axis=1)\n",
    "        )\n",
    "        return train.reset_index(drop=True), test.reset_index(drop=True)\n",
    "\n",
    "\n",
    "class HM_train(Data_HM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        total_cases,\n",
    "        portion_negatives,\n",
    "        df_transactions,\n",
    "        df_articles,\n",
    "        df_customers: pd.DataFrame,\n",
    "        train_portion=None,\n",
    "        test_portion=None,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            total_cases,\n",
    "            portion_negatives,\n",
    "            df_transactions,\n",
    "            df_articles,\n",
    "            df_customers,\n",
    "            train_portion,\n",
    "            test_portion,\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row, label = self.train.iloc[idx, :-1], self.train.iloc[idx, -1]\n",
    "        if self.transform:\n",
    "            row = self.transform(row)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return row, label\n",
    "\n",
    "\n",
    "class HM_val(Data_HM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        total_cases,\n",
    "        portion_negatives,\n",
    "        df_transactions,\n",
    "        df_articles,\n",
    "        df_customers: pd.DataFrame,\n",
    "        train_portion=None,\n",
    "        test_portion=None,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            total_cases,\n",
    "            portion_negatives,\n",
    "            df_transactions,\n",
    "            df_articles,\n",
    "            df_customers,\n",
    "            train_portion,\n",
    "            test_portion,\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row, label = self.test.iloc[idx, :-1], self.test.iloc[idx, -1]\n",
    "        if self.transform:\n",
    "            row = self.transform(row)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return row, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding models (same model as Mind Data example)\n",
    "\n",
    "\n",
    "class HM_model(torch.nn.Module):\n",
    "    def __init__(self, num_customer, num_transactions, embedding_size):\n",
    "        super(HM_model, self).__init__()\n",
    "        self.customer_embed = torch.nn.Embedding(\n",
    "            num_embeddings=num_customer, embedding_dim=embedding_size\n",
    "        )\n",
    "        self.trans_embed = torch.nn.Embedding(\n",
    "            num_embeddings=num_transactions, embedding_dim=embedding_size\n",
    "        )\n",
    "\n",
    "    def forward(self, users, items):\n",
    "        customer_embed = self.customer_embed(users)\n",
    "        trans_embed = self.trans_embed(items)\n",
    "        dot_prod = torch.sum(torch.mul(customer_embed, trans_embed), 1)\n",
    "        return torch.sigmoid(dot_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of HM_model(\n",
       "  (customer_embed): Embedding(200, 50)\n",
       "  (trans_embed): Embedding(200, 50)\n",
       ")>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "MyMod = HM_model(200, 200, 50)\n",
    "MyMod.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" Psudeo one epoch\n",
    "epoch_loss = 0\n",
    "* For each point in data\n",
    "    * retrieve column info + label and set to separate variables\n",
    "    * optimizer.zero_grad # Not sure if we should do this or not..\n",
    "    * compute prediction via model(row_info)\n",
    "    * compute loss_value via loss(prediction.view(-1), labels)\n",
    "    * loss.backward()\n",
    "    * optimizer.step()\n",
    "    * Potentially: LR scheduler.step()\n",
    "\n",
    "    epoch_loss += loss_value\n",
    "\n",
    "print(\"Epoch\", \"Loss\", \"Loss per data sample\", sep=\"\\t\")\n",
    "print(epoch+1, epoch_loss, epoch_loss/len(data), sep=\"\\t\")\n",
    "print(\"-\"*20)\n",
    "\"\"\"\n",
    "\n",
    "def train_one_epoch(model: HM_model, data, epoch_num: int, optimizer, loss):\n",
    "    epoch_loss = 0\n",
    "    for batch, row in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(row)\n",
    "        loss_value = loss(pred.view(-1), labels)\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss_value\n",
    "\n",
    "def train(model, train_DL, params):\n",
    "    # Uses binary cross entropy at the moment\n",
    "    loss_metric = torch.nn.BCELoss() # TODO change to MAP12 once the rest works\n",
    "\n",
    "    \"\"\" Psudeocode\n",
    "    * Initialize loss function and optimizer\n",
    "    * For epoch in epochs:\n",
    "        * Retrieve data from train_DL # Example uses custom sample_training_data \n",
    "        * train_one_epoch(...)\n",
    "        * Report eval statistics for each n-th epoch\n",
    "            * Both training accuracy and validation accuracy\n",
    "    \"\"\"\n",
    "def validate(model, DL, train=False):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "\n",
    "def main():\n",
    "    @dataclass\n",
    "    class Hyperparameters:\n",
    "        lr_rate: float = 1e-3\n",
    "        weight_decay: str = \"l2_reg\"\n",
    "        # Add more here...\n",
    "\n",
    "    # Load data\n",
    "    df_c, df_a, df_t = load_min_data(\n",
    "        [\n",
    "            f\"dataset_sample/{n}_min.csv\"\n",
    "            for n in (\"customer\", \"articles\", \"transactions\")\n",
    "        ]\n",
    "    )\n",
    "    df_c = clean_customer_data(df_c)\n",
    "\n",
    "    # Transform to training and testing set\n",
    "    dataset_params = {\n",
    "        \"total_cases\": 20,\n",
    "        \"portion_negatives\": 0.9,\n",
    "        \"df_transactions\": df_t,\n",
    "        \"df_articles\": df_a,\n",
    "        \"df_customers\": df_c,\n",
    "        \"train_portion\": 0.7,\n",
    "    }\n",
    "    data_train = HM_train(**dataset_params)\n",
    "    data_test = HM_val(**dataset_params)\n",
    "\n",
    "    model = HM_model(num_customer=20, num_transactions=20, embedding_size=5)\n",
    "\n",
    "    # Train, eval, save results and weights...\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of rows in complete transactions csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31788325"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newlines_in_csv(\"dataset/transactions_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Trying out user-user collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Psudeocode\n",
    "* Create customer profiles for all customers in (sampled) dataset\n",
    "    * i.e. each customer ID has a vector r_ID whose elements represent items purchased\n",
    "* Compute Jaccard similarity between all r_IDs, independent of position\n",
    "* For a given customer x, choose the k customers closest to x\n",
    "* For an article i, wether or not to recommend is based on the recommendation score\n",
    "    r(x, i) = mean( [rel(y, i) for y in top k] )\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def position_indep_jaccard(x: list | set, y: list | set) -> float:\n",
    "    # Position-independent jaccard-similarity\n",
    "    x, y = set(x), set(y)\n",
    "    return len(x.intersection(y)) / len(x.union(y))\n",
    "\n",
    "\n",
    "\n",
    "def find_customer_similarity(\n",
    "    df_customer: pd.DataFrame, df_transactions: pd.DataFrame\n",
    ") -> Tuple[pd.DataFrame, dict]:\n",
    "    articles_dict = {}\n",
    "    for cust_ID in df_customer[\"customer_id\"]:\n",
    "        articles_dict[cust_ID] = df_transactions[\"article_id\"][\n",
    "            df_transactions[\"customer_id\"] == cust_ID\n",
    "        ].to_list()\n",
    "        # Pop customers without purchase history\n",
    "        if len(articles_dict[cust_ID]) == 0:\n",
    "            articles_dict.pop(cust_ID)\n",
    "    num_customers = len(df_customer)\n",
    "    print(f\"{num_customers = }\")\n",
    "    similarity_matrix = np.zeros((num_customers, num_customers))\n",
    "    # Iterate over customers:\n",
    "    for r, cust in enumerate(articles_dict.keys()):\n",
    "        for c, second in enumerate(articles_dict.keys()):\n",
    "            sim = position_indep_jaccard(articles_dict[cust], articles_dict[second])\n",
    "            similarity_matrix[r, c] = sim\n",
    "\n",
    "    return (\n",
    "        pd.DataFrame(\n",
    "            similarity_matrix, index=articles_dict.keys(), columns=articles_dict.keys()\n",
    "        ),\n",
    "        articles_dict,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_recommendation(\n",
    "    similarity_matrix: pd.DataFrame,\n",
    "    articles_dict: dict,\n",
    "    customer_ID: str,\n",
    "    article_ID: int,\n",
    "    k: int,\n",
    ") -> float:\n",
    "    \"\"\"Produce recommendation score of an item based on its k closest customer behaviors\n",
    "\n",
    "    Args:\n",
    "        similarity_matrix (pd.DataFrame): nxn matrix of similarities between customers\n",
    "        articles_dict (dict): Dictionary of customer purchases on form {customer_id: [item1, item2, ...]}\n",
    "        customer_ID (str): The customer the recommendation score is based on\n",
    "        article_ID (int): The article the score is based on\n",
    "        k (int): How many (closest) customer-neighbors to include in computation.\n",
    "\n",
    "    Returns:\n",
    "        float: Measure of how well the item would fit the customer in question, between [0,1]\n",
    "    \"\"\"\n",
    "    # The k most similar customers IDs:\n",
    "    closest_customers = (\n",
    "        similarity_matrix[customer_ID].sort_values(ascending=False)[:k].index\n",
    "    )\n",
    "    return (\n",
    "        sum(1 if article_ID in articles_dict[cust] else 0 for cust in closest_customers)\n",
    "        / k\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_customers = 200\n"
     ]
    }
   ],
   "source": [
    "# Load sample data\n",
    "df_cust = pd.read_csv(\"dataset_sample/customer_min.csv\")\n",
    "df_tr = pd.read_csv(\"dataset_sample/transactions_min.csv\")\n",
    "df_art = pd.read_csv(\"dataset_sample/articles_min.csv\")\n",
    "sim_matr, art_dict = find_customer_similarity(df_cust, df_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_recommendations(\n",
    "    n: int,\n",
    "    similarity_matrix: pd.DataFrame,\n",
    "    articles_dict: dict,\n",
    "    customer_ID: str,\n",
    "    k: int,\n",
    "    ignore_purchased: bool = True,\n",
    ") -> list:\n",
    "    \"\"\"Get the n 'best' recommended items for a specific customer ID\n",
    "\n",
    "    Args:\n",
    "        n (int): How many items to recommend\n",
    "        similarity_matrix (pd.DataFrame): Customer similarity matrix\n",
    "        articles_dict (dict): Dictionary of customer purchases on form {customer_id: [item1, item2, ...]}\n",
    "        customer_ID (str): _description_\n",
    "        k (int): _description_\n",
    "        ignore_purchased (bool, optional): _description_. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        list: _description_\n",
    "    \"\"\"\n",
    "    # Get rec. score for all cases and choose n with highest score\n",
    "    # ignore_purchased to ignore those articles customer has already bought\n",
    "    blacklisted_articles = (\n",
    "        set(articles_dict[customer_ID]) if ignore_purchased else set()\n",
    "    )\n",
    "    art_IDs = set(itertools.chain(*articles_dict.values())) - blacklisted_articles\n",
    "    score_dict = {\n",
    "        art_ID: get_recommendation(\n",
    "            similarity_matrix, articles_dict, customer_ID, art_ID, k\n",
    "        )\n",
    "        for art_ID in art_IDs\n",
    "    }\n",
    "    n_best_items = {\n",
    "        k: v for k, v in sorted(score_dict.items(), key=lambda el: el[1], reverse=True)\n",
    "    }\n",
    "    # Return entire dict for debug purposes, but otherwise just the article IDs (not scores)\n",
    "    return list(itertools.islice(n_best_items.items(), n))\n",
    "    return list(n_best_items.keys())[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(770851001, 0.2),\n",
       " (806388003, 0.2),\n",
       " (615154002, 0.2),\n",
       " (830702001, 0.2),\n",
       " (677561001, 0.2)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_n_recommendations(\n",
    "    n=5,\n",
    "    similarity_matrix=sim_matr,\n",
    "    articles_dict=art_dict,\n",
    "    customer_ID=\"008068b49b6bdd622ed406e30c8603270770174ebf300dbac0f5beac522921e0\",\n",
    "    k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "get_recommendation(\n",
    "    similarity_matrix=sim_matr,\n",
    "    articles_dict=art_dict,\n",
    "    customer_ID='008068b49b6bdd622ed406e30c8603270770174ebf300dbac0f5beac522921e0',\n",
    "    article_ID=556255001,\n",
    "    k=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, two of the $k$ closest customers (including the customer itself) has bought the article in question. Thus we get a score of $\\frac25=0.4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Methods for metric evaluation (MAP@12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prec(k: int, preds: np.ndarray, true: np.ndarray) -> float:\n",
    "    \"\"\"Precision function with cutoff (k). Used for MAP@12 metric.\n",
    "\n",
    "    Args:\n",
    "        k (int): Cutoff point for prediction array\n",
    "        preds (np.ndarray): Prediction array\n",
    "        true (np.ndarray): Ground truth\n",
    "\n",
    "    Returns:\n",
    "        float: Precision, i.e. portion of correctly predicted values\n",
    "\n",
    "    \"\"\"\n",
    "    # Assumes that preds and true are 1d arrays ['a','b',...]\n",
    "    return len(np.intersect1d(preds[:k], true))/k\n",
    "\n",
    "def rel(k: int, preds: np.ndarray, true: np.ndarray) -> int:\n",
    "    assert 0 < k <= len(preds), \"k must be able to index preds!\"\n",
    "    return int(preds[k-1] in true)\n",
    "\n",
    "def MAPk(k, preds, true) -> float:\n",
    "    return np.mean([\n",
    "        np.sum([prec(i,p,t)*rel(i,p,t) for i in range(1,k+1)])/\\\n",
    "            min(k, len(true))\\\n",
    "                for t, p in zip(true, preds)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_mapk (__main__.TestMetricFunctions) ... ok\n",
      "test_prec (__main__.TestMetricFunctions) ... ok\n",
      "test_rel (__main__.TestMetricFunctions) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.003s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1bd1ab12110>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tests\n",
    "import unittest\n",
    "class TestMetricFunctions(unittest.TestCase):\n",
    "    def __init__(self, methodName: str = 'runTest') -> None:\n",
    "        self.gt = np.array(['a', 'b', 'c', 'd', 'e'])\n",
    "        self.preds1 = np.array(['b', 'c', 'a', 'd', 'e'])\n",
    "        self.preds2 = np.array(['a', 'b', 'c', 'd', 'e'])\n",
    "        self.preds3 = np.array(['f', 'b', 'c', 'd', 'e'])\n",
    "        self.preds4 = np.array(['a', 'f', 'e', 'g', 'b'])\n",
    "        self.preds5 = np.array(['a', 'f', 'c', 'g', 'b'])\n",
    "        self.preds6 = np.array(['d', 'c', 'b', 'a', 'e'])\n",
    "        super().__init__(methodName)\n",
    "\n",
    "    def test_prec(self):\n",
    "        self.assertAlmostEqual(prec(1, self.preds1, self.gt), 1.0)\n",
    "        self.assertAlmostEqual(prec(1, self.preds2, self.gt), 1.0)\n",
    "        self.assertAlmostEqual(prec(1, self.preds3, self.gt), 0.0)\n",
    "        self.assertAlmostEqual(prec(2, self.preds4, self.gt), 0.5)\n",
    "        self.assertAlmostEqual(prec(3, self.preds5, self.gt), 2/3)\n",
    "        self.assertAlmostEqual(prec(3, self.preds6, self.gt), 1.0)\n",
    "    \n",
    "    def test_rel(self):\n",
    "        self.assertAlmostEqual(rel(1, self.preds1, self.gt), 1.0)\n",
    "        self.assertAlmostEqual(rel(1, self.preds2, self.gt), 1.0)\n",
    "        self.assertAlmostEqual(rel(1, self.preds3, self.gt), 0.0)\n",
    "        self.assertAlmostEqual(rel(2, self.preds4, self.gt), 0.0)\n",
    "        self.assertAlmostEqual(rel(3, self.preds5, self.gt), 1.0)\n",
    "        self.assertAlmostEqual(rel(3, self.preds6, self.gt), 1.0)\n",
    "    \n",
    "    def test_mapk(self):\n",
    "        all_true = np.array([self.gt for i in range(6)])\n",
    "        all_pred = np.array([self.preds1, self.preds2, self.preds3,\\\n",
    "                            self.preds4, self.preds5, self.preds6])\n",
    "        self.assertAlmostEqual(MAPk(k=4, preds=all_pred, true=all_true), 0.71875)\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Playing around with Torch embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4272"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First network: let the labels determine if a customer has purchased it or not. Ignore also images for now\n",
    "import torch, os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class Dataset_HM(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        customer_id,\n",
    "        transactions_file,\n",
    "        customers_file,\n",
    "        articles_file,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "    ) -> None:\n",
    "        # I guess we need the customer ID to get the labels for all articles...\n",
    "            # Alternatively we need to repeat that process for each customer ID in dataset\n",
    "        self.df_articles = pd.read_csv(articles_file)\n",
    "        self.df_customers = pd.read_csv(customers_file)\n",
    "        self.df_transactions = pd.read_csv(transactions_file)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # for c_id in self.df_customers['customer_id']:\n",
    "        #     purchased_train = self.df_transactions[self.df_transactions[\"customer_id\"] == c_id][\n",
    "        #         \"article_id\" # Articles bought by customer with ID `id`\n",
    "        #     ]\n",
    "\n",
    "        #     with open(\"tmp.csv\", \"a\") as f:\n",
    "        #         for a_id in self.df_articles[\"article_id\"]:\n",
    "        #             f.write(f\"{c_id}, {a_id}, {1 if a_id in purchased_train.values else 0}\\n\")\n",
    "\n",
    "        # self.labels_train = pd.read_csv(\"tmp.csv\")\n",
    "        # os.remove(\"tmp.csv\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df_article.iloc[idx]  # Here we only use the info in the articles lol\n",
    "        label = self.labels_train.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            row = self.transform(row)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return row, label\n",
    "\n",
    "    def get_loader(self, test: bool = False):\n",
    "        data = self.test_data if test else self.train_data\n",
    "        return DataLoader(data, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "data_testing = Dataset_HM(\n",
    "    \"a301a140b47463f6daf3d9ca358729889e407581b02ce98ce05771d1028d75a3\",\n",
    "    \"dataset_sample/transactions_min.csv\",\n",
    "    \"dataset_sample/articles_min.csv\",\n",
    ")\n",
    "len(data_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torchvision.io import read_image\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Model_HM(torch.nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_size):\n",
    "        super(Model_HM, self).__init__()\n",
    "        self.user_embeddings = torch.nn.Embedding(\n",
    "            num_embeddings=num_users, embedding_dim=embedding_size\n",
    "        )\n",
    "        self.article_embeddings = torch.nn.Embedding(\n",
    "            num_embeddings=num_items, embedding_dim=embedding_size\n",
    "        )\n",
    "\n",
    "    def forward(self, users, items):\n",
    "        user_embeddings = self.user_embeddings(users)\n",
    "        article_embeddings = self.article_embeddings(items)\n",
    "        dot_prod = torch.sum(torch.mul(user_embeddings, article_embeddings), 1)\n",
    "        return torch.sigmoid(dot_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset_HM' object has no attribute 'train_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17752\\833856275.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;34m\"dataset_sample/articles_min.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m )\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdl_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_testing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mdl_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_testing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17752\\3576107508.py\u001b[0m in \u001b[0;36mget_loader\u001b[1;34m(self, test)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_data\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Dataset_HM' object has no attribute 'train_data'"
     ]
    }
   ],
   "source": [
    "data_testing = Dataset_HM(\n",
    "    \"a301a140b47463f6daf3d9ca358729889e407581b02ce98ce05771d1028d75a3\",\n",
    "    \"dataset_sample/transactions_min.csv\",\n",
    "    \"dataset_sample/articles_min.csv\",\n",
    ")\n",
    "dl_train = data_testing.get_loader()\n",
    "dl_test = data_testing.get_loader(test=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('Proj': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "402b62e985bc5250c3cc67917d34a79ef392b62d1143c3e95347f6ab24b3a3fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
