{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext jupyter_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaways from the dataset**\n",
    "\n",
    "* Some articles have no image\n",
    "* Some customers don't buy anything\n",
    "* The complete transaction data has 31 788 325 rows, just short of 32 million (!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import Union, Tuple\n",
    "from types import NoneType\n",
    "import random, shutil, os, itertools, black, jupyter_black\n",
    "\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling methods\n",
    "\n",
    "We need to be able to pull out realistic samples of the dataset. To do this, we first sample $n$ customers at random and include every transaction that they have done - these are the positive labels. In addition, we want to obtain additional transactions that are not related to the customers in the sample, working as a negative label. We implement this by saying that $k$% of the data are true labels, defaulting $k=10$%. Lastly, we pull out the article IDs in all the transactions and obtain the images for said article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_csv_sampler(\n",
    "    csv_path: str,\n",
    "    sample_size: int,\n",
    "    num_records: int | NoneType = None,\n",
    "    header: str | NoneType = \"infer\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Read samples of rows from csv file\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to file including file extensions\n",
    "        sample_size (int): Number of rows to sample\n",
    "        num_records (int | NoneType, optional): Total records in file, defaults to None. If None, the file will be scanned (costly)\n",
    "        header (str | NoneType, optional): 'header'-parameter for pandas, defaults to 'infer'. Set to None if file has no header.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with sampled entries (and potentially header)\n",
    "    \"\"\"\n",
    "    if num_records is None:\n",
    "        num_records = newlines_in_csv(csv_path)\n",
    "    indices_skip = sorted(\n",
    "        random.sample(range(1, num_records + 1), num_records - sample_size)\n",
    "    )\n",
    "    return pd.read_csv(csv_path, skiprows=indices_skip, header=header)\n",
    "\n",
    "\n",
    "def newlines_in_csv(csv_path: str, chunk_size: int = 1024) -> int:\n",
    "    \"\"\"Counts number of newlines in csv file without loading entire file to memory.\n",
    "    The number of newlines is the same as number of rows assuming,\n",
    "        * EITHER csv has a header and last entry does not end with newline\n",
    "        * OR csv does not have a header, but last entry ends with newline\n",
    "        * ALWAYS data does not have any nested newline madness\n",
    "    Originally from orlp, https://stackoverflow.com/a/64744699\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path of csv file\n",
    "        chunk_size (int, optional): How many KB to process at at a time. Defaults to 1024 = 1 MB.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of newlines\n",
    "    \"\"\"\n",
    "    chunk = chunk_size**2\n",
    "    f = np.memmap(csv_path)\n",
    "    number_newlines = sum(\n",
    "        np.sum(f[i : i + chunk] == ord(\"\\n\")) for i in range(0, len(f), chunk)\n",
    "    )\n",
    "    del f\n",
    "    return number_newlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_img_from_article(df: pd.DataFrame, outpath):\n",
    "    for id in df['article_id']:\n",
    "        id0 = \"0\" + str(id)\n",
    "        img_path = f\"./dataset/images/{id0[:3]}/{id0}.jpg\"\n",
    "        if not os.path.isfile(img_path):\n",
    "            continue # ID has no image (happens for some cases)\n",
    "        out_dir = f\"./{outpath}/images/{id0[:3]}/\"\n",
    "        if not os.path.isdir(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "            shutil.copy(img_path, out_dir)\n",
    "copy_img_from_article(df_art, \"dataset_sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up the datasets\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "def load_min_data(filename: str | Iterable):\n",
    "    dfs = []\n",
    "    if isinstance(filename, str):\n",
    "        filename = [filename]\n",
    "    for fn in filename:\n",
    "        df = pd.read_csv(fn)\n",
    "        # All min-datasets have an index column which has to be dropped:\n",
    "        dfs.append(df.drop(df.columns[0], axis=1))\n",
    "    return dfs\n",
    "\n",
    "def clean_customer_data(df):\n",
    "    # df = df.drop(\"FN\", axis=1) # I they're not exactly equal\n",
    "    df.loc[\n",
    "        ~df[\"fashion_news_frequency\"].isin([\"Regularly\", \"Monthly\"]),\n",
    "        \"fashion_news_frequency\",\n",
    "    ] = \"None\"\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of rows in complete transactions csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31788325"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newlines_in_csv(\"dataset/transactions_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Trying out user-user collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Psudeocode\n",
    "* Create customer profiles for all customers in (sampled) dataset\n",
    "    * i.e. each customer ID has a vector r_ID whose elements represent items purchased\n",
    "* Compute Jaccard similarity between all r_IDs, independent of position\n",
    "* For a given customer x, choose the k customers closest to x\n",
    "* For an article i, wether or not to recommend is based on the recommendation score\n",
    "    r(x, i) = mean( [rel(y, i) for y in top k] )\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def position_indep_jaccard(x: list | set, y: list | set) -> float:\n",
    "    # Position-independent jaccard-similarity\n",
    "    x, y = set(x), set(y)\n",
    "    return len(x.intersection(y)) / len(x.union(y))\n",
    "\n",
    "\n",
    "\n",
    "def find_customer_similarity(\n",
    "    df_customer: pd.DataFrame, df_transactions: pd.DataFrame\n",
    ") -> Tuple[pd.DataFrame, dict]:\n",
    "    articles_dict = {}\n",
    "    for cust_ID in df_customer[\"customer_id\"]:\n",
    "        articles_dict[cust_ID] = df_transactions[\"article_id\"][\n",
    "            df_transactions[\"customer_id\"] == cust_ID\n",
    "        ].to_list()\n",
    "        # Pop customers without purchase history\n",
    "        if len(articles_dict[cust_ID]) == 0:\n",
    "            articles_dict.pop(cust_ID)\n",
    "    num_customers = len(df_customer)\n",
    "    print(f\"{num_customers = }\")\n",
    "    similarity_matrix = np.zeros((num_customers, num_customers))\n",
    "    # Iterate over customers:\n",
    "    for r, cust in enumerate(articles_dict.keys()):\n",
    "        for c, second in enumerate(articles_dict.keys()):\n",
    "            sim = position_indep_jaccard(articles_dict[cust], articles_dict[second])\n",
    "            similarity_matrix[r, c] = sim\n",
    "\n",
    "    return (\n",
    "        pd.DataFrame(\n",
    "            similarity_matrix, index=articles_dict.keys(), columns=articles_dict.keys()\n",
    "        ),\n",
    "        articles_dict,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_recommendation(\n",
    "    similarity_matrix: pd.DataFrame,\n",
    "    articles_dict: dict,\n",
    "    customer_ID: str,\n",
    "    article_ID: int,\n",
    "    k: int,\n",
    ") -> float:\n",
    "    \"\"\"Produce recommendation score of an item based on its k closest customer behaviors\n",
    "\n",
    "    Args:\n",
    "        similarity_matrix (pd.DataFrame): nxn matrix of similarities between customers\n",
    "        articles_dict (dict): Dictionary of customer purchases on form {customer_id: [item1, item2, ...]}\n",
    "        customer_ID (str): The customer the recommendation score is based on\n",
    "        article_ID (int): The article the score is based on\n",
    "        k (int): How many (closest) customer-neighbors to include in computation.\n",
    "\n",
    "    Returns:\n",
    "        float: Measure of how well the item would fit the customer in question, between [0,1]\n",
    "    \"\"\"\n",
    "    # The k most similar customers IDs:\n",
    "    closest_customers = (\n",
    "        similarity_matrix[customer_ID].sort_values(ascending=False)[:k].index\n",
    "    )\n",
    "    return (\n",
    "        sum(1 if article_ID in articles_dict[cust] else 0 for cust in closest_customers)\n",
    "        / k\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_customers = 200\n"
     ]
    }
   ],
   "source": [
    "# Load sample data\n",
    "df_cust = pd.read_csv(\"dataset_sample/customer_min.csv\")\n",
    "df_tr = pd.read_csv(\"dataset_sample/transactions_min.csv\")\n",
    "df_art = pd.read_csv(\"dataset_sample/articles_min.csv\")\n",
    "sim_matr, art_dict = find_customer_similarity(df_cust, df_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_recommendations(\n",
    "    n: int,\n",
    "    similarity_matrix: pd.DataFrame,\n",
    "    articles_dict: dict,\n",
    "    customer_ID: str,\n",
    "    k: int,\n",
    "    ignore_purchased: bool = True,\n",
    ") -> list:\n",
    "    \"\"\"Get the n 'best' recommended items for a specific customer ID\n",
    "\n",
    "    Args:\n",
    "        n (int): How many items to recommend\n",
    "        similarity_matrix (pd.DataFrame): Customer similarity matrix\n",
    "        articles_dict (dict): Dictionary of customer purchases on form {customer_id: [item1, item2, ...]}\n",
    "        customer_ID (str): _description_\n",
    "        k (int): _description_\n",
    "        ignore_purchased (bool, optional): _description_. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        list: _description_\n",
    "    \"\"\"\n",
    "    # Get rec. score for all cases and choose n with highest score\n",
    "    # ignore_purchased to ignore those articles customer has already bought\n",
    "    blacklisted_articles = (\n",
    "        set(articles_dict[customer_ID]) if ignore_purchased else set()\n",
    "    )\n",
    "    art_IDs = set(itertools.chain(*articles_dict.values())) - blacklisted_articles\n",
    "    score_dict = {\n",
    "        art_ID: get_recommendation(\n",
    "            similarity_matrix, articles_dict, customer_ID, art_ID, k\n",
    "        )\n",
    "        for art_ID in art_IDs\n",
    "    }\n",
    "    n_best_items = {\n",
    "        k: v for k, v in sorted(score_dict.items(), key=lambda el: el[1], reverse=True)\n",
    "    }\n",
    "    # Return entire dict for debug purposes, but otherwise just the article IDs (not scores)\n",
    "    return list(itertools.islice(n_best_items.items(), n))\n",
    "    return list(n_best_items.keys())[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(770851001, 0.2),\n",
       " (806388003, 0.2),\n",
       " (615154002, 0.2),\n",
       " (830702001, 0.2),\n",
       " (677561001, 0.2)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_n_recommendations(\n",
    "    n=5,\n",
    "    similarity_matrix=sim_matr,\n",
    "    articles_dict=art_dict,\n",
    "    customer_ID=\"008068b49b6bdd622ed406e30c8603270770174ebf300dbac0f5beac522921e0\",\n",
    "    k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "get_recommendation(\n",
    "    similarity_matrix=sim_matr,\n",
    "    articles_dict=art_dict,\n",
    "    customer_ID='008068b49b6bdd622ed406e30c8603270770174ebf300dbac0f5beac522921e0',\n",
    "    article_ID=556255001,\n",
    "    k=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, two of the $k$ closest customers (including the customer itself) has bought the article in question. Thus we get a score of $\\frac25=0.4$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('Proj': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "402b62e985bc5250c3cc67917d34a79ef392b62d1143c3e95347f6ab24b3a3fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
