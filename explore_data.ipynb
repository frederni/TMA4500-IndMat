{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import Union, Tuple\n",
    "from types import NoneType\n",
    "import random, shutil, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_csv_sampler(csv_path: str, sample_size: int, num_records: int | NoneType = None, header: str | NoneType = 'infer') -> pd.DataFrame:\n",
    "    \"\"\"Read samples of rows from csv file\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to file including file extensions\n",
    "        sample_size (int): Number of rows to sample\n",
    "        num_records (int | NoneType, optional): Total records in file, defaults to None. If None, the file will be scanned (costly)\n",
    "        header (str | NoneType, optional): 'header'-parameter for pandas, defaults to 'infer'. Set to None if file has no header.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with sampled entries (and potentially header)\n",
    "    \"\"\"\n",
    "    if num_records is None:\n",
    "        num_records = newlines_in_csv(csv_path)\n",
    "    indices_skip = sorted(random.sample(range(1, num_records+1), num_records-sample_size))\n",
    "    return pd.read_csv(csv_path, skiprows=indices_skip, header=header)\n",
    "\n",
    "def newlines_in_csv(csv_path: str, chunk_size: int = 1024) -> int:\n",
    "    \"\"\"Counts number of newlines in csv file without loading entire file to memory.\n",
    "    The number of newlines is the same as number of rows assuming,\n",
    "        * EITHER csv has a header and last entry does not end with newline\n",
    "        * OR csv does not have a header, but last entry ends with newline\n",
    "        * ALWAYS data does not have any nested newline madness\n",
    "    Originally from orlp, https://stackoverflow.com/a/64744699\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path of csv file\n",
    "        chunk_size (int, optional): How many KB to process at at a time. Defaults to 1024 = 1 MB.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of newlines\n",
    "    \"\"\"\n",
    "    chunk = chunk_size**2\n",
    "    f = np.memmap(csv_path)\n",
    "    number_newlines = sum(np.sum(f[i:i+chunk] == ord('\\n')) for i in range(0, len(f), chunk))\n",
    "    del f\n",
    "    return number_newlines\n",
    "\n",
    "def sample_all_hm(num_users: int) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Sample a dataset with `num_user` customers without any unreferenced rows.\n",
    "\n",
    "    Args:\n",
    "        num_users (int): How many users to sample from\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]: DataFrame of customers (1), transactions belonging to customers (2),\n",
    "                                                            and articles belonging to transactions (3)\n",
    "    \"\"\"\n",
    "    df_customers = naive_csv_sampler(\"dataset/customers.csv\", num_users)\n",
    "    df_transactions = pd.read_csv(\"dataset/transactions_train.csv\") # TODO slow!!\n",
    "    df_transactions = df_transactions[df_transactions['customer_id'].isin(df_customers['customer_id'])]\n",
    "    \n",
    "    df_articles = pd.read_csv(\"dataset/articles.csv\")\n",
    "    df_articles = df_articles[df_articles['article_id'].isin(df_transactions['article_id'])]\n",
    "    return df_customers, df_transactions, df_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cust, df_tr, df_art = sample_all_hm(200)\n",
    "\n",
    "df_cust.to_csv(\"dataset/customer_min.csv\")\n",
    "df_tr.to_csv(\"dataset/transactions_min.csv\")\n",
    "df_art.to_csv(\"dataset/articles_min.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_img_from_article(df: pd.DataFrame, outpath):\n",
    "    for id in df['article_id']:\n",
    "        id0 = \"0\" + str(id)\n",
    "        img_path = f\"./dataset/images/{id0[:3]}/{id0}.jpg\"\n",
    "        if not os.path.isfile(img_path):\n",
    "            continue # ID has no image (happens for some cases)\n",
    "        out_dir = f\"./{outpath}/images/{id0[:3]}/\"\n",
    "        if not os.path.isdir(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "            shutil.copy(img_path, out_dir)\n",
    "copy_img_from_article(df_art, \"dataset_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = naive_csv_sampler(\"dataset/transactions_train.csv\", sample_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_dat</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>aaf7a4cf881cc71b8cf97cd8e9c88ce300eb4fe2a279de...</td>\n",
       "      <td>649445003</td>\n",
       "      <td>0.059305</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-09-21</td>\n",
       "      <td>31287b3d29b025cf00822b66b462a415e9c58d65385627...</td>\n",
       "      <td>620337036</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-09-23</td>\n",
       "      <td>04ebf0daa6de941f870109b5536bc226f574264bd13b25...</td>\n",
       "      <td>637673005</td>\n",
       "      <td>0.033881</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-09-28</td>\n",
       "      <td>2e25374e1dd6141985ef534edabbe3ff436b395d1ce8d1...</td>\n",
       "      <td>672498003</td>\n",
       "      <td>0.025407</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-10-12</td>\n",
       "      <td>93cb3a871d8997d85f8d765d37d5526b2eabab693919e4...</td>\n",
       "      <td>677219003</td>\n",
       "      <td>0.033881</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>c6ae7c8e763d1127d6991e86a37d4e6fef69742ef2661c...</td>\n",
       "      <td>907527001</td>\n",
       "      <td>0.041441</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>2020-08-31</td>\n",
       "      <td>3296834ebcbd763dbd8d854f0883998bcf397cc02e6abb...</td>\n",
       "      <td>805947003</td>\n",
       "      <td>0.042356</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>2020-09-06</td>\n",
       "      <td>cfdc06ef05cf8e982bad3ce856bdcdbf4b141b35c2e1ad...</td>\n",
       "      <td>570189003</td>\n",
       "      <td>0.025407</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>2020-09-20</td>\n",
       "      <td>d4003b0349e30d5569547bb11ccd69669cdc9db6463c81...</td>\n",
       "      <td>715828028</td>\n",
       "      <td>0.033881</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>77c31afb6e9ae436227d84c955d23c27ac1013392870cf...</td>\n",
       "      <td>888908001</td>\n",
       "      <td>0.082475</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          t_dat                                        customer_id  \\\n",
       "0    2018-09-20  aaf7a4cf881cc71b8cf97cd8e9c88ce300eb4fe2a279de...   \n",
       "1    2018-09-21  31287b3d29b025cf00822b66b462a415e9c58d65385627...   \n",
       "2    2018-09-23  04ebf0daa6de941f870109b5536bc226f574264bd13b25...   \n",
       "3    2018-09-28  2e25374e1dd6141985ef534edabbe3ff436b395d1ce8d1...   \n",
       "4    2018-10-12  93cb3a871d8997d85f8d765d37d5526b2eabab693919e4...   \n",
       "..          ...                                                ...   \n",
       "195  2020-08-30  c6ae7c8e763d1127d6991e86a37d4e6fef69742ef2661c...   \n",
       "196  2020-08-31  3296834ebcbd763dbd8d854f0883998bcf397cc02e6abb...   \n",
       "197  2020-09-06  cfdc06ef05cf8e982bad3ce856bdcdbf4b141b35c2e1ad...   \n",
       "198  2020-09-20  d4003b0349e30d5569547bb11ccd69669cdc9db6463c81...   \n",
       "199  2020-09-22  77c31afb6e9ae436227d84c955d23c27ac1013392870cf...   \n",
       "\n",
       "     article_id     price  sales_channel_id  \n",
       "0     649445003  0.059305                 1  \n",
       "1     620337036  0.016932                 2  \n",
       "2     637673005  0.033881                 2  \n",
       "3     672498003  0.025407                 2  \n",
       "4     677219003  0.033881                 2  \n",
       "..          ...       ...               ...  \n",
       "195   907527001  0.041441                 2  \n",
       "196   805947003  0.042356                 2  \n",
       "197   570189003  0.025407                 2  \n",
       "198   715828028  0.033881                 1  \n",
       "199   888908001  0.082475                 2  \n",
       "\n",
       "[200 rows x 5 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of rows in complete transactions csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31788325"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newlines_in_csv(\"dataset/transactions_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out user-user collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_customers = 200\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Psudeocode\n",
    "* Create customer profiles for all customers in (sampled) dataset\n",
    "    * i.e. each customer ID has a vector r_ID whose elements represent items purchased\n",
    "* Compute Jaccard similarity between all r_IDs, independent of position\n",
    "* For a given customer x, choose the k customers closest to x\n",
    "* For an article i, wether or not to recommend is based on the recommendation score\n",
    "    r(x, i) = mean( [rel(y, i) for y in top k] )\n",
    "\"\"\"\n",
    "def position_indep_jaccard(x: list | set, y: list | set) -> float:\n",
    "    # Position-independent jaccard-similarity\n",
    "    x, y = set(x), set(y)\n",
    "    try:\n",
    "        return len(x.intersection(y)) / len(x.union(y))\n",
    "    except ZeroDivisionError as e:\n",
    "        print(\"Err this shouldn't happen\")\n",
    "        print(e, f\"{x = }\", f\"{y = }\", sep=\"\\n\")\n",
    "        raise ZeroDivisionError\n",
    "\n",
    "def find_customer_similarity(df_customer: pd.DataFrame, df_transactions: pd.DataFrame) -> Tuple[pd.DataFrame, dict]:\n",
    "    articles_dict = {}\n",
    "    for cust_ID in df_customer['customer_id']:\n",
    "        articles_dict[cust_ID] = df_transactions['article_id'][df_transactions['customer_id'] == cust_ID].to_list()\n",
    "        # Pop customers without purchase history\n",
    "        if len(articles_dict[cust_ID]) == 0: articles_dict.pop(cust_ID)\n",
    "    num_customers = len(df_customer)\n",
    "    print(f\"{num_customers = }\")\n",
    "    similarity_matrix = np.zeros((num_customers, num_customers))\n",
    "    # Iterate over customers:\n",
    "    for r, cust in enumerate(articles_dict.keys()):\n",
    "        for c, second in enumerate(articles_dict.keys()):\n",
    "            sim = position_indep_jaccard(articles_dict[cust], articles_dict[second])\n",
    "            similarity_matrix[r,c] = sim\n",
    "    \n",
    "    return pd.DataFrame(similarity_matrix, index=articles_dict.keys(), columns=articles_dict.keys()), articles_dict\n",
    "\n",
    "def get_recommendation(similarity_matrix: pd.DataFrame, articles_dict: dict, customer_ID: str, article_ID: int, k: int):\n",
    "    # The k most similar customers IDs:\n",
    "    closest_customers = similarity_matrix[customer_ID].sort_values(ascending=False)[:k].index\n",
    "    return sum(1 if article_ID in articles_dict[cust] else 0 for cust in closest_customers)/k\n",
    "\n",
    "sim_matr, art_dict = find_customer_similarity(df_cust, df_tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def get_n_recommendations(n: int, similarity_matrix: pd.DataFrame, articles_dict: dict, customer_ID: str, k: int, ignore_purchased: bool = True):\n",
    "    # Get rec. score for all cases and choose n with highest score\n",
    "    # ignore_purchased to ignore those articles customer has already bought\n",
    "    blacklisted_articles = articles_dict[customer_ID] if ignore_purchased else []\n",
    "    for art_ID in itertools.chain(*articles_dict.values()):\n",
    "        score = get_recommendation(similarity_matrix, articles_dict, customer_ID, art_ID, k) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, two of the $k$ closest customers (including the customer itself) has bought the article in question. Thus we get a score of $\\frac25=0.4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "get_recommendation(\n",
    "    similarity_matrix=sim_matr,\n",
    "    articles_dict=art_dict,\n",
    "    customer_ID='008068b49b6bdd622ed406e30c8603270770174ebf300dbac0f5beac522921e0',\n",
    "    article_ID=556255001,\n",
    "    k=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods for metric evaluation (MAP@12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prec(k: int, preds: np.ndarray, true: np.ndarray) -> float:\n",
    "    \"\"\"Precision function with cutoff (k). Used for MAP@12 metric.\n",
    "\n",
    "    Args:\n",
    "        k (int): Cutoff point for prediction array\n",
    "        preds (np.ndarray): Prediction array\n",
    "        true (np.ndarray): Ground truth\n",
    "\n",
    "    Returns:\n",
    "        float: Precision, i.e. portion of correctly predicted values\n",
    "\n",
    "    \"\"\"\n",
    "    # Assumes that preds and true are 1d arrays ['a','b',...]\n",
    "    return len(np.intersect1d(preds[:k], true))/k\n",
    "\n",
    "def rel(k: int, preds: np.ndarray, true: np.ndarray) -> int:\n",
    "    assert 0 < k <= len(preds), \"k must be able to index preds!\"\n",
    "    return int(preds[k-1] in true)\n",
    "\n",
    "def MAPk(k, preds, true) -> float:\n",
    "    return np.mean([\n",
    "        np.sum([prec(i,p,t)*rel(i,p,t) for i in range(1,k+1)])/\\\n",
    "            min(k, len(true))\\\n",
    "                for t, p in zip(true, preds)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_mapk (__main__.TestMetricFunctions) ... ok\n",
      "test_prec (__main__.TestMetricFunctions) ... ok\n",
      "test_rel (__main__.TestMetricFunctions) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.003s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1bd1ab12110>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tests\n",
    "import unittest\n",
    "class TestMetricFunctions(unittest.TestCase):\n",
    "    def __init__(self, methodName: str = 'runTest') -> None:\n",
    "        self.gt = np.array(['a', 'b', 'c', 'd', 'e'])\n",
    "        self.preds1 = np.array(['b', 'c', 'a', 'd', 'e'])\n",
    "        self.preds2 = np.array(['a', 'b', 'c', 'd', 'e'])\n",
    "        self.preds3 = np.array(['f', 'b', 'c', 'd', 'e'])\n",
    "        self.preds4 = np.array(['a', 'f', 'e', 'g', 'b'])\n",
    "        self.preds5 = np.array(['a', 'f', 'c', 'g', 'b'])\n",
    "        self.preds6 = np.array(['d', 'c', 'b', 'a', 'e'])\n",
    "        super().__init__(methodName)\n",
    "\n",
    "    def test_prec(self):\n",
    "        self.assertAlmostEqual(prec(1, self.preds1, self.gt), 1.0)\n",
    "        self.assertAlmostEqual(prec(1, self.preds2, self.gt), 1.0)\n",
    "        self.assertAlmostEqual(prec(1, self.preds3, self.gt), 0.0)\n",
    "        self.assertAlmostEqual(prec(2, self.preds4, self.gt), 0.5)\n",
    "        self.assertAlmostEqual(prec(3, self.preds5, self.gt), 2/3)\n",
    "        self.assertAlmostEqual(prec(3, self.preds6, self.gt), 1.0)\n",
    "    \n",
    "    def test_rel(self):\n",
    "        self.assertAlmostEqual(rel(1, self.preds1, self.gt), 1.0)\n",
    "        self.assertAlmostEqual(rel(1, self.preds2, self.gt), 1.0)\n",
    "        self.assertAlmostEqual(rel(1, self.preds3, self.gt), 0.0)\n",
    "        self.assertAlmostEqual(rel(2, self.preds4, self.gt), 0.0)\n",
    "        self.assertAlmostEqual(rel(3, self.preds5, self.gt), 1.0)\n",
    "        self.assertAlmostEqual(rel(3, self.preds6, self.gt), 1.0)\n",
    "    \n",
    "    def test_mapk(self):\n",
    "        all_true = np.array([self.gt for i in range(6)])\n",
    "        all_pred = np.array([self.preds1, self.preds2, self.preds3,\\\n",
    "                            self.preds4, self.preds5, self.preds6])\n",
    "        self.assertAlmostEqual(MAPk(k=4, preds=all_pred, true=all_true), 0.71875)\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('Proj': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "402b62e985bc5250c3cc67917d34a79ef392b62d1143c3e95347f6ab24b3a3fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
