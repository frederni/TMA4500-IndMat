{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a recommender system with embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up the datasets\n",
    "from typing import Iterable\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import Tuple, Any\n",
    "import sklearn.model_selection\n",
    "\n",
    "\n",
    "def load_min_data(filename: str | Iterable):\n",
    "    dfs = []\n",
    "    if isinstance(filename, str):\n",
    "        filename = [filename]\n",
    "    for fn in filename:\n",
    "        df = pd.read_csv(fn)\n",
    "        # All min-datasets have an index column which has to be dropped:\n",
    "        dfs.append(df.drop(df.columns[0], axis=1))\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def clean_customer_data(df):\n",
    "    # df = df.drop(\"FN\", axis=1) # I they're not exactly equal\n",
    "    df.loc[\n",
    "        ~df[\"fashion_news_frequency\"].isin([\"Regularly\", \"Monthly\"]),\n",
    "        \"fashion_news_frequency\",\n",
    "    ] = \"None\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# df = pd.read_csv(\"dataset/transactions_train.csv\", dtype={\"article_id\": str})\n",
    "# df_np = df[[\"customer_id\", \"article_id\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the size of the data, it's important to generate negative labels in an efficient way. The function `pandas.DataFrame.sample()` takes almost five seconds for each sample, which is called at least `n_negative` times, we instead transform the dataframe to a NumPy array. Below is a comparison to highlight the importance of working with simpler objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_pd_vs_np(n_negative, df):\n",
    "    import time\n",
    "\n",
    "    start_pd = time.time()\n",
    "    num_written = 0\n",
    "    tmpStr = \"customer_id,article_id\\n\"\n",
    "    while num_written < n_negative:\n",
    "        # Choose random customer and article\n",
    "        selection = np.array(  # TODO this can probably be optimized further\n",
    "            [\n",
    "                df[\"customer_id\"].sample().values,\n",
    "                df[\"article_id\"].sample().values,\n",
    "            ]\n",
    "        ).flatten()\n",
    "        if not (\n",
    "            (df[\"customer_id\"] == selection[0]) & (df[\"article_id\"] == selection[1])\n",
    "        ).any():\n",
    "            tmpStr += f\"{selection[0]}, {selection[1]}\\n\"\n",
    "            num_written += 1\n",
    "    with open(\"tmp.csv\", \"w\") as f:\n",
    "        f.write(tmpStr)\n",
    "    df_negative = pd.read_csv(\"tmp.csv\")\n",
    "    os.remove(\"tmp.csv\")\n",
    "    time_pd = time.time() - start_pd\n",
    "\n",
    "    # Numpy method\n",
    "    start_np = time.time()\n",
    "    df_np = df[[\"customer_id\", \"article_id\"]].to_numpy()\n",
    "    neg_np = np.empty((n_negative, df_np.shape[1]), dtype=\"<U64\")\n",
    "    for i in range(n_negative):\n",
    "        legit = False\n",
    "        while not legit:\n",
    "            sample = [np.random.choice(df_np[:, col]) for col in range(df_np.shape[1])]\n",
    "            legit = not ((df_np[:, 0] == sample[0]) & (df_np[:, 1] == sample[1])).any()\n",
    "        neg_np[i, :] = sample\n",
    "    time_np = time.time() - start_np\n",
    "\n",
    "    return time_pd, time_np\n",
    "\n",
    "\n",
    "def plot_negative_sampling(\n",
    "    start,\n",
    "    stop,\n",
    "    step: int = 1,\n",
    "    filename: str | None = None,\n",
    "    persist_data: bool = True,\n",
    "    cont_from_checkpoint: bool = True,\n",
    "):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from tqdm import tqdm\n",
    "    import pickle\n",
    "\n",
    "    xax = list(range(start, stop, step))\n",
    "\n",
    "    if cont_from_checkpoint:\n",
    "        with open(\"plotData.pckl\", \"rb\") as f:\n",
    "            plot_values = pickle.load(f)\n",
    "        # Add empty list for keys not covered by checkpoint:\n",
    "        computed = set([x_i for x_i in plot_values.keys()])\n",
    "        to_add = set(xax) - computed\n",
    "        for elem in to_add:\n",
    "            plot_values[elem] = []\n",
    "        # Skip those already computed\n",
    "        xax = [x for x in xax if x not in computed]\n",
    "\n",
    "    else:\n",
    "        plot_values = {x_i: [] for x_i in xax}\n",
    "\n",
    "    for n_negative in tqdm(xax):\n",
    "        time_pd, time_np = time_pd_vs_np(n_negative)\n",
    "        plot_values[n_negative].extend([time_pd, time_np])\n",
    "\n",
    "        if persist_data:\n",
    "            with open(\"plotData.pckl\", \"wb\") as f:\n",
    "                pickle.dump(plot_values, f)\n",
    "\n",
    "    plt.plot(\n",
    "        plot_values.keys(),\n",
    "        plot_values.values(),\n",
    "        label=[\n",
    "            \"pandas.DataFrame.Sample implementation\",\n",
    "            \"NumPy.random.choice implementation\",\n",
    "        ],\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Number of negative (generated) samples\")\n",
    "    plt.ylabel(\"Time [s]\")\n",
    "    plt.title(\"Comparison between sampling methods time\")\n",
    "    if filename is not None:\n",
    "        plt.savefig(f\"{filename}.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# plot_negative_sampling(\n",
    "#     start=1,\n",
    "#     stop=50,\n",
    "#     step=1,\n",
    "#     filename=\"Comp_1_to_50\",\n",
    "#     persist_data=True,\n",
    "#     cont_from_checkpoint=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "class Data_HM(Dataset):\n",
    "    \"\"\"This is the general HM Dataset class whose children are train-dataset and validation-dataset\n",
    "    no\n",
    "\n",
    "    Args:\n",
    "        Dataset: Abstract Dataset class from pyTorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        total_cases: int,\n",
    "        portion_negatives: float,\n",
    "        df_transactions: pd.DataFrame,\n",
    "        df_articles: pd.DataFrame,\n",
    "        df_customers: pd.DataFrame,\n",
    "        batch_size: int,\n",
    "        train_portion: float | None = None,\n",
    "        test_portion: float | None = None,\n",
    "        transform: Any = None,\n",
    "        target_transform: Any = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if train_portion is None:\n",
    "            if test_portion is None:\n",
    "                raise ValueError(\"Both train portion and test portion cannot be None.\")\n",
    "            self.train_portion = 1 - test_portion\n",
    "        self.batch_size = batch_size\n",
    "        self.df_id = self.generate_dataset(\n",
    "            total_cases, portion_negatives, df_transactions\n",
    "        )\n",
    "        self.train_portion = train_portion\n",
    "        self.train, self.val = self.split_dataset()\n",
    "        self.transform, self.target_transform = transform, target_transform\n",
    "\n",
    "    def generate_dataset(\n",
    "        self, total_cases: int, portion_negatives: float, df_transactions: pd.DataFrame\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Produce DataFrames for positive labels and generated negative samples\n",
    "\n",
    "        Args:\n",
    "            total_cases (int): Total number of transactions\n",
    "            portion_negatives (float): The portion of the `total_cases` that should be negative. Balanced 0/1 when 0.5\n",
    "            df_transactions (pd.DataFrame): Transactions to pull samples/generate samples from\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: _description_\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            0 <= portion_negatives <= 1\n",
    "        ), r\"portion negatives must be a float between 0%=0.0 and 100%=1.0!\"\n",
    "        n_positive = round(total_cases * (1 - portion_negatives))\n",
    "        n_negative = total_cases - n_positive\n",
    "\n",
    "        df_positive = df_transactions.sample(n=n_positive).reset_index(drop=True)\n",
    "        df_positive = df_positive[[\"customer_id\", \"article_id\"]]\n",
    "        df_positive[\"label\"] = 1\n",
    "\n",
    "        # Sampling negative labels:\n",
    "        #   We select a random combination of `customer_id`, `article_id`, and ensure that this is not a true transaction.\n",
    "        #   Then we make a 2-column dataframe on same form as `df_positive`\n",
    "\n",
    "        df_np = df_transactions[[\"customer_id\", \"article_id\"]].to_numpy()\n",
    "        neg_np = np.empty((n_negative, df_np.shape[1]), dtype=\"<U64\")\n",
    "        for i in range(n_negative):\n",
    "            legit = False\n",
    "            while not legit:\n",
    "                sample = [\n",
    "                    np.random.choice(df_np[:, col]) for col in range(df_np.shape[1])\n",
    "                ]\n",
    "                legit = not (\n",
    "                    (df_np[:, 0] == sample[0]) & (df_np[:, 1] == sample[1])\n",
    "                ).any()\n",
    "            neg_np[i, :] = sample\n",
    "        neg_np = np.column_stack((neg_np, [0] * neg_np.shape[0]))\n",
    "        df_negative = pd.DataFrame(neg_np, columns=df_positive.columns)\n",
    "        # Return a shuffled concatenation of the two dataframes\n",
    "        full_data = (\n",
    "            pd.concat((df_positive, df_negative)).sample(frac=1).reset_index(drop=True)\n",
    "        )\n",
    "        le_cust = LabelEncoder()\n",
    "        le_art = LabelEncoder()\n",
    "        le_cust.fit(full_data[\"customer_id\"])\n",
    "        le_art.fit(full_data[\"article_id\"])\n",
    "        cust_encode = le_cust.transform(full_data[\"customer_id\"])\n",
    "        art_encode = le_art.transform(full_data[\"article_id\"])\n",
    "        return pd.DataFrame(data={\"customer_id\": cust_encode, \"article_id\": art_encode, \"label\": full_data[\"label\"].astype(np.uint8)})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_id.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row, label = self.df_id.iloc[idx, :-1].values, self.df_id.iloc[idx, -1]\n",
    "        label = int(label) # Stored as str for some reason\n",
    "        if self.transform:\n",
    "            row = self.transform(row)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return row, label\n",
    "\n",
    "    def split_dataset(self):\n",
    "        \"\"\"Split full data to train and validation Subset-objects\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Subset, Subset]: Train and validation subsets\n",
    "        \"\"\"\n",
    "        length = len(self)\n",
    "        train_size = int(length * self.train_portion)\n",
    "        valid_size = length - train_size\n",
    "        train, val = torch.utils.data.random_split(self, [train_size, valid_size])\n",
    "        return train, val\n",
    "\n",
    "    def get_data_from_subset(subset):\n",
    "        return subset.dataset.df_id.iloc[subset.indices]\n",
    "\n",
    "    def get_DataLoader(self, trainDL: bool = True):\n",
    "        subset = self.train if trainDL else self.val\n",
    "        return DataLoader(dataset=subset, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding model (same model as Mind Data example)\n",
    "\n",
    "\n",
    "class HM_model(torch.nn.Module):\n",
    "    def __init__(self, num_customer, num_articles, embedding_size):\n",
    "        super(HM_model, self).__init__()\n",
    "        self.customer_embed = torch.nn.Embedding(\n",
    "            num_embeddings=num_customer, embedding_dim=embedding_size\n",
    "        )\n",
    "        self.art_embed = torch.nn.Embedding(\n",
    "            num_embeddings=num_articles, embedding_dim=embedding_size\n",
    "        )\n",
    "\n",
    "    def forward(self, customer_row, article_row):\n",
    "        customer_embed = self.customer_embed(customer_row)\n",
    "        art_embed = self.art_embed(article_row)\n",
    "        dot_prod = torch.sum(torch.mul(customer_embed, art_embed), 1)\n",
    "        return torch.sigmoid(dot_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model: HM_model, data: Data_HM, epoch_num: int, optimizer, loss):\n",
    "    epoch_loss = 0\n",
    "    for row, label in data.get_DataLoader(trainDL=True):\n",
    "        optimizer.zero_grad()\n",
    "        # TODO: MindData seem to pass along several IDs and not only one. Which is correct?\n",
    "        pred = model(row[:,0], row[:,1])\n",
    "        loss_value = loss(pred.view(-1), label)\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss_value\n",
    "    print(f\"\\t| Training loss for epoch {epoch_num+1}: {epoch_loss}\")\n",
    "\n",
    "\n",
    "def train(model, data, params):\n",
    "    # Uses binary cross entropy at the moment\n",
    "    loss_metric = torch.nn.BCELoss()  # TODO change to MAP12 once the rest works\n",
    "    optimizer = params.optimizer(\n",
    "        model.parameters(), lr=params.lr_rate, weight_decay=params.weight_decay\n",
    "    )\n",
    "    for epoch in range(params.epochs):\n",
    "        train_one_epoch(model, data, epoch, optimizer, loss_metric)\n",
    "        if not epoch % params.validation_frequency:\n",
    "\n",
    "            print(f\"Provisory results for epoch {epoch+1}:\")\n",
    "            print(\n",
    "                \"Loss for training set\",\n",
    "                validate(model, data, train=True),\n",
    "                sep=\"\\t\",\n",
    "            )\n",
    "            print(\n",
    "                \"Loss for validation set\",\n",
    "                validate(model, data, train=False),\n",
    "                sep=\"\\t\",\n",
    "            )\n",
    "            print(\"-\" * 20)\n",
    "\n",
    "\n",
    "import utils.metrics as metric\n",
    "\n",
    "\n",
    "def validate(model, data, train):\n",
    "    with torch.no_grad():\n",
    "        preds, labels = [], []\n",
    "        for row in data.getDataLoader(trainDL=train):\n",
    "            customer_id, article_id, label = row\n",
    "            pred_i = model(customer_id, article_id).view(-1)\n",
    "            preds.append(\n",
    "                pred_i.detach().numpy()\n",
    "            )  # TODO same case for our case? not sure\n",
    "            labels.append(label.detach().numpy())\n",
    "        return metric.MAPk(k=12, preds=preds, true=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 4\n"
     ]
    }
   ],
   "source": [
    "test = pd.DataFrame(\n",
    "    {\n",
    "        \"customer_id\": [\"a\", \"b\", \"c\", \"c\"],\n",
    "        \"article_id\": [\"0\", \"01\", \"04\", \"08\"],\n",
    "        \"label\": [0, 0, 1, 1],\n",
    "    }\n",
    ")\n",
    "n_cust, n_art, _ = test.nunique()\n",
    "print(n_cust, n_art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def main(use_min_dataset: bool = False):\n",
    "    @dataclass\n",
    "    class Hyperparameters:\n",
    "        lr_rate: float = 1e-3\n",
    "        weight_decay: str = 1e-5\n",
    "        epochs: int = 100\n",
    "        validation_frequency: int = 10\n",
    "        optimizer: Any = torch.optim.Adam\n",
    "        embedding_size: int = 5\n",
    "        # Add more here...\n",
    "\n",
    "    # Load data\n",
    "    if use_min_dataset:\n",
    "        df_c, df_a, df_t = load_min_data(  # TODO change this so we just pd.readcsv\n",
    "            [\n",
    "                f\"dataset_sample/{n}_min.csv\"\n",
    "                for n in (\"customer\", \"articles\", \"transactions\")\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        df_c = pd.read_csv(\"dataset/customers.csv\")\n",
    "        # Articles IDs all start with 0 which disappears if cast to a number\n",
    "        df_a = pd.read_csv(\"dataset/articles.csv\", dtype={\"article_id\": str})\n",
    "        df_t = pd.read_csv(\"dataset/transactions_train.csv\", dtype={\"article_id\": str})\n",
    "    df_c = clean_customer_data(df_c)\n",
    "\n",
    "    # Transform to training and testing set\n",
    "    dataset_params = {\n",
    "        \"total_cases\": 20,\n",
    "        \"portion_negatives\": 0.9,\n",
    "        \"df_transactions\": df_t,\n",
    "        \"df_articles\": df_a,\n",
    "        \"df_customers\": df_c,\n",
    "        \"train_portion\": 0.7,\n",
    "        \"batch_size\": 5,\n",
    "    }\n",
    "    hyperparams = Hyperparameters()\n",
    "    data = Data_HM(**dataset_params)\n",
    "    n_cust, n_art, _ = data.df_id.nunique()\n",
    "    model = HM_model(\n",
    "        num_customer=n_cust,\n",
    "        num_articles=n_art,\n",
    "        embedding_size=hyperparams.embedding_size,\n",
    "    )\n",
    "    train(model, data, hyperparams)\n",
    "\n",
    "    # Train, eval, save results and weights...\n",
    "\n",
    "\n",
    "# main()\n",
    "@dataclass\n",
    "class Hyperparameters:\n",
    "    lr_rate: float = 1e-3\n",
    "    weight_decay: str = 1e-5\n",
    "    epochs: int = 100\n",
    "    validation_frequency: int = 10\n",
    "    optimizer: Any = torch.optim.Adam\n",
    "    embedding_size: int = 5\n",
    "    # Add more here...\n",
    "\n",
    "\n",
    "# Load data\n",
    "\n",
    "# df_c = pd.read_csv(\"dataset/customers.csv\")\n",
    "# # Articles IDs all start with 0 which disappears if cast to a number\n",
    "# df_a = pd.read_csv(\"dataset/articles.csv\", dtype={\"article_id\": str})\n",
    "# df_t = pd.read_csv(\"dataset/transactions_train.csv\", dtype={\"article_id\": str})\n",
    "# df_c = clean_customer_data(df_c)\n",
    "\n",
    "# Transform to training and testing set\n",
    "dataset_params = {\n",
    "    \"total_cases\": 20,\n",
    "    \"portion_negatives\": 0.9,\n",
    "    \"df_transactions\": df_t,\n",
    "    \"df_articles\": df_a,\n",
    "    \"df_customers\": df_c,\n",
    "    \"train_portion\": 0.7,\n",
    "    \"batch_size\": 5,\n",
    "}\n",
    "hyperparams = Hyperparameters()\n",
    "data = Data_HM(**dataset_params)\n",
    "n_cust, n_art, _ = data.df_id.nunique()\n",
    "model = HM_model(\n",
    "    num_customer=n_cust,\n",
    "    num_articles=n_art,\n",
    "    embedding_size=hyperparams.embedding_size,\n",
    ")\n",
    "# train(model, data, hyperparams)\n",
    "\n",
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Long but expected Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8612\\2975187385.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhyperparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8612\\3189369598.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, data, params)\u001b[0m\n\u001b[0;32m     19\u001b[0m     )\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mtrain_one_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_metric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation_frequency\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8612\\3189369598.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, data, epoch_num, optimizer, loss)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;31m# TODO: MindData seem to pass along several IDs and not only one. Which is correct?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mloss_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\frede\\Desktop\\TMA4500-IndMat\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\frede\\Desktop\\TMA4500-IndMat\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 613\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    614\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\frede\\Desktop\\TMA4500-IndMat\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3081\u001b[0m         \u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3083\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3084\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3085\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Found dtype Long but expected Float"
     ]
    }
   ],
   "source": [
    "train(model, data, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id     int32\n",
       "article_id      int32\n",
       "label          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.df_id.dtypes\n",
    "# pd.DataFrame({\"a\": [1,2,3], \"b\": [4,5,6]}, dtype={\"a\": np.uint8, \"b\": np.int32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([14, 16, 19, 12, 13], dtype=torch.int32)\n",
      "tensor([ 9,  7, 11,  2,  4], dtype=torch.int32)\n",
      "tensor([ 6, 17, 10,  8], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "dl = data.get_DataLoader(trainDL=True)\n",
    "for el, lab in dl:\n",
    "    print(el[:,0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TMA4500-IndMat",
   "language": "python",
   "name": "tma4500-indmat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "402b62e985bc5250c3cc67917d34a79ef392b62d1143c3e95347f6ab24b3a3fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
