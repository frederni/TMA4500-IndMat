{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a recommender system with embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up the datasets\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "def load_min_data(filename: str | Iterable):\n",
    "    dfs = []\n",
    "    if isinstance(filename, str):\n",
    "        filename = [filename]\n",
    "    for fn in filename:\n",
    "        df = pd.read_csv(fn)\n",
    "        # All min-datasets have an index column which has to be dropped:\n",
    "        dfs.append(df.drop(df.columns[0], axis=1))\n",
    "    return dfs\n",
    "\n",
    "def clean_customer_data(df):\n",
    "    # df = df.drop(\"FN\", axis=1) # I they're not exactly equal\n",
    "    df.loc[\n",
    "        ~df[\"fashion_news_frequency\"].isin([\"Regularly\", \"Monthly\"]),\n",
    "        \"fashion_news_frequency\",\n",
    "    ] = \"None\"\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New data loading principle\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class Data_HM(Dataset):\n",
    "    \"\"\"This is the general HM Dataset class whose children are train-dataset and validation-dataset\n",
    "\n",
    "    Args:\n",
    "        Dataset: Abstract Dataset class from pyTorch\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        total_cases: int,\n",
    "        portion_negatives: float,\n",
    "        df_transactions: pd.DataFrame,\n",
    "        df_articles: pd.DataFrame,\n",
    "        df_customers: pd.DataFrame,\n",
    "        train_portion: float | None = None,\n",
    "        test_portion: float | None = None,\n",
    "    ) -> None:\n",
    "        super().__init__()  # TODO not sure if we need this\n",
    "        self.pos, self.neg = self.generate_dataset(\n",
    "            total_cases, portion_negatives, df_transactions\n",
    "        )\n",
    "        self.df = pd.concat(\n",
    "            [\n",
    "                self.merge_dfs_add_label(\n",
    "                    self.pos,\n",
    "                    df_articles,\n",
    "                    df_customers,\n",
    "                    positive=True,\n",
    "                ),\n",
    "                self.merge_dfs_add_label(\n",
    "                    self.neg,\n",
    "                    df_articles,\n",
    "                    df_customers,\n",
    "                    positive=False,\n",
    "                ),\n",
    "            ]\n",
    "        ).reset_index(drop=True)\n",
    "        self.train, self.test = self.split(train_portion, test_portion)\n",
    "\n",
    "    def generate_dataset(\n",
    "        self, total_cases: int, portion_negatives: float, df_transactions: pd.DataFrame\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Produce DataFrames for positive labels and generated negative samples\n",
    "\n",
    "        Args:\n",
    "            total_cases (int): Total number of transactions\n",
    "            portion_negatives (float): The portion of the `total_cases` that should be negative. Balanced 0/1 when 0.5\n",
    "            df_transactions (pd.DataFrame): Transactions to pull samples/generate samples from\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: _description_\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            0 <= portion_negatives <= 1\n",
    "        ), r\"portion negatives must be a float between 0%=0.0 and 100%=1.0!\"\n",
    "        n_positive = int(total_cases * (1 - portion_negatives))\n",
    "        n_negative = int(total_cases * portion_negatives)\n",
    "        df_positive = df_transactions.sample(n=n_positive).reset_index(drop=True)\n",
    "        df_positive = df_positive[[\"customer_id\", \"article_id\"]]\n",
    "\n",
    "        \n",
    "        # Sampling negative labels:\n",
    "        #   We select a random combination of `customer_id`, `article_id`, and ensure that this is not a true transaction.\n",
    "        #   Then we write this tuple to a csv which is transformed into a DataFrame similar to `df_positive`\n",
    "\n",
    "        num_written = 0\n",
    "        tmpStr = \"customer_id,article_id\\n\"\n",
    "        while num_written < n_negative:\n",
    "            # Choose random customer and article\n",
    "            selection = np.array(  # TODO this can probably be optimized further\n",
    "                [\n",
    "                    df_transactions[\"customer_id\"].sample().values,\n",
    "                    df_transactions[\"article_id\"].sample().values,\n",
    "                ]\n",
    "            ).flatten()\n",
    "            if not (\n",
    "                (df_transactions[\"customer_id\"] == selection[0])\n",
    "                & (df_transactions[\"article_id\"] == selection[1])\n",
    "            ).any():\n",
    "                tmpStr += f\"{selection[0]}, {selection[1]}\\n\"\n",
    "                num_written += 1\n",
    "        with open(\"tmp.csv\", \"w\") as f:\n",
    "            f.write(tmpStr)\n",
    "        df_negative = pd.read_csv(\"tmp.csv\")\n",
    "        os.remove(\"tmp.csv\")\n",
    "        return df_positive, df_negative\n",
    "\n",
    "    def merge_dfs_add_label(\n",
    "        self, df_transactions: pd.DataFrame, df_articles: pd.DataFrame, df_customers: pd.DataFrame, positive: bool = False\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Merge customer and article data to the sampled data `df_transactions`, excluding customer/article IDs\n",
    "\n",
    "        Args:\n",
    "            df_transactions (pd.DataFrame): DataFrame from `generate_dataset`\n",
    "            df_articles (pd.DataFrame): Articles DataFrame\n",
    "            df_customers (pd.DataFrame): Customers DataFrame\n",
    "            positive (bool, optional): Wether or not df_transactions represent positive labels. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DF with all columns included\n",
    "        \"\"\"\n",
    "        columns_articles = [\n",
    "            \"article_id\",\n",
    "            \"prod_name\",\n",
    "            \"product_type_name\",\n",
    "            \"product_group_name\",\n",
    "            \"graphical_appearance_name\",\n",
    "            \"colour_group_name\",\n",
    "            \"perceived_colour_value_name\",\n",
    "            \"perceived_colour_master_name\",\n",
    "            \"department_name\",\n",
    "            \"index_name\",\n",
    "            \"index_group_name\",\n",
    "            \"section_name\",\n",
    "            \"garment_group_name\",\n",
    "            \"detail_desc\",\n",
    "        ]\n",
    "        # TODO consider storing blacklisted cols instead of whitelisted\n",
    "\n",
    "        df_articles = df_articles[columns_articles]\n",
    "\n",
    "        df = pd.merge(\n",
    "            df_transactions, df_customers, how=\"inner\", on=[\"customer_id\"]\n",
    "        ).drop([\"customer_id\"], axis=1)\n",
    "        df = pd.merge(df, df_articles, how=\"inner\", on=[\"article_id\"]).drop(\n",
    "            [\"article_id\"], axis=1\n",
    "        )\n",
    "        df[\"label\"] = 1 if positive else 0\n",
    "        return df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row, label = self.df.iloc[idx, :-1], self.df.iloc[idx, -1]\n",
    "        if self.transform:\n",
    "            row = self.transform(row)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return row, label\n",
    "\n",
    "    def split(\n",
    "        self, train_portion: float | None = None, test_portion: float | None = None\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Split full dataset into training and validation set. Note that only one of train_portion or\n",
    "            test_portion are required (test_portion = 100% - test_portion)\n",
    "\n",
    "        Args:\n",
    "            train_portion (float | None, optional): Percentage of rows assigned to training set. Defaults to None.\n",
    "            test_portion (float | None, optional): Percentage of rows assigned to validation set. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: Train-set and validation-set\n",
    "        \"\"\"\n",
    "        assert any(\n",
    "            [train_portion, test_portion]\n",
    "        ), \"At least one of train or test portion must be float\"\n",
    "        if train_portion is None:\n",
    "            train_portion = 1-test_portion\n",
    "        train = self.df.sample(frac=train_portion)\n",
    "        test = (\n",
    "            pd.merge(self.df, train, indicator=True, how=\"outer\")\n",
    "            .query('_merge==\"left_only\"')\n",
    "            .drop(\"_merge\", axis=1)\n",
    "        )\n",
    "        return train.reset_index(drop=True), test.reset_index(drop=True)\n",
    "\n",
    "\n",
    "class HM_train(Data_HM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        total_cases,\n",
    "        portion_negatives,\n",
    "        df_transactions,\n",
    "        df_articles,\n",
    "        df_customers: pd.DataFrame,\n",
    "        train_portion=None,\n",
    "        test_portion=None,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            total_cases,\n",
    "            portion_negatives,\n",
    "            df_transactions,\n",
    "            df_articles,\n",
    "            df_customers,\n",
    "            train_portion,\n",
    "            test_portion,\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row, label = self.train.iloc[idx, :-1], self.train.iloc[idx, -1]\n",
    "        if self.transform:\n",
    "            row = self.transform(row)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return row, label\n",
    "\n",
    "\n",
    "class HM_val(Data_HM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        total_cases,\n",
    "        portion_negatives,\n",
    "        df_transactions,\n",
    "        df_articles,\n",
    "        df_customers: pd.DataFrame,\n",
    "        train_portion=None,\n",
    "        test_portion=None,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            total_cases,\n",
    "            portion_negatives,\n",
    "            df_transactions,\n",
    "            df_articles,\n",
    "            df_customers,\n",
    "            train_portion,\n",
    "            test_portion,\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row, label = self.test.iloc[idx, :-1], self.test.iloc[idx, -1]\n",
    "        if self.transform:\n",
    "            row = self.transform(row)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return row, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding models (same model as Mind Data example)\n",
    "\n",
    "\n",
    "class HM_model(torch.nn.Module):\n",
    "    def __init__(self, num_customer, num_transactions, embedding_size):\n",
    "        super(HM_model, self).__init__()\n",
    "        self.customer_embed = torch.nn.Embedding(\n",
    "            num_embeddings=num_customer, embedding_dim=embedding_size\n",
    "        )\n",
    "        self.trans_embed = torch.nn.Embedding(\n",
    "            num_embeddings=num_transactions, embedding_dim=embedding_size\n",
    "        )\n",
    "\n",
    "    def forward(self, users, items):\n",
    "        customer_embed = self.customer_embed(users)\n",
    "        trans_embed = self.trans_embed(items)\n",
    "        dot_prod = torch.sum(torch.mul(customer_embed, trans_embed), 1)\n",
    "        return torch.sigmoid(dot_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" Psudeo one epoch\n",
    "epoch_loss = 0\n",
    "* For each point in data\n",
    "    * retrieve column info + label and set to separate variables\n",
    "    * optimizer.zero_grad # Not sure if we should do this or not..\n",
    "    * compute prediction via model(row_info)\n",
    "    * compute loss_value via loss(prediction.view(-1), labels)\n",
    "    * loss.backward()\n",
    "    * optimizer.step()\n",
    "    * Potentially: LR scheduler.step()\n",
    "\n",
    "    epoch_loss += loss_value\n",
    "\n",
    "print(\"Epoch\", \"Loss\", \"Loss per data sample\", sep=\"\\t\")\n",
    "print(epoch+1, epoch_loss, epoch_loss/len(data), sep=\"\\t\")\n",
    "print(\"-\"*20)\n",
    "\"\"\"\n",
    "\n",
    "def train_one_epoch(model: HM_model, data, epoch_num: int, optimizer, loss):\n",
    "    epoch_loss = 0\n",
    "    for batch, row in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(row)\n",
    "        loss_value = loss(pred.view(-1), labels)\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss_value\n",
    "\n",
    "def train(model, train_DL, params):\n",
    "    # Uses binary cross entropy at the moment\n",
    "    loss_metric = torch.nn.BCELoss() # TODO change to MAP12 once the rest works\n",
    "\n",
    "    \"\"\" Psudeocode\n",
    "    * Initialize loss function and optimizer\n",
    "    * For epoch in epochs:\n",
    "        * Retrieve data from train_DL # Example uses custom sample_training_data \n",
    "        * train_one_epoch(...)\n",
    "        * Report eval statistics for each n-th epoch\n",
    "            * Both training accuracy and validation accuracy\n",
    "    \"\"\"\n",
    "def validate(model, DL, train=False):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "\n",
    "def main():\n",
    "    @dataclass\n",
    "    class Hyperparameters:\n",
    "        lr_rate: float = 1e-3\n",
    "        weight_decay: str = \"l2_reg\"\n",
    "        # Add more here...\n",
    "\n",
    "    # Load data\n",
    "    df_c, df_a, df_t = load_min_data(\n",
    "        [\n",
    "            f\"dataset_sample/{n}_min.csv\"\n",
    "            for n in (\"customer\", \"articles\", \"transactions\")\n",
    "        ]\n",
    "    )\n",
    "    df_c = clean_customer_data(df_c)\n",
    "\n",
    "    # Transform to training and testing set\n",
    "    dataset_params = {\n",
    "        \"total_cases\": 20,\n",
    "        \"portion_negatives\": 0.9,\n",
    "        \"df_transactions\": df_t,\n",
    "        \"df_articles\": df_a,\n",
    "        \"df_customers\": df_c,\n",
    "        \"train_portion\": 0.7,\n",
    "    }\n",
    "    data_train = HM_train(**dataset_params)\n",
    "    data_test = HM_val(**dataset_params)\n",
    "\n",
    "    model = HM_model(num_customer=20, num_transactions=20, embedding_size=5)\n",
    "\n",
    "    # Train, eval, save results and weights...\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('Proj': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "402b62e985bc5250c3cc67917d34a79ef392b62d1143c3e95347f6ab24b3a3fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
