{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a recommender system with embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up the datasets\n",
    "from typing import Iterable\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import Tuple, Any\n",
    "import sklearn.model_selection\n",
    "\n",
    "\n",
    "def load_min_data(filename: str | Iterable):\n",
    "    dfs = []\n",
    "    if isinstance(filename, str):\n",
    "        filename = [filename]\n",
    "    for fn in filename:\n",
    "        df = pd.read_csv(fn)\n",
    "        # All min-datasets have an index column which has to be dropped:\n",
    "        dfs.append(df.drop(df.columns[0], axis=1))\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def clean_customer_data(df):\n",
    "    # df = df.drop(\"FN\", axis=1) # I they're not exactly equal\n",
    "    df.loc[\n",
    "        ~df[\"fashion_news_frequency\"].isin([\"Regularly\", \"Monthly\"]),\n",
    "        \"fashion_news_frequency\",\n",
    "    ] = \"None\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the size of the data, it's important to generate negative labels in an efficient way. The function `pandas.DataFrame.sample()` takes almost five seconds for each sample, which is called at least `n_negative` times, we instead transform the dataframe to a NumPy array. Below is a comparison to highlight the importance of working with simpler objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_pd_vs_np(n_negative, df) -> Tuple[float, float]:\n",
    "    \"\"\"Compute time it takes to sample n_negative negative transactions\n",
    "\n",
    "    Args:\n",
    "        n_negative (int): Number of negative samples\n",
    "        df (pd.DataFrame): Dataframe to sample from, requires columns 'customer_id' and 'article_id'\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: Time taken using Pandas objects (first value) and NumPy objects (second value)\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    start_pd = time.time()\n",
    "    num_written = 0\n",
    "    tmpStr = \"customer_id,article_id\\n\"\n",
    "    while num_written < n_negative:\n",
    "        # Choose random customer and article\n",
    "        selection = np.array(\n",
    "            [\n",
    "                df[\"customer_id\"].sample().values,\n",
    "                df[\"article_id\"].sample().values,\n",
    "            ]\n",
    "        ).flatten()\n",
    "        if not (\n",
    "            (df[\"customer_id\"] == selection[0]) & (df[\"article_id\"] == selection[1])\n",
    "        ).any():\n",
    "            tmpStr += f\"{selection[0]}, {selection[1]}\\n\"\n",
    "            num_written += 1\n",
    "    with open(\"tmp.csv\", \"w\") as f:\n",
    "        f.write(tmpStr)\n",
    "    df_negative = pd.read_csv(\"tmp.csv\")\n",
    "    os.remove(\"tmp.csv\")\n",
    "    time_pd = time.time() - start_pd\n",
    "\n",
    "    # Numpy method\n",
    "    start_np = time.time()\n",
    "    df_np = df[[\"customer_id\", \"article_id\"]].to_numpy()\n",
    "    neg_np = np.empty((n_negative, df_np.shape[1]), dtype=\"<U64\")\n",
    "    for i in range(n_negative):\n",
    "        legit = False\n",
    "        while not legit:\n",
    "            sample = [np.random.choice(df_np[:, col]) for col in range(df_np.shape[1])]\n",
    "            legit = not ((df_np[:, 0] == sample[0]) & (df_np[:, 1] == sample[1])).any()\n",
    "        neg_np[i, :] = sample\n",
    "    time_np = time.time() - start_np\n",
    "\n",
    "    return time_pd, time_np\n",
    "\n",
    "\n",
    "def plot_negative_sampling(\n",
    "    start: int,\n",
    "    stop: int,\n",
    "    step: int = 1,\n",
    "    filename: str | None = None,\n",
    "    persist_data: bool = True,\n",
    "    cont_from_checkpoint: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"Plot the outputs of `time_pd_vs_np` for different ranges of n_negative\n",
    "\n",
    "    Args:\n",
    "        start (int): Range of n_negative (inclusive)\n",
    "        stop (int): Range of n_negative (exclusive)\n",
    "        step (int, optional): Step in range of n_negative. Defaults to 1.\n",
    "        filename (str | None, optional): Plot output file name, if None, does not save file. Defaults to None.\n",
    "        persist_data (bool, optional): Serialization option to store each iterate's result. Defaults to True.\n",
    "        cont_from_checkpoint (bool, optional): Reads previous runs and doesn't recompute if done before.\n",
    "                                                Defaults to True.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from tqdm import tqdm\n",
    "    import pickle\n",
    "\n",
    "    xax = list(range(start, stop, step))\n",
    "\n",
    "    if cont_from_checkpoint:\n",
    "        with open(\"plotData.pckl\", \"rb\") as f:\n",
    "            plot_values = pickle.load(f)\n",
    "\n",
    "        # Add empty list for keys not covered by checkpoint:\n",
    "        computed = set([x_i for x_i in plot_values.keys()])\n",
    "        to_add = set(xax) - computed\n",
    "        for elem in to_add:\n",
    "            plot_values[elem] = []\n",
    "\n",
    "        # Skip those already computed\n",
    "        xax = [x for x in xax if x not in computed]\n",
    "\n",
    "    else:\n",
    "        plot_values = {x_i: [] for x_i in xax}\n",
    "\n",
    "    for n_negative in tqdm(xax):\n",
    "        time_pd, time_np = time_pd_vs_np(n_negative)\n",
    "        plot_values[n_negative].extend([time_pd, time_np])\n",
    "\n",
    "        if persist_data:\n",
    "            with open(\"plotData.pckl\", \"wb\") as f:\n",
    "                pickle.dump(plot_values, f)\n",
    "\n",
    "    plt.plot(\n",
    "        plot_values.keys(),\n",
    "        plot_values.values(),\n",
    "        label=[\n",
    "            \"pandas.DataFrame.sample implementation\",\n",
    "            \"NumPy.random.choice implementation\",\n",
    "        ],\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Number of negative (generated) samples\")\n",
    "    plt.ylabel(\"Time [s]\")\n",
    "    plt.title(\"Comparison between sampling methods time\")\n",
    "    if filename is not None:\n",
    "        plt.savefig(f\"{filename}.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# plot_negative_sampling(\n",
    "#     start=1,\n",
    "#     stop=50,\n",
    "#     step=1,\n",
    "#     filename=\"Comp_1_to_50\",\n",
    "#     persist_data=True,\n",
    "#     cont_from_checkpoint=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "class Data_HM(Dataset):\n",
    "    \"\"\"This is the general HM Dataset class whose children are train-dataset and validation-dataset\n",
    "    no\n",
    "\n",
    "    Args:\n",
    "        Dataset: Abstract Dataset class from pyTorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        total_cases: int,\n",
    "        portion_negatives: float,\n",
    "        df_transactions: pd.DataFrame,\n",
    "        df_articles: pd.DataFrame,\n",
    "        df_customers: pd.DataFrame,\n",
    "        batch_size: int,\n",
    "        train_portion: float | None = None,\n",
    "        test_portion: float | None = None,\n",
    "        transform: Any = None,\n",
    "        target_transform: Any = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if train_portion is None:\n",
    "            if test_portion is None:\n",
    "                raise ValueError(\"Both train portion and test portion cannot be None.\")\n",
    "            self.train_portion = 1 - test_portion\n",
    "        self.batch_size = batch_size\n",
    "        self.df_id = self.generate_dataset(\n",
    "            total_cases, portion_negatives, df_transactions\n",
    "        )\n",
    "        self.train_portion = train_portion\n",
    "        self.train, self.val = self.split_dataset()\n",
    "        self.transform, self.target_transform = transform, target_transform\n",
    "\n",
    "    def generate_dataset(\n",
    "        self, total_cases: int, portion_negatives: float, df_transactions: pd.DataFrame\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Produce DataFrames for positive labels and generated negative samples\n",
    "\n",
    "        Args:\n",
    "            total_cases (int): Total number of transactions\n",
    "            portion_negatives (float): The portion of the `total_cases` that should be negative. Balanced 0/1 when 0.5\n",
    "            df_transactions (pd.DataFrame): Transactions to pull samples/generate samples from\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: _description_\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            0 <= portion_negatives <= 1\n",
    "        ), r\"portion negatives must be a float between 0%=0.0 and 100%=1.0!\"\n",
    "        n_positive = round(total_cases * (1 - portion_negatives))\n",
    "        n_negative = total_cases - n_positive\n",
    "\n",
    "        df_positive = df_transactions.sample(n=n_positive).reset_index(drop=True)\n",
    "        df_positive = df_positive[[\"customer_id\", \"article_id\"]]\n",
    "        df_positive[\"label\"] = 1\n",
    "\n",
    "        # Sampling negative labels:\n",
    "        #   We select a random combination of `customer_id`, `article_id`, and ensure that this is not a true transaction.\n",
    "        #   Then we make a 2-column dataframe on same form as `df_positive`\n",
    "\n",
    "        df_np = df_transactions[[\"customer_id\", \"article_id\"]].to_numpy()\n",
    "        neg_np = np.empty((n_negative, df_np.shape[1]), dtype=\"<U64\")\n",
    "        for i in range(n_negative):\n",
    "            legit = False\n",
    "            while not legit:\n",
    "                sample = [\n",
    "                    np.random.choice(df_np[:, col]) for col in range(df_np.shape[1])\n",
    "                ]\n",
    "                legit = not (\n",
    "                    (df_np[:, 0] == sample[0]) & (df_np[:, 1] == sample[1])\n",
    "                ).any()\n",
    "            neg_np[i, :] = sample\n",
    "        neg_np = np.column_stack((neg_np, [0] * neg_np.shape[0]))\n",
    "        df_negative = pd.DataFrame(neg_np, columns=df_positive.columns)\n",
    "        # Return a shuffled concatenation of the two dataframes\n",
    "        full_data = (\n",
    "            pd.concat((df_positive, df_negative)).sample(frac=1).reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        # Make label encodings of the IDs\n",
    "        le_cust = LabelEncoder()\n",
    "        le_art = LabelEncoder()\n",
    "        le_cust.fit(full_data[\"customer_id\"])\n",
    "        le_art.fit(full_data[\"article_id\"])\n",
    "        cust_encode = le_cust.transform(full_data[\"customer_id\"])\n",
    "        art_encode = le_art.transform(full_data[\"article_id\"])\n",
    "        return pd.DataFrame(\n",
    "            data={\n",
    "                \"customer_id\": cust_encode,\n",
    "                \"article_id\": art_encode,\n",
    "                \"label\": full_data[\"label\"].astype(np.uint8),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_id.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row, label = self.df_id.iloc[idx, :-1].values, self.df_id.iloc[idx, -1]\n",
    "        label = int(label)  # Stored as str for some reason\n",
    "        if self.transform:\n",
    "            row = self.transform(row)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return row, label\n",
    "\n",
    "    def split_dataset(self):\n",
    "        \"\"\"Split full data to train and validation Subset-objects\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Subset, Subset]: Train and validation subsets\n",
    "        \"\"\"\n",
    "        length = len(self)\n",
    "        train_size = int(length * self.train_portion)\n",
    "        valid_size = length - train_size\n",
    "        train, val = torch.utils.data.random_split(self, [train_size, valid_size])\n",
    "        return train, val\n",
    "\n",
    "    def get_data_from_subset(self, subset: torch.utils.data.Subset):\n",
    "        \"\"\"Not in use currently, but can retrieve data from Subset object directly\"\"\"\n",
    "        return subset.dataset.df_id.iloc[subset.indices]\n",
    "\n",
    "    def get_DataLoader(self, trainDL: bool = True):\n",
    "        subset = self.train if trainDL else self.val\n",
    "        return DataLoader(dataset=subset, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding model (same model as Mind Data example)\n",
    "\n",
    "\n",
    "class HM_model(torch.nn.Module):\n",
    "    def __init__(self, num_customer, num_articles, embedding_size):\n",
    "        super(HM_model, self).__init__()\n",
    "        self.customer_embed = torch.nn.Embedding(\n",
    "            num_embeddings=num_customer, embedding_dim=embedding_size\n",
    "        )\n",
    "        self.art_embed = torch.nn.Embedding(\n",
    "            num_embeddings=num_articles, embedding_dim=embedding_size\n",
    "        )\n",
    "\n",
    "    def forward(self, customer_row, article_row):\n",
    "        customer_embed = self.customer_embed(customer_row)\n",
    "        art_embed = self.art_embed(article_row)\n",
    "        dot_prod = torch.sum(torch.mul(customer_embed, art_embed), 1)\n",
    "        return torch.sigmoid(dot_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model: HM_model, data: Data_HM, epoch_num: int, optimizer, loss):\n",
    "    epoch_loss = 0\n",
    "    for row, label in data.get_DataLoader(trainDL=True):\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(row[:, 0], row[:, 1])\n",
    "        loss_value = loss(pred.view(-1), torch.FloatTensor(label.tolist()))\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss_value\n",
    "    print(f\"\\t| Training loss for epoch {epoch_num+1}: {epoch_loss}\")\n",
    "\n",
    "\n",
    "def train(model, data, params):\n",
    "    # Uses binary cross entropy at the moment\n",
    "    loss_metric = torch.nn.BCELoss()\n",
    "    optimizer = params.optimizer(\n",
    "        model.parameters(), lr=params.lr_rate, weight_decay=params.weight_decay\n",
    "    )\n",
    "    for epoch in range(params.epochs):\n",
    "        train_one_epoch(model, data, epoch, optimizer, loss_metric)\n",
    "        if not epoch % params.validation_frequency:\n",
    "\n",
    "            print(f\"Provisory results for epoch {epoch+1}:\")\n",
    "            print(\n",
    "                \"Loss for training set\",\n",
    "                validate(model, data, train=True),\n",
    "                sep=\"\\t\",\n",
    "            )\n",
    "            print(\n",
    "                \"Loss for validation set\",\n",
    "                validate(model, data, train=False),\n",
    "                sep=\"\\t\",\n",
    "            )\n",
    "            print(\"-\" * 20)\n",
    "\n",
    "\n",
    "import utils.metrics as metric\n",
    "import importlib\n",
    "\n",
    "importlib.reload(metric)\n",
    "\n",
    "\n",
    "def validate(model, data, train):\n",
    "    with torch.no_grad():\n",
    "        preds, labels = [], []\n",
    "        for row, label in data.get_DataLoader(trainDL=train):\n",
    "            pred_i = model(row[:, 0], row[:, 1]).view(-1)\n",
    "            preds.append(pred_i.detach().numpy())\n",
    "            labels.append(label.detach().numpy())\n",
    "        preds = [p.round() for p in preds]\n",
    "        return metric.MAPk(\n",
    "            k=12, preds=np.array(preds), true=np.array(labels)\n",
    "        ) \n",
    "\n",
    "\n",
    "def save_dataset_obj(data: HM_model, dst: str) -> None:\n",
    "    import pickle, os\n",
    "\n",
    "    if not os.path.isdir(os.path.dirname(dst)):\n",
    "        os.makedirs(os.path.dirname(dst))\n",
    "    with open(dst, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "def read_dataset_obj(src: str) -> Any:\n",
    "    import pickle\n",
    "\n",
    "    with open(src, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t| Training loss for epoch 1: 280.9172058105469\n",
      "Provisory results for epoch 1:\n",
      "Loss for training set\t0.24448082010582012\n",
      "Loss for validation set\t0.22918362694404362\n",
      "--------------------\n",
      "\t| Training loss for epoch 2: 270.80908203125\n",
      "\t| Training loss for epoch 3: 258.5587463378906\n",
      "\t| Training loss for epoch 4: 246.20457458496094\n",
      "\t| Training loss for epoch 5: 234.10345458984375\n",
      "\t| Training loss for epoch 6: 222.36279296875\n",
      "\t| Training loss for epoch 7: 211.0205078125\n",
      "\t| Training loss for epoch 8: 200.08872985839844\n",
      "\t| Training loss for epoch 9: 189.5693359375\n",
      "\t| Training loss for epoch 10: 179.457763671875\n",
      "\t| Training loss for epoch 11: 169.74777221679688\n",
      "Provisory results for epoch 11:\n",
      "Loss for training set\t0.28143563612313616\n",
      "Loss for validation set\t0.25748807519640854\n",
      "--------------------\n",
      "\t| Training loss for epoch 12: 160.43215942382812\n",
      "\t| Training loss for epoch 13: 151.50314331054688\n",
      "\t| Training loss for epoch 14: 142.9530487060547\n",
      "\t| Training loss for epoch 15: 134.77349853515625\n",
      "\t| Training loss for epoch 16: 126.95683288574219\n",
      "\t| Training loss for epoch 17: 119.49467468261719\n",
      "\t| Training loss for epoch 18: 112.37871551513672\n",
      "\t| Training loss for epoch 19: 105.60057830810547\n",
      "\t| Training loss for epoch 20: 99.15167999267578\n",
      "\t| Training loss for epoch 21: 93.0234146118164\n",
      "Provisory results for epoch 21:\n",
      "Loss for training set\t0.2993862949220092\n",
      "Loss for validation set\t0.25776044171877505\n",
      "--------------------\n",
      "\t| Training loss for epoch 22: 87.20668029785156\n",
      "\t| Training loss for epoch 23: 81.69244384765625\n",
      "\t| Training loss for epoch 24: 76.47139739990234\n",
      "\t| Training loss for epoch 25: 71.53402709960938\n",
      "\t| Training loss for epoch 26: 66.87078857421875\n",
      "\t| Training loss for epoch 27: 62.471858978271484\n",
      "\t| Training loss for epoch 28: 58.32748031616211\n",
      "\t| Training loss for epoch 29: 54.42790603637695\n",
      "\t| Training loss for epoch 30: 50.763301849365234\n",
      "\t| Training loss for epoch 31: 47.32396697998047\n",
      "Provisory results for epoch 31:\n",
      "Loss for training set\t0.312147506098399\n",
      "Loss for validation set\t0.25860088985088986\n",
      "--------------------\n",
      "\t| Training loss for epoch 32: 44.100101470947266\n",
      "\t| Training loss for epoch 33: 41.082069396972656\n",
      "\t| Training loss for epoch 34: 38.26015090942383\n",
      "\t| Training loss for epoch 35: 35.624977111816406\n",
      "\t| Training loss for epoch 36: 33.167179107666016\n",
      "\t| Training loss for epoch 37: 30.877559661865234\n",
      "\t| Training loss for epoch 38: 28.747173309326172\n",
      "\t| Training loss for epoch 39: 26.76720428466797\n",
      "\t| Training loss for epoch 40: 24.92911148071289\n",
      "\t| Training loss for epoch 41: 23.224666595458984\n",
      "Provisory results for epoch 41:\n",
      "Loss for training set\t0.31546386483886485\n",
      "Loss for validation set\t0.25860088985088986\n",
      "--------------------\n",
      "\t| Training loss for epoch 42: 21.645811080932617\n",
      "\t| Training loss for epoch 43: 20.184865951538086\n",
      "\t| Training loss for epoch 44: 18.834440231323242\n",
      "\t| Training loss for epoch 45: 17.58740997314453\n",
      "\t| Training loss for epoch 46: 16.436979293823242\n",
      "\t| Training loss for epoch 47: 15.376619338989258\n",
      "\t| Training loss for epoch 48: 14.400137901306152\n",
      "\t| Training loss for epoch 49: 13.501636505126953\n",
      "\t| Training loss for epoch 50: 12.675520896911621\n",
      "\t| Training loss for epoch 51: 11.916522979736328\n",
      "Provisory results for epoch 51:\n",
      "Loss for training set\t0.31753422833779976\n",
      "Loss for validation set\t0.25860088985088986\n",
      "--------------------\n",
      "\t| Training loss for epoch 52: 11.21970272064209\n",
      "\t| Training loss for epoch 53: 10.580428123474121\n",
      "\t| Training loss for epoch 54: 9.994339942932129\n",
      "\t| Training loss for epoch 55: 9.457425117492676\n",
      "\t| Training loss for epoch 56: 8.965885162353516\n",
      "\t| Training loss for epoch 57: 8.516218185424805\n",
      "\t| Training loss for epoch 58: 8.105141639709473\n",
      "\t| Training loss for epoch 59: 7.729626178741455\n",
      "\t| Training loss for epoch 60: 7.386839866638184\n",
      "\t| Training loss for epoch 61: 7.074180603027344\n",
      "Provisory results for epoch 61:\n",
      "Loss for training set\t0.31793696961107676\n",
      "Loss for validation set\t0.25860088985088986\n",
      "--------------------\n",
      "\t| Training loss for epoch 62: 6.7892165184021\n",
      "\t| Training loss for epoch 63: 6.529712677001953\n",
      "\t| Training loss for epoch 64: 6.293593406677246\n",
      "\t| Training loss for epoch 65: 6.0789337158203125\n",
      "\t| Training loss for epoch 66: 5.88395357131958\n",
      "\t| Training loss for epoch 67: 5.7070136070251465\n",
      "\t| Training loss for epoch 68: 5.5465850830078125\n",
      "\t| Training loss for epoch 69: 5.401261329650879\n",
      "\t| Training loss for epoch 70: 5.269745349884033\n",
      "\t| Training loss for epoch 71: 5.150832176208496\n",
      "Provisory results for epoch 71:\n",
      "Loss for training set\t0.31793696961107676\n",
      "Loss for validation set\t0.25860088985088986\n",
      "--------------------\n",
      "\t| Training loss for epoch 72: 5.043423175811768\n",
      "\t| Training loss for epoch 73: 4.9465012550354\n",
      "\t| Training loss for epoch 74: 4.859139442443848\n",
      "\t| Training loss for epoch 75: 4.7804741859436035\n",
      "\t| Training loss for epoch 76: 4.709720611572266\n",
      "\t| Training loss for epoch 77: 4.64616060256958\n",
      "\t| Training loss for epoch 78: 4.589132785797119\n",
      "\t| Training loss for epoch 79: 4.538027763366699\n",
      "\t| Training loss for epoch 80: 4.492290019989014\n",
      "\t| Training loss for epoch 81: 4.451408386230469\n",
      "Provisory results for epoch 81:\n",
      "Loss for training set\t0.31793696961107676\n",
      "Loss for validation set\t0.25860088985088986\n",
      "--------------------\n",
      "\t| Training loss for epoch 82: 4.414919853210449\n",
      "\t| Training loss for epoch 83: 4.382392406463623\n",
      "\t| Training loss for epoch 84: 4.353431701660156\n",
      "\t| Training loss for epoch 85: 4.327690124511719\n",
      "\t| Training loss for epoch 86: 4.304840087890625\n",
      "\t| Training loss for epoch 87: 4.2845892906188965\n",
      "\t| Training loss for epoch 88: 4.266671657562256\n",
      "\t| Training loss for epoch 89: 4.250845432281494\n",
      "\t| Training loss for epoch 90: 4.236894130706787\n",
      "\t| Training loss for epoch 91: 4.224618911743164\n",
      "Provisory results for epoch 91:\n",
      "Loss for training set\t0.31793696961107676\n",
      "Loss for validation set\t0.25860088985088986\n",
      "--------------------\n",
      "\t| Training loss for epoch 92: 4.213842391967773\n",
      "\t| Training loss for epoch 93: 4.204399585723877\n",
      "\t| Training loss for epoch 94: 4.196141719818115\n",
      "\t| Training loss for epoch 95: 4.188934803009033\n",
      "\t| Training loss for epoch 96: 4.18265438079834\n",
      "\t| Training loss for epoch 97: 4.1771931648254395\n",
      "\t| Training loss for epoch 98: 4.172451019287109\n",
      "\t| Training loss for epoch 99: 4.16834020614624\n",
      "\t| Training loss for epoch 100: 4.1647844314575195\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def main(use_min_dataset: bool = False, persisted_dataset_path: str | None = True):\n",
    "    @dataclass\n",
    "    class Hyperparameters:\n",
    "        lr_rate: float = 1e-3  # TODO consider dynamically changing lr\n",
    "        weight_decay: str = 1e-5\n",
    "        epochs: int = 100\n",
    "        validation_frequency: int = 10\n",
    "        optimizer: Any = torch.optim.Adam\n",
    "        embedding_size: int = 5\n",
    "        # Add more here...\n",
    "\n",
    "    # Load data\n",
    "    if use_min_dataset:\n",
    "        df_c, df_a, df_t = load_min_data(\n",
    "            [\n",
    "                f\"dataset_sample/{n}_min.csv\"\n",
    "                for n in (\"customer\", \"articles\", \"transactions\")\n",
    "            ]\n",
    "        )\n",
    "    if persisted_dataset_path is None:\n",
    "        df_c = pd.read_csv(\"dataset/customers.csv\")\n",
    "        # Articles IDs all start with 0 which disappears if cast to a number\n",
    "        df_a = pd.read_csv(\"dataset/articles.csv\", dtype={\"article_id\": str})\n",
    "        df_t = pd.read_csv(\"dataset/transactions_train.csv\", dtype={\"article_id\": str})\n",
    "        df_c = clean_customer_data(df_c)\n",
    "\n",
    "        dataset_params = {\n",
    "            \"total_cases\": 2000,\n",
    "            \"portion_negatives\": 0.9,\n",
    "            \"df_transactions\": df_t,\n",
    "            \"df_articles\": df_a,\n",
    "            \"df_customers\": df_c,\n",
    "            \"train_portion\": 0.7,\n",
    "            \"batch_size\": 5,\n",
    "        }\n",
    "        data = Data_HM(**dataset_params)\n",
    "    else:\n",
    "        data = read_dataset_obj(persisted_dataset_path)\n",
    "    hyperparams = Hyperparameters()\n",
    "    n_cust, n_art, _ = data.df_id.nunique()\n",
    "    model = HM_model(\n",
    "        num_customer=n_cust,\n",
    "        num_articles=n_art,\n",
    "        embedding_size=hyperparams.embedding_size,\n",
    "    )\n",
    "\n",
    "    train(model, data, hyperparams)\n",
    "\n",
    "    # Train, eval, save results and weights...\n",
    "\n",
    "\n",
    "main(persisted_dataset_path=\"object_storage/HM_data.pckl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looking more into the objects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>1566</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1174</td>\n",
       "      <td>563</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1107</td>\n",
       "      <td>1256</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1017</td>\n",
       "      <td>310</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1948</td>\n",
       "      <td>1472</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  article_id  label\n",
       "0           58        1566      0\n",
       "1         1174         563      0\n",
       "2         1107        1256      0\n",
       "3         1017         310      0\n",
       "4         1948        1472      0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = read_dataset_obj(\"object_storage/HM_data.pckl\")\n",
    "dataset.df_id.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.df_id.describe().loc[[\"count\", \"min\", \"max\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>231</td>\n",
       "      <td>821</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>1062</td>\n",
       "      <td>953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>1251</td>\n",
       "      <td>991</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>970</td>\n",
       "      <td>152</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>1242</td>\n",
       "      <td>1122</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>1772</td>\n",
       "      <td>1584</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1845</th>\n",
       "      <td>1625</td>\n",
       "      <td>1583</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>187</td>\n",
       "      <td>493</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190</th>\n",
       "      <td>583</td>\n",
       "      <td>1850</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1220</td>\n",
       "      <td>1699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      customer_id  article_id  label\n",
       "468           231         821      0\n",
       "746          1062         953      0\n",
       "977          1251         991      0\n",
       "449           970         152      0\n",
       "1626         1242        1122      0\n",
       "...           ...         ...    ...\n",
       "1347         1772        1584      0\n",
       "1845         1625        1583      0\n",
       "584           187         493      0\n",
       "1190          583        1850      0\n",
       "93           1220        1699      0\n",
       "\n",
       "[1400 rows x 3 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.train.dataset.df_id.iloc[dataset.train.indices]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('Proj': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "402b62e985bc5250c3cc67917d34a79ef392b62d1143c3e95347f6ab24b3a3fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
