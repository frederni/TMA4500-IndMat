{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a recommender system with embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext jupyter_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Disable use of gpu\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "import fastai.collab, torch\n",
    "from typing import Any, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset from main embedding notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "\n",
    "# Data_HM definition has to be here for pickle to understand what object it loads in\n",
    "class Data_HM(Dataset):\n",
    "    \"\"\"This is the general HM Dataset class whose children are train-dataset and validation-dataset\n",
    "    no\n",
    "\n",
    "    Args:\n",
    "        Dataset: Abstract Dataset class from pyTorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        total_cases: int,\n",
    "        portion_negatives: float,\n",
    "        df_transactions: pd.DataFrame,\n",
    "        df_articles: pd.DataFrame,\n",
    "        df_customers: pd.DataFrame,\n",
    "        batch_size: int,\n",
    "        train_portion: float | None = None,\n",
    "        test_portion: float | None = None,\n",
    "        transform: Any = None,\n",
    "        target_transform: Any = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if train_portion is None:\n",
    "            if test_portion is None:\n",
    "                raise ValueError(\"Both train portion and test portion cannot be None.\")\n",
    "            self.train_portion = 1 - test_portion\n",
    "        self.batch_size = batch_size\n",
    "        self.df_id = self.generate_dataset(\n",
    "            total_cases, portion_negatives, df_transactions\n",
    "        )\n",
    "        self.train_portion = train_portion\n",
    "        self.train, self.val = self.split_dataset()\n",
    "        self.transform, self.target_transform = transform, target_transform\n",
    "\n",
    "    def generate_dataset(\n",
    "        self, total_cases: int, portion_negatives: float, df_transactions: pd.DataFrame\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Produce DataFrames for positive labels and generated negative samples\n",
    "\n",
    "        Args:\n",
    "            total_cases (int): Total number of transactions\n",
    "            portion_negatives (float): The portion of the `total_cases` that should be negative. Balanced 0/1 when 0.5\n",
    "            df_transactions (pd.DataFrame): Transactions to pull samples/generate samples from\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: _description_\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            0 <= portion_negatives <= 1\n",
    "        ), r\"portion negatives must be a float between 0%=0.0 and 100%=1.0!\"\n",
    "        n_positive = round(total_cases * (1 - portion_negatives))\n",
    "        n_negative = total_cases - n_positive\n",
    "\n",
    "        df_positive = df_transactions.sample(n=n_positive).reset_index(drop=True)\n",
    "        df_positive = df_positive[[\"customer_id\", \"article_id\"]]\n",
    "        df_positive[\"label\"] = 1\n",
    "\n",
    "        # Sampling negative labels:\n",
    "        #   We select a random combination of `customer_id`, `article_id`, and ensure that this is not a true transaction.\n",
    "        #   Then we make a 2-column dataframe on same form as `df_positive`\n",
    "\n",
    "        df_np = df_transactions[[\"customer_id\", \"article_id\"]].to_numpy()\n",
    "        neg_np = np.empty((n_negative, df_np.shape[1]), dtype=\"<U64\")\n",
    "        for i in range(n_negative):\n",
    "            legit = False\n",
    "            while not legit:\n",
    "                sample = [\n",
    "                    np.random.choice(df_np[:, col]) for col in range(df_np.shape[1])\n",
    "                ]\n",
    "                legit = not (\n",
    "                    (df_np[:, 0] == sample[0]) & (df_np[:, 1] == sample[1])\n",
    "                ).any()\n",
    "            neg_np[i, :] = sample\n",
    "        neg_np = np.column_stack((neg_np, [0] * neg_np.shape[0]))\n",
    "        df_negative = pd.DataFrame(neg_np, columns=df_positive.columns)\n",
    "        # Return a shuffled concatenation of the two dataframes\n",
    "        full_data = (\n",
    "            pd.concat((df_positive, df_negative)).sample(frac=1).reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        # Make label encodings of the IDs\n",
    "        le_cust = LabelEncoder()\n",
    "        le_art = LabelEncoder()\n",
    "        le_cust.fit(full_data[\"customer_id\"])\n",
    "        le_art.fit(full_data[\"article_id\"])\n",
    "        cust_encode = le_cust.transform(full_data[\"customer_id\"])\n",
    "        art_encode = le_art.transform(full_data[\"article_id\"])\n",
    "        return pd.DataFrame(\n",
    "            data={\n",
    "                \"customer_id\": cust_encode,\n",
    "                \"article_id\": art_encode,\n",
    "                \"label\": full_data[\"label\"].astype(np.uint8),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_id.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row, label = self.df_id.iloc[idx, :-1].values, self.df_id.iloc[idx, -1]\n",
    "        label = int(label)  # Stored as str initially\n",
    "        if self.transform:\n",
    "            row = self.transform(row)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return row, label\n",
    "\n",
    "    def split_dataset(self):\n",
    "        \"\"\"Split full data to train and validation Subset-objects\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Subset, Subset]: Train and validation subsets\n",
    "        \"\"\"\n",
    "        length = len(self)\n",
    "        train_size = int(length * self.train_portion)\n",
    "        valid_size = length - train_size\n",
    "        train, val = torch.utils.data.random_split(self, [train_size, valid_size])\n",
    "        return train, val\n",
    "\n",
    "    def get_data_from_subset(self, subset: torch.utils.data.Subset):\n",
    "        \"\"\"Not in use currently, but can retrieve data from Subset object directly\"\"\"\n",
    "        return subset.dataset.df_id.iloc[subset.indices]\n",
    "\n",
    "    def get_DataLoader(self, trainDL: bool = True):\n",
    "        subset = self.train if trainDL else self.val\n",
    "        return DataLoader(dataset=subset, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset_obj(src: str) -> Any:\n",
    "\n",
    "    with open(src, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "dataset = read_dataset_obj(\"object_storage/HM_data.pckl\")\n",
    "dls = fastai.collab.CollabDataLoaders.from_df(dataset.df_id, bs=8).to(\"cpu\")\n",
    "# Total of 2000 articles to use here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HM_baseline(torch.nn.Module):\n",
    "    \"\"\"Collborative filtering model with bias,\n",
    "    Modified to work with fastai\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_customer, num_articles, embedding_size):\n",
    "        super(HM_baseline, self).__init__()\n",
    "        self.customer_embed = torch.nn.Embedding(\n",
    "            num_embeddings=num_customer + 1, embedding_dim=embedding_size\n",
    "        )\n",
    "        self.art_embed = torch.nn.Embedding(\n",
    "            num_embeddings=num_articles + 1, embedding_dim=embedding_size\n",
    "        )\n",
    "        # self.customer_bias = torch.nn.Embedding(num_customer, 1)\n",
    "        # self.article_bias = torch.nn.Embedding(num_articles, 1)\n",
    "\n",
    "    def forward(self, row):\n",
    "        try:\n",
    "            customer_embed = self.customer_embed(row[:, 0])\n",
    "            art_embed = self.art_embed(row[:, 1])\n",
    "        except:\n",
    "            print(\"FAIIIL\")\n",
    "            print(f\"{row[:,0].shape = }\")\n",
    "            print(f\"{row[:,1].shape = }\")\n",
    "            raise IndexError\n",
    "        finally:\n",
    "\n",
    "            dot_prod = (customer_embed * art_embed).sum(dim=1, keepdim=True)\n",
    "            # Add bias nodes to model:\n",
    "            # dot_prod = (\n",
    "            #     dot_prod + self.customer_bias(row[:, 0]) + self.article_bias(row[:, 1])\n",
    "            # )\n",
    "            return torch.sigmoid(dot_prod).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_dataset_obj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5928\\193188196.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Revert to basic fitting scheme....\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_dataset_obj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"object_storage/HM_data.pckl\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mn_cust\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"customer_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mn_art\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"article_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfastai\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCollabDataLoaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'read_dataset_obj' is not defined"
     ]
    }
   ],
   "source": [
    "# Revert to basic fitting scheme....\n",
    "dataset = read_dataset_obj(\"object_storage/HM_data.pckl\")\n",
    "n_cust = len(dls.classes[\"customer_id\"])\n",
    "n_art = len(dls.classes[\"article_id\"])\n",
    "dls = fastai.collab.CollabDataLoaders.from_df(dataset.df_id, bs=64).to(\"cpu\")\n",
    "model = HM_baseline(n_cust, n_art, 50)\n",
    "learner = fastai.collab.Learner(dls, model, loss_func=fastai.losses.BCELossFlat())\n",
    "learner.fit_one_cycle(5, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_cust, n_art, _ = dls.all_cols.nunique()\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelParameters:\n",
    "    learn_rate_max: float = 0.01\n",
    "    weight_decay: float = 0.1\n",
    "    epochs: int = 5\n",
    "    emb_size: int = 600\n",
    "    batch_size: int = 64\n",
    "    lr_red_patience: int = 1\n",
    "    lr_red_factor = 10\n",
    "\n",
    "\n",
    "def test_hyperparams(params: ModelParameters, dataset):\n",
    "    # assert (\n",
    "    #     params.emb_size >= params.batch_size\n",
    "    # ), \"Embedding size cannot be smaller than batch size.\"\n",
    "    # dataset = read_dataset_obj(\"object_storage/HM_data.pckl\")\n",
    "    device = \"cpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dls = fastai.collab.CollabDataLoaders.from_df(\n",
    "        dataset.df_id, bs=params.batch_size\n",
    "    ).to(device)\n",
    "    n_cust = len(dls.classes[\"customer_id\"])\n",
    "    n_art = len(dls.classes[\"article_id\"])\n",
    "\n",
    "    learner = fastai.collab.Learner(\n",
    "        dls=dls,\n",
    "        model=HM_baseline(n_cust, n_art, params.emb_size).to(device),\n",
    "        loss_func=fastai.losses.BCELossFlat(),\n",
    "        cbs=[\n",
    "            fastai.callback.tracker.SaveModelCallback(),\n",
    "            fastai.callback.tracker.ReduceLROnPlateau(\n",
    "                patience=params.lr_red_patience, factor=params.lr_red_factor\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "    learner.fit_one_cycle(\n",
    "        n_epoch=params.epochs, lr_max=params.learn_rate_max, wd=params.weight_decay\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "    dataset = Data_HM(\n",
    "        2000,\n",
    "        0.7,\n",
    "        pd.read_csv(\"dataset/transactions_train.csv\", dtype={\"article_id\": str}),\n",
    "        pd.read_csv(\"dataset/articles.csv\", dtype={\"article_id\": str}),\n",
    "        pd.read_csv(\"dataset/customers.csv\", dtype={\"article_id\": str}),\n",
    "        batch_size=64,\n",
    "        train_portion=0.7,\n",
    "    )\n",
    "    for wd in (1e-4, 1e-3, 1e-2, 1e-1):\n",
    "        for emb_sz in (10, 50, 100, 500, 1000):\n",
    "            for batch_size in (1, 8, 32, 64, 128):\n",
    "                param = ModelParameters(\n",
    "                    weight_decay=wd, emb_size=emb_sz, batch_size=batch_size\n",
    "                )\n",
    "                test_hyperparams(param, dataset)\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('Proj': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "402b62e985bc5250c3cc67917d34a79ef392b62d1143c3e95347f6ab24b3a3fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
