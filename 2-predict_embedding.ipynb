{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a recommender system with embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up the datasets\n",
    "from typing import Iterable\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import Tuple, Any\n",
    "import sklearn.model_selection\n",
    "\n",
    "\n",
    "def load_min_data(filename: str | Iterable):\n",
    "    dfs = []\n",
    "    if isinstance(filename, str):\n",
    "        filename = [filename]\n",
    "    for fn in filename:\n",
    "        df = pd.read_csv(fn)\n",
    "        # All min-datasets have an index column which has to be dropped:\n",
    "        dfs.append(df.drop(df.columns[0], axis=1))\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def clean_customer_data(df):\n",
    "    # df = df.drop(\"FN\", axis=1) # I they're not exactly equal\n",
    "    df.loc[\n",
    "        ~df[\"fashion_news_frequency\"].isin([\"Regularly\", \"Monthly\"]),\n",
    "        \"fashion_news_frequency\",\n",
    "    ] = \"None\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the size of the data, it's important to generate negative labels in an efficient way. The function `pandas.DataFrame.sample()` takes almost five seconds for each sample, which is called at least `n_negative` times, we instead transform the dataframe to a NumPy array. Below is a comparison to highlight the importance of working with simpler objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_pd_vs_np(n_negative, df) -> Tuple[float, float]:\n",
    "    \"\"\"Compute time it takes to sample n_negative negative transactions\n",
    "\n",
    "    Args:\n",
    "        n_negative (int): Number of negative samples\n",
    "        df (pd.DataFrame): Dataframe to sample from, requires columns 'customer_id' and 'article_id'\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: Time taken using Pandas objects (first value) and NumPy objects (second value)\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    start_pd = time.time()\n",
    "    num_written = 0\n",
    "    tmpStr = \"customer_id,article_id\\n\"\n",
    "    while num_written < n_negative:\n",
    "        # Choose random customer and article\n",
    "        selection = np.array(\n",
    "            [\n",
    "                df[\"customer_id\"].sample().values,\n",
    "                df[\"article_id\"].sample().values,\n",
    "            ]\n",
    "        ).flatten()\n",
    "        if not (\n",
    "            (df[\"customer_id\"] == selection[0]) & (df[\"article_id\"] == selection[1])\n",
    "        ).any():\n",
    "            tmpStr += f\"{selection[0]}, {selection[1]}\\n\"\n",
    "            num_written += 1\n",
    "    with open(\"tmp.csv\", \"w\") as f:\n",
    "        f.write(tmpStr)\n",
    "    df_negative = pd.read_csv(\"tmp.csv\")\n",
    "    os.remove(\"tmp.csv\")\n",
    "    time_pd = time.time() - start_pd\n",
    "\n",
    "    # Numpy method\n",
    "    start_np = time.time()\n",
    "    df_np = df[[\"customer_id\", \"article_id\"]].to_numpy()\n",
    "    neg_np = np.empty((n_negative, df_np.shape[1]), dtype=\"<U64\")\n",
    "    for i in range(n_negative):\n",
    "        legit = False\n",
    "        while not legit:\n",
    "            sample = [np.random.choice(df_np[:, col]) for col in range(df_np.shape[1])]\n",
    "            legit = not ((df_np[:, 0] == sample[0]) & (df_np[:, 1] == sample[1])).any()\n",
    "        neg_np[i, :] = sample\n",
    "    time_np = time.time() - start_np\n",
    "\n",
    "    return time_pd, time_np\n",
    "\n",
    "\n",
    "def plot_negative_sampling(\n",
    "    start: int,\n",
    "    stop: int,\n",
    "    step: int = 1,\n",
    "    filename: str | None = None,\n",
    "    persist_data: bool = True,\n",
    "    cont_from_checkpoint: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"Plot the outputs of `time_pd_vs_np` for different ranges of n_negative\n",
    "\n",
    "    Args:\n",
    "        start (int): Range of n_negative (inclusive)\n",
    "        stop (int): Range of n_negative (exclusive)\n",
    "        step (int, optional): Step in range of n_negative. Defaults to 1.\n",
    "        filename (str | None, optional): Plot output file name, if None, does not save file. Defaults to None.\n",
    "        persist_data (bool, optional): Serialization option to store each iterate's result. Defaults to True.\n",
    "        cont_from_checkpoint (bool, optional): Reads previous runs and doesn't recompute if done before.\n",
    "                                                Defaults to True.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from tqdm import tqdm\n",
    "    import pickle\n",
    "\n",
    "    xax = list(range(start, stop, step))\n",
    "\n",
    "    if cont_from_checkpoint:\n",
    "        with open(\"plotData.pckl\", \"rb\") as f:\n",
    "            plot_values = pickle.load(f)\n",
    "\n",
    "        # Add empty list for keys not covered by checkpoint:\n",
    "        computed = set([x_i for x_i in plot_values.keys()])\n",
    "        to_add = set(xax) - computed\n",
    "        for elem in to_add:\n",
    "            plot_values[elem] = []\n",
    "\n",
    "        # Skip those already computed\n",
    "        xax = [x for x in xax if x not in computed]\n",
    "\n",
    "    else:\n",
    "        plot_values = {x_i: [] for x_i in xax}\n",
    "\n",
    "    for n_negative in tqdm(xax):\n",
    "        time_pd, time_np = time_pd_vs_np(n_negative)\n",
    "        plot_values[n_negative].extend([time_pd, time_np])\n",
    "\n",
    "        if persist_data:\n",
    "            with open(\"plotData.pckl\", \"wb\") as f:\n",
    "                pickle.dump(plot_values, f)\n",
    "\n",
    "    plt.plot(\n",
    "        plot_values.keys(),\n",
    "        plot_values.values(),\n",
    "        label=[\n",
    "            \"pandas.DataFrame.sample implementation\",\n",
    "            \"NumPy.random.choice implementation\",\n",
    "        ],\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Number of negative (generated) samples\")\n",
    "    plt.ylabel(\"Time [s]\")\n",
    "    plt.title(\"Comparison between sampling methods time\")\n",
    "    if filename is not None:\n",
    "        plt.savefig(f\"{filename}.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# plot_negative_sampling(\n",
    "#     start=1,\n",
    "#     stop=50,\n",
    "#     step=1,\n",
    "#     filename=\"Comp_1_to_50\",\n",
    "#     persist_data=True,\n",
    "#     cont_from_checkpoint=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "class Data_HM(Dataset):\n",
    "    \"\"\"This is the general HM Dataset class whose children are train-dataset and validation-dataset\n",
    "    no\n",
    "\n",
    "    Args:\n",
    "        Dataset: Abstract Dataset class from pyTorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        total_cases: int,\n",
    "        portion_negatives: float,\n",
    "        df_transactions: pd.DataFrame,\n",
    "        df_articles: pd.DataFrame,\n",
    "        df_customers: pd.DataFrame,\n",
    "        batch_size: int,\n",
    "        train_portion: float | None = None,\n",
    "        test_portion: float | None = None,\n",
    "        transform: Any = None,\n",
    "        target_transform: Any = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if train_portion is None:\n",
    "            if test_portion is None:\n",
    "                raise ValueError(\"Both train portion and test portion cannot be None.\")\n",
    "            self.train_portion = 1 - test_portion\n",
    "        self.batch_size = batch_size\n",
    "        self.df_id, self.le_cust, self.le_art = self.generate_dataset(\n",
    "            total_cases, portion_negatives, df_transactions\n",
    "        )\n",
    "        self.train_portion = train_portion\n",
    "        self.train, self.val = self.split_dataset()\n",
    "        self.transform, self.target_transform = transform, target_transform\n",
    "\n",
    "    def generate_dataset(\n",
    "        self, total_cases: int, portion_negatives: float, df_transactions: pd.DataFrame\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Produce DataFrames for positive labels and generated negative samples\n",
    "\n",
    "        Args:\n",
    "            total_cases (int): Total number of transactions\n",
    "            portion_negatives (float): The portion of the `total_cases` that should be negative. Balanced 0/1 when 0.5\n",
    "            df_transactions (pd.DataFrame): Transactions to pull samples/generate samples from\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: _description_\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            0 <= portion_negatives <= 1\n",
    "        ), r\"portion negatives must be a float between 0%=0.0 and 100%=1.0!\"\n",
    "        if total_cases is None:\n",
    "            total_cases = len(df_transactions)\n",
    "        n_positive = round(total_cases * (1 - portion_negatives))\n",
    "        n_negative = total_cases - n_positive\n",
    "\n",
    "        df_positive = df_transactions.sample(n=n_positive).reset_index(drop=True)\n",
    "        df_positive = df_positive[[\"customer_id\", \"article_id\"]]\n",
    "        df_positive[\"label\"] = 1\n",
    "\n",
    "        # Sampling negative labels:\n",
    "        #   We select a random combination of `customer_id`, `article_id`, and ensure that this is not a true transaction.\n",
    "        #   Then we make a 2-column dataframe on same form as `df_positive`\n",
    "\n",
    "        df_np = df_transactions[[\"customer_id\", \"article_id\"]].to_numpy()\n",
    "        neg_np = np.empty((n_negative, df_np.shape[1]), dtype=\"<U64\")\n",
    "        for i in range(n_negative):\n",
    "            legit = False\n",
    "            while not legit:\n",
    "                sample = [\n",
    "                    np.random.choice(df_np[:, col]) for col in range(df_np.shape[1])\n",
    "                ]\n",
    "                legit = not (\n",
    "                    (df_np[:, 0] == sample[0]) & (df_np[:, 1] == sample[1])\n",
    "                ).any()\n",
    "            neg_np[i, :] = sample\n",
    "        neg_np = np.column_stack((neg_np, [0] * neg_np.shape[0]))\n",
    "        df_negative = pd.DataFrame(neg_np, columns=df_positive.columns)\n",
    "        # Return a shuffled concatenation of the two dataframes\n",
    "        full_data = (\n",
    "            pd.concat((df_positive, df_negative)).sample(frac=1).reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        # Make label encodings of the IDs\n",
    "        le_cust = LabelEncoder()\n",
    "        le_art = LabelEncoder()\n",
    "        le_cust.fit(full_data[\"customer_id\"])\n",
    "        le_art.fit(full_data[\"article_id\"])\n",
    "        cust_encode = le_cust.transform(full_data[\"customer_id\"])\n",
    "        art_encode = le_art.transform(full_data[\"article_id\"])\n",
    "        return (\n",
    "            pd.DataFrame(\n",
    "                data={\n",
    "                    \"customer_id\": cust_encode,\n",
    "                    \"article_id\": art_encode,\n",
    "                    \"label\": full_data[\"label\"].astype(np.uint8),\n",
    "                }\n",
    "            ),\n",
    "            le_cust,\n",
    "            le_art,\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_id.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row, label = self.df_id.iloc[idx, :-1].values, self.df_id.iloc[idx, -1]\n",
    "        label = int(label)  # Stored as str initially\n",
    "        if self.transform:\n",
    "            row = self.transform(row)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return row, label\n",
    "\n",
    "    def split_dataset(self):\n",
    "        \"\"\"Split full data to train and validation Subset-objects\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Subset, Subset]: Train and validation subsets\n",
    "        \"\"\"\n",
    "        length = len(self)\n",
    "        train_size = int(length * self.train_portion)\n",
    "        valid_size = length - train_size\n",
    "        train, val = torch.utils.data.random_split(self, [train_size, valid_size])\n",
    "        return train, val\n",
    "\n",
    "    def get_data_from_subset(self, subset: torch.utils.data.Subset):\n",
    "        \"\"\"Not in use currently, but can retrieve data from Subset object directly\"\"\"\n",
    "        return subset.dataset.df_id.iloc[subset.indices]\n",
    "\n",
    "    def get_DataLoader(self, trainDL: bool = True):\n",
    "        subset = self.train if trainDL else self.val\n",
    "        return DataLoader(dataset=subset, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding model (same model as Mind Data example)\n",
    "\n",
    "\n",
    "class HM_model(torch.nn.Module):\n",
    "    def __init__(self, num_customer, num_articles, embedding_size):\n",
    "        super(HM_model, self).__init__()\n",
    "        self.customer_embed = torch.nn.Embedding(\n",
    "            num_embeddings=num_customer, embedding_dim=embedding_size\n",
    "        )\n",
    "        self.art_embed = torch.nn.Embedding(\n",
    "            num_embeddings=num_articles, embedding_dim=embedding_size\n",
    "        )\n",
    "        self.customer_bias = torch.nn.Embedding(num_customer, 1)\n",
    "        self.article_bias = torch.nn.Embedding(num_articles, 1)\n",
    "\n",
    "    def forward(self, customer_row, article_row):\n",
    "        customer_embed = self.customer_embed(customer_row)\n",
    "        art_embed = self.art_embed(article_row)\n",
    "        # dot_prod_old = torch.sum(torch.mul(customer_embed, art_embed), 1)\n",
    "        dot_prod = (customer_embed * art_embed).sum(dim=1, keepdim=True)\n",
    "        # Add bias nodes to model:\n",
    "        dot_prod = (\n",
    "            dot_prod + self.customer_bias(customer_row) + self.article_bias(article_row)\n",
    "        )\n",
    "        return torch.sigmoid(dot_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: HM_model,\n",
    "    data: Data_HM,\n",
    "    epoch_num: int,\n",
    "    optimizer,\n",
    "    loss,\n",
    "    lr_scheduler,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    epoch_loss = 0\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    for item in data.get_DataLoader(trainDL=True):\n",
    "        item = tuple(t.to(device) for t in item)\n",
    "        row, label = item\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(row[:, 0], row[:, 1])\n",
    "        loss_value = loss(pred.view(-1), torch.FloatTensor(label.tolist()).to(device))\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss_value\n",
    "    if verbose:\n",
    "        print(f\"\\t| Training loss for epoch {epoch_num+1}: {epoch_loss}\")\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def train(model, data, params):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # Uses binary cross entropy at the moment\n",
    "    loss_metric = torch.nn.BCELoss().to(device)\n",
    "    optimizer = params.optimizer(\n",
    "        model.parameters(), lr=params.lr_rate, weight_decay=params.weight_decay\n",
    "    )\n",
    "    # Adjust lr once model stops improving using scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "    save_loss = params.save_loss\n",
    "    if save_loss:\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        # Settings not in use atm but we can get the hyperparams from it:))\n",
    "        settings = \",\".join([str(v) for v in params.__dict__.values()])\n",
    "    for epoch in tqdm(range(params.epochs)):\n",
    "        model.train()\n",
    "        epoch_loss = train_one_epoch(\n",
    "            model, data, epoch, optimizer, loss_metric, lr_scheduler, params.verbose\n",
    "        )\n",
    "        if not epoch % params.validation_frequency:\n",
    "            # Validate step\n",
    "            model.eval()\n",
    "            valid_loss = 0.0\n",
    "            for item in data.get_DataLoader(trainDL=False):\n",
    "                item = tuple(t.to(device) for t in item)\n",
    "                row, label = item\n",
    "                pred = model(row[:, 0], row[:, 1])\n",
    "                loss = loss_metric(\n",
    "                    pred.view(-1), torch.FloatTensor(label.tolist()).to(device)\n",
    "                )\n",
    "                valid_loss = loss.item() * row.size(0)\n",
    "\n",
    "            lr_scheduler.step(valid_loss)  # Update lr scheduler\n",
    "            if params.verbose:\n",
    "                print(f\"Provisory results for epoch {epoch+1}:\")\n",
    "                print(\n",
    "                    \"Loss for training set\",\n",
    "                    epoch_loss.tolist() / len(data.get_DataLoader(trainDL=True)),\n",
    "                    sep=\"\\t\",\n",
    "                )\n",
    "                print(\n",
    "                    \"Loss for validation set\",\n",
    "                    valid_loss / len(data.get_DataLoader(trainDL=False)),\n",
    "                    sep=\"\\t\",\n",
    "                )\n",
    "                print(\"-\" * 20)\n",
    "            if save_loss:\n",
    "                train_losses.append(\n",
    "                    epoch_loss.tolist() / len(data.get_DataLoader(trainDL=True))\n",
    "                )\n",
    "                valid_losses.append(\n",
    "                    valid_loss / len(data.get_DataLoader(trainDL=False))\n",
    "                )\n",
    "    if save_loss:\n",
    "        fn_append = save_loss if isinstance(save_loss, str) else \"\"\n",
    "        save_dir = os.path.join(\"results\", datetime.today().strftime(\"%Y.%m.%d.%H.%M\"))\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        np.savetxt(\n",
    "            os.path.join(save_dir, f\"losses_{fn_append}.csv\"),\n",
    "            np.transpose([train_losses, valid_losses]),\n",
    "            delimiter=\",\",\n",
    "        )\n",
    "    return valid_loss / len(data.get_DataLoader(trainDL=False))\n",
    "\n",
    "\n",
    "import utils.metrics as metric\n",
    "import importlib\n",
    "\n",
    "importlib.reload(metric)  # To caputre changes in metrics module\n",
    "\n",
    "\n",
    "def save_dataset_obj(data: HM_model, dst: str) -> None:\n",
    "    import pickle, os\n",
    "\n",
    "    if not os.path.isdir(os.path.dirname(dst)):\n",
    "        os.makedirs(os.path.dirname(dst))\n",
    "    with open(dst, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "def read_dataset_obj(src: str) -> Any:\n",
    "    import pickle\n",
    "\n",
    "    with open(src, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def plot_loss_results(lossfile):\n",
    "    res = pd.read_csv(lossfile, header=None)\n",
    "    plt.plot(res[0], label=\"Training loss\")\n",
    "    plt.plot(res[1], label=\"Validation loss\")\n",
    "    plt.xlabel(\"Epoch number\")\n",
    "    plt.ylabel(\"Loss value\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "from tabnanny import verbose\n",
    "from typing import Any\n",
    "\n",
    "from numpy import isin\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Hyperparameters:\n",
    "    lr_rate: float = 1e-3\n",
    "    weight_decay: str = 1e-4\n",
    "    epochs: int = 20\n",
    "    validation_frequency: int = 1\n",
    "    optimizer: Any = torch.optim.Adam\n",
    "    embedding_size: int = 500\n",
    "    save_loss: bool | str = True\n",
    "    verbose: bool = False\n",
    "    dataset_cases: int = 2000\n",
    "    dataset_portion_negatives: float = 0.9\n",
    "    dataset_train_portion: float = 0.7\n",
    "    datset_batch_size: int = 5\n",
    "    # Add more here...\n",
    "\n",
    "\n",
    "def main(\n",
    "    use_min_dataset: bool = False,\n",
    "    persisted_dataset_path: str | None = None,\n",
    "    save_model: str | bool = False,\n",
    "    hyperparams=Hyperparameters(),\n",
    "    transactions_path: str = \"dataset/transactions_train.csv\",\n",
    "):\n",
    "\n",
    "    # Load data\n",
    "    if use_min_dataset:\n",
    "        df_c, df_a, df_t = load_min_data(\n",
    "            [\n",
    "                f\"dataset_sample/{n}_min.csv\"\n",
    "                for n in (\"customer\", \"articles\", \"transactions\")\n",
    "            ]\n",
    "        )\n",
    "    if persisted_dataset_path is None:\n",
    "        df_c = pd.read_csv(\"dataset/customers.csv\")\n",
    "        # Articles IDs all start with 0 which disappears if cast to a number\n",
    "        df_a = pd.read_csv(\"dataset/articles.csv\", dtype={\"article_id\": str})\n",
    "        df_t = pd.read_csv(transactions_path, dtype={\"article_id\": str})\n",
    "        df_c = clean_customer_data(df_c)\n",
    "\n",
    "        dataset_params = {\n",
    "            \"total_cases\": hyperparams.dataset_cases,\n",
    "            \"portion_negatives\": hyperparams.dataset_portion_negatives,\n",
    "            \"df_transactions\": df_t,\n",
    "            \"df_articles\": df_a,\n",
    "            \"df_customers\": df_c,\n",
    "            \"train_portion\": hyperparams.dataset_train_portion,\n",
    "            \"batch_size\": hyperparams.datset_batch_size,\n",
    "        }\n",
    "        data = Data_HM(**dataset_params)\n",
    "        save_dataset_obj(\n",
    "            data,\n",
    "            f\"object_storage/dataset-{datetime.today().strftime('%Y.%m.%d.%H.%M')}.pckl\",\n",
    "        )\n",
    "    else:\n",
    "        data = read_dataset_obj(persisted_dataset_path)\n",
    "    n_cust, n_art, _ = data.df_id.nunique()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = HM_model(\n",
    "        num_customer=n_cust,\n",
    "        num_articles=n_art,\n",
    "        embedding_size=hyperparams.embedding_size,\n",
    "    )\n",
    "    model = model.to(device)\n",
    "\n",
    "    last_valid_loss = train(model, data, hyperparams)\n",
    "    if save_model:\n",
    "        if isinstance(save_model, bool):\n",
    "            save_model = datetime.today().strftime(\"%Y.%m.%d.%H.%M\") + \".pth\"\n",
    "        torch.save(model.state_dict(), os.path.join(\"models\", save_model))\n",
    "    return last_valid_loss\n",
    "\n",
    "\n",
    "# main(persisted_dataset_path=\"object_storage/HM_data.pckl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot_loss_results(\"results/2022.10.19.11.44/losses.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Explore different hyperparameters and compare\n",
    "def heuristic_embedding_size(cardinality):\n",
    "    # https://github.com/fastai/fastai/blob/master/fastai/tabular/model.py#L12\n",
    "    return min(600, round(1.6 * cardinality**0.56))\n",
    "\n",
    "\n",
    "def explore_hyperparameters():\n",
    "    weight_decays = (1e-4, 1e-3, 1e-2, 1e-1)\n",
    "    embedding_size = (10, 100, 500, 1000, int(1e4))\n",
    "    lr_rates = (1e-5, 1e-4, 1e-3, 1e-2)\n",
    "    print(\"Testing EMBEDDING SIZE\")\n",
    "    for emb_size in embedding_size:\n",
    "        testing_param = f\"{emb_size = }\"\n",
    "        print(testing_param)\n",
    "        hparams = Hyperparameters(embedding_size=emb_size, save_loss=testing_param)\n",
    "        main(persisted_dataset_path=\"object_storage/HM_data.pckl\", hyperparams=hparams)\n",
    "    print(\"Testing WEIGHT DECAYS\")\n",
    "    for wd in weight_decays:\n",
    "        testing_param = f\"{wd = }\"\n",
    "        print(testing_param)\n",
    "        hparams = Hyperparameters(weight_decay=wd, save_loss=testing_param)\n",
    "        main(persisted_dataset_path=\"object_storage/HM_data.pckl\", hyperparams=hparams)\n",
    "    for lr in lr_rates:\n",
    "        testing_param = f\"{lr = }\"\n",
    "        print(testing_param)\n",
    "        hparams = Hyperparameters(lr_rate=lr, save_loss=testing_param)\n",
    "        main(persisted_dataset_path=\"object_storage/HM_data.pckl\", hyperparams=hparams)\n",
    "\n",
    "\n",
    "# explore_hyperparameters()\n",
    "\n",
    "\n",
    "def alternative_hyperparam_exploration(wds, embszs, lrs, model_path=None):\n",
    "    best_model = {}\n",
    "    best_loss = np.inf\n",
    "    for emb_size in embszs:\n",
    "        for wd in wds:\n",
    "            for lr in lrs:\n",
    "                params = {\n",
    "                    \"embedding_size\": emb_size,\n",
    "                    \"epochs\": 20,\n",
    "                    \"save_loss\": False,\n",
    "                    \"weight_decay\": wd,\n",
    "                    \"lr_rate\": lr,\n",
    "                }\n",
    "                loss = main(\n",
    "                    persisted_dataset_path=model_path,\n",
    "                    hyperparams=Hyperparameters(**params),\n",
    "                )\n",
    "                print(\"Last epoch loss\", loss)\n",
    "                if loss < best_loss:\n",
    "                    best_model = params\n",
    "                    best_loss = loss\n",
    "                    print(\"Found better model\", best_model)\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looking more into the objects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "\n",
    "def compare_hyperparameter_results(data: Iterable[str]):\n",
    "    plt_calls = {}\n",
    "    for results_fn in data:\n",
    "        plt.xlabel(\"Epoch number\")\n",
    "        plt.ylabel(\"Loss value\")\n",
    "        res = pd.read_csv(results_fn, header=None)\n",
    "        setting = results_fn[results_fn.find(\"_\") + 1 :]\n",
    "        variable = setting.split(\"=\")[0]\n",
    "        if variable in plt_calls:\n",
    "            plt_calls[variable].append(\n",
    "                functools.partial(plt.plot, res[1], label=setting)\n",
    "            )\n",
    "        else:\n",
    "            plt_calls[variable] = [functools.partial(plt.plot, res[0], label=setting)]\n",
    "    for var, cmd_list in plt_calls.items():\n",
    "        for cmd in cmd_list:\n",
    "            cmd()  # plt.plot(...) for specific variable\n",
    "        plt.legend()\n",
    "        plt.yscale(\"log\")\n",
    "        plt.title(f\"Training loss loss for different choices of {var}\")\n",
    "        plt.savefig(f\"hyperparams_train_{var}.pdf\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "files_20epochs = [\n",
    "    \"results/2022.10.24.11.33/losses_emb_size = 10.csv\",\n",
    "    \"results/2022.10.24.11.34/losses_emb_size = 100.csv\",\n",
    "    \"results/2022.10.24.11.39/losses_emb_size = 500.csv\",\n",
    "    \"results/2022.10.24.11.47/losses_emb_size = 1000.csv\",\n",
    "    \"results/2022.10.24.13.29/losses_wd = 0.0001.csv\",\n",
    "    \"results/2022.10.24.13.33/losses_wd = 0.001.csv\",\n",
    "    \"results/2022.10.24.13.37/losses_wd = 0.01.csv\",\n",
    "    \"results/2022.10.24.13.50/losses_lr = 0.001.csv\",\n",
    "    \"results/2022.10.24.13.56/losses_lr = 0.01.csv\",\n",
    "]\n",
    "files = [\n",
    "    \"results/2022.10.30.21.21/losses_emb_size = 10.csv\",\n",
    "    \"results/2022.10.30.21.23/losses_emb_size = 100.csv\",\n",
    "    \"results/2022.10.30.21.24/losses_emb_size = 500.csv\",\n",
    "    \"results/2022.10.30.21.26/losses_emb_size = 1000.csv\",\n",
    "    \"results/2022.10.30.21.32/losses_emb_size = 10000.csv\",\n",
    "    \"results/2022.10.30.21.33/losses_wd = 0.0001.csv\",\n",
    "    \"results/2022.10.30.21.34/losses_wd = 0.001.csv\",\n",
    "    \"results/2022.10.30.21.36/losses_wd = 0.01.csv\",\n",
    "    \"results/2022.10.30.21.37/losses_wd = 0.1.csv\",\n",
    "    \"results/2022.10.30.21.39/losses_lr = 1e-05.csv\",\n",
    "    \"results/2022.10.30.21.40/losses_lr = 0.0001.csv\",\n",
    "    \"results/2022.10.30.21.42/losses_lr = 0.001.csv\",\n",
    "    \"results/2022.10.30.21.43/losses_lr = 0.01.csv\",\n",
    "]\n",
    "# compare_hyperparameter_results(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:11<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 2.4900793075561523\n",
      "Found better model {'embedding_size': 10, 'epochs': 20, 'save_loss': False, 'weight_decay': 0.0001, 'lr_rate': 1e-05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:10<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 3.4742016792297363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:10<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 3.6106879234313967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:11<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 1.7028180599212646\n",
      "Found better model {'embedding_size': 10, 'epochs': 20, 'save_loss': False, 'weight_decay': 0.0001, 'lr_rate': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:11<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 6.141492462158203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:11<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 3.811809539794922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:11<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 1.8474946975708009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:13<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 1.6697128772735597\n",
      "Found better model {'embedding_size': 10, 'epochs': 20, 'save_loss': False, 'weight_decay': 0.001, 'lr_rate': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:16<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 3.840830898284912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:13<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 3.9673602104187013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:13<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 2.5104323387145997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:13<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 1.6639936923980714\n",
      "Found better model {'embedding_size': 10, 'epochs': 20, 'save_loss': False, 'weight_decay': 0.01, 'lr_rate': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:12<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 2.922794151306152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:14<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 3.4021302223205567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:16<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 2.935817241668701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:12<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 1.6782650470733642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:20<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 16.814336013793945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:22<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 9.943900680541992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:18<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 8.002048301696778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:16<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 1.6543161392211914\n",
      "Found better model {'embedding_size': 100, 'epochs': 20, 'save_loss': False, 'weight_decay': 0.0001, 'lr_rate': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:14<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 6.150139045715332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:14<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 7.272113800048828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:17<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 13.361962509155273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:18<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 1.6554449558258058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:18<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 3.29329833984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:16<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 15.379697799682617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:14<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 2.5301499366760254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:14<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 1.6550323963165283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:16<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 7.9881792068481445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:15<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 26.26534652709961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:18<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 5.791460037231445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:21<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 1.6631458282470704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:27<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 65.9300308227539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:23<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 35.87611770629883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:26<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 44.172541809082034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:27<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 1.6485471725463867\n",
      "Found better model {'embedding_size': 500, 'epochs': 20, 'save_loss': False, 'weight_decay': 0.0001, 'lr_rate': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:29<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 58.971153259277344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:25<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 57.78805389404297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:26<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 35.0969123840332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:31<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 1.6560801029205323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:24<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 44.88863067626953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:24<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 39.648468017578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:23<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 24.752426147460938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:25<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 1.6570525646209717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:24<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 52.61857452392578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:22<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 59.15065155029297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:23<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 47.98751678466797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:23<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 1.6635149002075196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:41<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 54.55704345703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:31<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 86.34003295898438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:31<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 34.288011932373045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:30<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 1.656031608581543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:30<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 47.80487823486328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:31<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 53.82985382080078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:36<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 11.107667541503906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:33<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 1.6767711639404297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:32<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 75.02671508789062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:32<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 54.35316009521485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:32<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 10.408576583862304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:30<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 1.6561115741729737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:32<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 69.4107421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:32<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 99.67109069824218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:35<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 59.38119964599609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:32<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 1.6582161426544189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:31<00:00, 10.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 80.98785095214843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:58<00:00, 11.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 61.735546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:28<00:00, 10.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 112.81108703613282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:57<00:00, 11.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 1.7513917922973632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:22<00:00, 10.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 111.52493591308594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:20<00:00, 10.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 122.21600646972657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:44<00:00, 11.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 74.4575912475586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:18<00:00,  9.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 1.6586201190948486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:34<00:00, 10.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 74.94165802001953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:27<00:00, 10.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 91.62860412597657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [04:04<00:00, 12.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 109.97106628417968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:15<00:00,  9.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 1.6775054454803466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:16<00:00,  9.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 104.99858093261719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:26<00:00, 10.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 94.61934814453124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:26<00:00, 10.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 125.88479919433594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:14<00:00,  9.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last epoch loss 1.6540914058685303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'embedding_size': 500,\n",
       " 'epochs': 20,\n",
       " 'save_loss': False,\n",
       " 'weight_decay': 0.0001,\n",
       " 'lr_rate': 0.01}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alternative_hyperparam_exploration(\n",
    "#     wds=(1e-4, 1e-3, 1e-2, 1e-1),\n",
    "#     embszs=(10, 100, 500, 1000, int(1e4)),\n",
    "#     lrs=(1e-5, 1e-4, 1e-3, 1e-2),\n",
    "#     model_path=\"object_storage/dataset-full.pckl\",\n",
    "# )\n",
    "# # With 100k data 'embedding_size': 500, 'epochs': 20,  'weight_decay': 0.0001, 'lr_rate': 0.01 is the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18576\\3484870005.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Finally one last training with best hyperparameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m _ = main(\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0msave_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# transactions_path=\"dataset/tr_train.csv\",\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     hyperparams=Hyperparameters(\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18576\\1024188958.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m(use_min_dataset, persisted_dataset_path, save_model, hyperparams, transactions_path)\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;34m\"batch_size\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhyperparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatset_batch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         }\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mData_HM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdataset_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         save_dataset_obj(\n\u001b[0;32m     59\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18576\\2851898721.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, total_cases, portion_negatives, df_transactions, df_articles, df_customers, batch_size, train_portion, test_portion, transform, target_transform)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_portion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtest_portion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         self.df_id, self.le_cust, self.le_art = self.generate_dataset(\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[0mtotal_cases\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mportion_negatives\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_transactions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         )\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18576\\2851898721.py\u001b[0m in \u001b[0;36mgenerate_dataset\u001b[1;34m(self, total_cases, portion_negatives, df_transactions)\u001b[0m\n\u001b[0;32m     76\u001b[0m                 legit = not (\n\u001b[0;32m     77\u001b[0m                     \u001b[1;33m(\u001b[0m\u001b[0mdf_np\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdf_np\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m                 ).any()\n\u001b[0m\u001b[0;32m     79\u001b[0m             \u001b[0mneg_np\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mneg_np\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneg_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mneg_np\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\frede\\Desktop\\TMA4500-IndMat\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_any\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mumr_prod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0m_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m     \u001b[1;31m# Parsing keyword arguments is currently fairly slow, so avoid it for now\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwhere\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Finally one last training with best hyperparameters\n",
    "_ = main(\n",
    "    save_model=True,\n",
    "    # transactions_path=\"dataset/tr_train.csv\",\n",
    "    hyperparams=Hyperparameters(\n",
    "        epochs=20,\n",
    "        weight_decay=0.0001,\n",
    "        lr_rate=0.01,\n",
    "        embedding_size=500,\n",
    "        save_loss=True,\n",
    "        dataset_cases=100_000,  # TODO change when ready for the entire chablam\n",
    "        dataset_portion_negatives=0.5,  # TODO consider changing\n",
    "        datset_batch_size=64,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07152691578814216"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save_dataset_obj(data, \"dataset-full\")\n",
    "_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Inference pipeline\n",
    "* Once a model is fully trained, the weights (model state) is stored to disk.\n",
    "* This is again loaded in this section and we do as follows:\n",
    "* for each unique (encoded) customer id:\n",
    "  * for each (batch of encoded) article ids:\n",
    "    * make prediction, and store value\n",
    "  * find top $k$ article IDs - i.e. those with highest value\n",
    "* compute MAP for all customers.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning best predicted values back to true IDs ...\n",
      "Reading ground truth ...\n",
      "Index(['index', 0], dtype='object')\n",
      "Value counts before removing article duplicates\n",
      " {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}\n",
      "Value counts after removing article duplicates\n",
      " {7: 399, 6: 379, 4: 378, 10: 377, 9: 368, 11: 368, 3: 365, 5: 358, 15: 357, 16: 357, 8: 357, 17: 355, 13: 344, 14: 338, 20: 337, 18: 335, 22: 332, 12: 329, 19: 327, 21: 322, 29: 321, 23: 318, 28: 316, 31: 312, 33: 308, 30: 306, 2: 304, 25: 299, 27: 298, 34: 296, 32: 293, 26: 278, 41: 278, 37: 277, 38: 276, 35: 271, 42: 270, 44: 265, 24: 264, 36: 260, 39: 250, 40: 245, 46: 228, 47: 225, 48: 223, 51: 220, 57: 218, 43: 218, 49: 214, 1: 212, 55: 211, 45: 211, 56: 208, 50: 206, 54: 205, 58: 202, 59: 198, 61: 194, 52: 192, 53: 192, 62: 186, 64: 185, 60: 176, 63: 176, 66: 175, 65: 165, 67: 165, 69: 159, 68: 158, 79: 158, 70: 157, 71: 155, 78: 151, 72: 148, 74: 145, 76: 141, 75: 139, 82: 137, 89: 131, 73: 131, 88: 130, 81: 125, 87: 122, 77: 121, 83: 119, 85: 119, 84: 119, 93: 116, 92: 115, 90: 110, 94: 110, 86: 108, 91: 108, 80: 106, 104: 103, 105: 99, 102: 97, 95: 97, 113: 95, 98: 92, 116: 90, 107: 87, 100: 86, 97: 85, 117: 84, 96: 82, 101: 81, 120: 81, 108: 81, 103: 80, 109: 80, 115: 79, 111: 77, 99: 75, 114: 75, 119: 74, 106: 73, 124: 73, 126: 71, 110: 71, 122: 67, 118: 66, 112: 66, 123: 65, 125: 64, 129: 63, 133: 63, 130: 61, 137: 60, 145: 59, 131: 56, 143: 56, 127: 56, 147: 55, 121: 55, 134: 54, 136: 53, 135: 53, 132: 53, 128: 50, 154: 49, 141: 49, 140: 47, 149: 46, 142: 46, 150: 46, 139: 46, 148: 45, 138: 44, 153: 44, 144: 42, 158: 40, 146: 40, 163: 39, 162: 38, 170: 37, 155: 36, 159: 36, 187: 36, 176: 36, 171: 36, 166: 35, 157: 35, 164: 35, 169: 35, 160: 34, 151: 34, 188: 34, 167: 34, 174: 33, 165: 32, 177: 32, 161: 31, 152: 31, 173: 30, 182: 30, 156: 29, 168: 28, 172: 28, 175: 27, 200: 27, 211: 27, 191: 26, 184: 26, 198: 26, 185: 25, 183: 25, 186: 25, 190: 25, 217: 24, 195: 24, 199: 24, 194: 24, 179: 23, 223: 23, 178: 23, 201: 22, 192: 22, 208: 21, 222: 21, 181: 21, 216: 21, 203: 20, 206: 20, 202: 20, 210: 20, 224: 20, 212: 20, 193: 19, 221: 18, 213: 18, 236: 18, 220: 18, 189: 18, 209: 17, 247: 17, 197: 17, 218: 17, 180: 17, 207: 16, 205: 16, 280: 16, 196: 16, 233: 16, 204: 15, 238: 15, 232: 15, 219: 15, 227: 14, 226: 14, 215: 14, 228: 14, 239: 14, 242: 14, 231: 13, 244: 13, 251: 13, 272: 13, 230: 13, 252: 12, 261: 12, 273: 12, 214: 12, 246: 12, 245: 12, 250: 12, 262: 12, 258: 11, 234: 11, 249: 11, 257: 11, 240: 11, 229: 11, 235: 11, 259: 10, 304: 10, 243: 10, 270: 10, 287: 10, 237: 9, 256: 9, 290: 9, 266: 9, 255: 9, 265: 9, 269: 9, 278: 9, 248: 9, 241: 9, 320: 8, 310: 8, 322: 8, 254: 8, 288: 8, 299: 8, 268: 8, 289: 8, 292: 8, 260: 7, 305: 7, 284: 7, 308: 7, 315: 7, 277: 7, 264: 6, 225: 6, 276: 6, 384: 6, 357: 6, 291: 6, 294: 6, 253: 6, 335: 6, 347: 6, 319: 6, 285: 5, 326: 5, 309: 5, 314: 5, 331: 5, 307: 5, 293: 5, 271: 5, 339: 5, 327: 5, 354: 5, 318: 5, 267: 5, 311: 5, 283: 5, 336: 5, 302: 5, 387: 5, 346: 5, 342: 5, 448: 5, 296: 4, 351: 4, 321: 4, 279: 4, 353: 4, 348: 4, 330: 4, 374: 4, 352: 4, 301: 4, 341: 4, 338: 4, 375: 4, 349: 4, 431: 4, 389: 4, 378: 4, 458: 4, 281: 4, 372: 4, 306: 4, 365: 4, 275: 4, 274: 4, 286: 3, 360: 3, 298: 3, 340: 3, 437: 3, 399: 3, 396: 3, 461: 3, 364: 3, 541: 3, 300: 3, 303: 3, 297: 3, 358: 3, 337: 3, 367: 3, 328: 3, 332: 3, 313: 3, 363: 3, 350: 3, 395: 3, 263: 3, 317: 3, 361: 2, 377: 2, 295: 2, 333: 2, 436: 2, 490: 2, 373: 2, 403: 2, 765: 2, 386: 2, 510: 2, 435: 2, 282: 2, 427: 2, 443: 2, 406: 2, 323: 2, 470: 2, 450: 2, 482: 2, 324: 2, 492: 2, 420: 2, 426: 2, 404: 2, 600: 2, 376: 2, 391: 2, 523: 2, 700: 2, 371: 2, 381: 2, 505: 2, 613: 2, 343: 2, 423: 2, 345: 2, 382: 2, 583: 2, 383: 2, 392: 2, 356: 2, 430: 2, 499: 2, 425: 2, 440: 2, 370: 2, 394: 2, 485: 2, 530: 2, 329: 2, 393: 1, 558: 1, 452: 1, 874: 1, 906: 1, 447: 1, 334: 1, 554: 1, 522: 1, 527: 1, 590: 1, 718: 1, 411: 1, 468: 1, 596: 1, 566: 1, 495: 1, 504: 1, 359: 1, 402: 1, 677: 1, 690: 1, 539: 1, 616: 1, 424: 1, 639: 1, 719: 1, 626: 1, 466: 1, 428: 1, 464: 1, 388: 1, 580: 1, 836: 1, 312: 1, 1346: 1, 514: 1, 419: 1, 618: 1, 368: 1, 577: 1, 385: 1, 465: 1, 408: 1, 512: 1, 444: 1, 442: 1, 591: 1, 526: 1, 400: 1, 454: 1, 473: 1, 362: 1, 507: 1, 410: 1, 316: 1, 1088: 1, 642: 1, 686: 1, 516: 1, 432: 1, 479: 1, 418: 1, 564: 1, 476: 1, 538: 1, 415: 1, 834: 1, 379: 1, 446: 1, 601: 1, 455: 1, 494: 1, 422: 1, 413: 1, 559: 1, 972: 1, 679: 1, 674: 1, 441: 1, 723: 1, 439: 1, 369: 1, 708: 1, 486: 1, 545: 1, 453: 1, 840: 1, 401: 1, 710: 1, 456: 1, 467: 1, 397: 1, 506: 1, 542: 1, 462: 1, 429: 1, 807: 1, 421: 1, 491: 1, 552: 1}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def inference_alternative(model_path, test_data, k: int = 12) -> Tuple[dict, object]:\n",
    "    \"\"\"Make dictionary on form {customer: [art_1, ..., art_k]} for k highest predicted articles,\n",
    "    for each customer and article in the *validation set*\"\"\"\n",
    "\n",
    "    def _update_out_dict(all_preds: defaultdict):\n",
    "        \"\"\"Helper to add predictions to out_dict\"\"\"\n",
    "        for customer_dict in all_preds:\n",
    "            k_i = min(k, len(all_preds[customer_dict]))\n",
    "            keys_i = np.array(list(all_preds[customer_dict].keys()))\n",
    "            values_i = list(all_preds[customer_dict].values())\n",
    "            top_ind = np.argpartition(values_i, -k_i)[-k_i:]\n",
    "            out_dict[customer_dict] = [keys_i[top_ind].tolist()]\n",
    "\n",
    "    # Initialize model and dataset\n",
    "    num_customer, num_articles = test_data.df_id.nunique().values[:2]\n",
    "    all_preds = defaultdict(dict)\n",
    "    # Best preds now just finds the label-encoded stuff since we already have LF on those\n",
    "    device = \"cpu\"\n",
    "    model = HM_model(num_customer, num_articles, embedding_size=500).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n",
    "    model.eval()\n",
    "\n",
    "    # Compute predictions\n",
    "    data = test_data.get_data_from_subset(test_data.val).to_numpy()\n",
    "    valid_customers, valid_articles = np.unique(data[:, 0]), np.unique(\n",
    "        data[:, 1]\n",
    "    )  # TODO valid cust should np.unique(data[:, 0])\n",
    "    valid_customers, valid_articles = torch.IntTensor(valid_customers).to(\n",
    "        device\n",
    "    ), torch.IntTensor(valid_articles).to(device)\n",
    "    from math import ceil\n",
    "\n",
    "    valid_articles = np.array_split(\n",
    "        valid_articles, ceil(valid_articles.shape[0] / test_data.batch_size)\n",
    "    )\n",
    "    out_dict = dict()\n",
    "    print(\"Computing predictions for customers ...\")\n",
    "    customer_count = 0\n",
    "    for customer in tqdm(valid_customers):\n",
    "        customer_count += 1\n",
    "        for article in valid_articles:\n",
    "            customer_exp = customer.expand(article.shape[0])\n",
    "            pred = model(customer_exp, article).view(-1)\n",
    "            for i, pred_i in enumerate(pred):\n",
    "                all_preds[customer.item()][article[i]] = pred_i\n",
    "        if customer_count % 100 == 0:\n",
    "            # print(\"Sending batch to out dict...\")\n",
    "            _update_out_dict(all_preds)\n",
    "            # Free memory and re-initiate defaultdict\n",
    "            del all_preds\n",
    "            all_preds = defaultdict(dict)\n",
    "\n",
    "    # Call this once more for the last n<100 customers\n",
    "    _update_out_dict(all_preds)\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def compute_map(\n",
    "    best_preds: dict,\n",
    "    test_data: Data_HM,\n",
    "    k: int = 12,\n",
    "    use_all_data_as_ground_truth: bool = False,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    vprint = lambda *args: print(*args) if verbose else None\n",
    "    if use_all_data_as_ground_truth:\n",
    "        vprint(\"Returning best predicted values back to true IDs ...\")\n",
    "        decoded_preds = dict()\n",
    "        for k, v in best_preds.items():\n",
    "            unenc_k = test_data.le_cust.inverse_transform([k])[0]\n",
    "            unenc_v = test_data.le_art.inverse_transform(v[0])\n",
    "            decoded_preds[unenc_k] = [unenc_v]\n",
    "        best_preds = decoded_preds  # Overwrites best_preds\n",
    "\n",
    "        # Ground truth from entire database:\n",
    "        vprint(\"Reading ground truth ...\")\n",
    "        ground_truth = pd.read_csv(\n",
    "            \"dataset/transactions_train.csv\",\n",
    "            dtype={\"article_id\": str},\n",
    "            usecols=[\"customer_id\", \"article_id\"],\n",
    "        )\n",
    "        ground_truth = ground_truth[\n",
    "            ground_truth[\"customer_id\"].isin(list(decoded_preds.keys()))\n",
    "        ]\n",
    "        ground_truth = (\n",
    "            ground_truth.groupby(\"customer_id\").agg({\"article_id\": list}).reset_index()\n",
    "        )\n",
    "    else:\n",
    "        vprint(\"Loading only data in validation set\")\n",
    "        ground_truth = (  # This only checks the true values of the validation set\n",
    "            test_data.df_id[test_data.df_id[\"label\"] == 1]\n",
    "            .groupby(\"customer_id\")\n",
    "            .agg({\"article_id\": list})\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "    preds = pd.DataFrame.from_dict(best_preds, orient=\"index\").reset_index()\n",
    "    preds.columns = [\"customer_id\", \"est_article_id\"]\n",
    "    vprint(\n",
    "        \"Value counts before removing article duplicates\\n\",\n",
    "        ground_truth[\"article_id\"].apply(len).value_counts().to_dict(),\n",
    "    )\n",
    "    # Remove duplicates\n",
    "    ground_truth[\"article_id\"] = ground_truth[\"article_id\"].apply(\n",
    "        lambda c: list(set(c))\n",
    "    )\n",
    "    vprint(\n",
    "        \"Value counts after removing article duplicates\\n\",\n",
    "        ground_truth[\"article_id\"].apply(len).value_counts().to_dict(),\n",
    "    )\n",
    "\n",
    "    merged = ground_truth.merge(preds)\n",
    "    from utils.metrics import prec, rel\n",
    "\n",
    "    average_precision = merged.apply(\n",
    "        lambda x: sum(\n",
    "            [\n",
    "                prec(i, x.est_article_id, x.article_id)\n",
    "                * rel(i, x.est_article_id, x.article_id)\n",
    "                for i in range(1, k + 1)\n",
    "            ]\n",
    "        )\n",
    "        / min(k, len(x.est_article_id)),\n",
    "        axis=1,\n",
    "    )\n",
    "    map_score = average_precision.mean()\n",
    "    return average_precision  # map_score\n",
    "\n",
    "\n",
    "def wrapper_old():\n",
    "    @torch.inference_mode()\n",
    "    def inference(user_id, model, test_data, k: int = 12):\n",
    "        \"\"\"Based on a specific user id and test data, compute the articles with best score\"\"\"\n",
    "        preds = []\n",
    "        labels = []\n",
    "        articles = []\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        for item in test_data.get_DataLoader(trainDL=False):\n",
    "            item = tuple(t.to(device) for t in item)\n",
    "            row, label = item\n",
    "            pred = model(\n",
    "                torch.IntTensor([user_id]).to(device).expand(row[:, 1].shape[0]),\n",
    "                row[:, 1],\n",
    "            ).view(-1)\n",
    "            preds.extend(pred.cpu().detach().numpy())\n",
    "            labels.extend(label.cpu().detach().numpy())\n",
    "            articles.extend(row[:, 1].cpu().detach().numpy())\n",
    "        preds = np.array(preds)\n",
    "        labels = np.array(labels)\n",
    "        topk_ind = np.argpartition(np.array(preds), k)[\n",
    "            :k\n",
    "        ]  # k best predictions for user\n",
    "        topk_articles_enc = np.array(articles)[topk_ind]\n",
    "        # Transform articles back to original shape\n",
    "        topk_articles = test_data.le_art.inverse_transform(topk_articles_enc)\n",
    "        return topk_articles\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def make_submission(model_path, test_data, k: int = 12):\n",
    "        # Assume test_data is HM_Data object\n",
    "        best_preds = {}\n",
    "        num_customer, num_articles = test_data.df_id.nunique().values[:2]\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model = HM_model(num_customer, num_articles, embedding_size=500).to(device)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n",
    "        model.eval()  # Superfluous but idc\n",
    "        for user_id in tqdm(test_data.df_id[\"customer_id\"].unique()):\n",
    "            user = test_data.le_cust.inverse_transform([user_id])\n",
    "            # print(user)\n",
    "            best_preds[user[0]] = [inference(user_id, model, test_data, k)]\n",
    "        import pickle\n",
    "\n",
    "        # with open(\"tmp.obj\", \"wb\") as f:\n",
    "        #     pickle.dump(best_preds, f)\n",
    "\n",
    "        # print(best_preds)\n",
    "        preds = pd.DataFrame.from_dict(best_preds, orient=\"index\")\n",
    "        preds.reset_index(inplace=True)\n",
    "        preds.columns = [\"customer_id\", \"est_article_id\"]\n",
    "\n",
    "        ground_truth = (\n",
    "            test_data.df_id.groupby(\"customer_id\")\n",
    "            .agg({\"article_id\": list})\n",
    "            .reset_index()\n",
    "        )\n",
    "        # Remove duplicates and transform to true IDs\n",
    "        ground_truth[\"article_id\"] = ground_truth[\"article_id\"].apply(\n",
    "            lambda c: test_data.le_art.inverse_transform(list(set(c)))\n",
    "        )\n",
    "        ground_truth[\"customer_id\"] = [\n",
    "            test_data.le_cust.inverse_transform([cust])[0]\n",
    "            for cust in ground_truth[\"customer_id\"]\n",
    "        ]\n",
    "        all = ground_truth.merge(preds)\n",
    "        from utils.metrics import prec, rel\n",
    "\n",
    "        average_precision = all.apply(\n",
    "            lambda x: sum(\n",
    "                [\n",
    "                    prec(i, x.est_article_id, x.article_id)\n",
    "                    * rel(i, x.est_article_id, x.article_id)\n",
    "                    for i in range(1, k + 1)\n",
    "                ]\n",
    "            )\n",
    "            / min(k, len(x.est_article_id)),\n",
    "            axis=1,\n",
    "        )\n",
    "        map_score = average_precision.mean()\n",
    "        return average_precision\n",
    "\n",
    "\n",
    "# This is the dataset from 100k samples\n",
    "data_test = read_dataset_obj(\"object_storage/dataset-2022.11.09.19.34.pckl\")\n",
    "\n",
    "# best_preds = inference_alternative(\"models/2022.11.09.19.36.pth\", data_test)\n",
    "# save_dataset_obj(best_preds, \"object_storage/predictions.pckl\")\n",
    "check_alt_inference = compute_map(\n",
    "    best_preds, data_test, use_all_data_as_ground_truth=True, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00014042874054137264"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best_preds\n",
    "res = compute_map(best_preds, data_test)\n",
    "np.mean(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_test_set_file():\n",
    "    import numpy as np, pandas as pd, torch\n",
    "    full_set = pd.read_csv(\"dataset/transactions_train.csv\", dtype={\"article_id\": str})\n",
    "    num_train = int(len(full_set)*0.7)\n",
    "    num_test = len(full_set) - num_train\n",
    "    test_idx = np.random.randint(0, len(full_set),size=num_test)\n",
    "    full_set.iloc[test_idx].to_csv(\"dataset/tr_test.csv\")\n",
    "    full_set.drop(test_idx).to_csv(\"dataset/tr_train.csv\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04a4c663c1c9b43728e49bf64ea34e4d1fe986a9327a4efda5dcdc8b739aba35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
