{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring modifications to embedding model\n",
    "\n",
    "We start by calling necessary dataset functions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, pickle, numpy as np, pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import Any, Tuple\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "\n",
    "# Data_HM definition has to be here for pickle to understand what object it loads in\n",
    "class Data_HM(Dataset):\n",
    "    \"\"\"This is the general HM Dataset class whose children are train-dataset and validation-dataset\n",
    "    no\n",
    "\n",
    "    Args:\n",
    "        Dataset: Abstract Dataset class from pyTorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        total_cases: int,\n",
    "        portion_negatives: float,\n",
    "        df_transactions: pd.DataFrame,\n",
    "        df_articles: pd.DataFrame,\n",
    "        df_customers: pd.DataFrame,\n",
    "        batch_size: int,\n",
    "        train_portion: float | None = None,\n",
    "        test_portion: float | None = None,\n",
    "        transform: Any = None,\n",
    "        target_transform: Any = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if train_portion is None:\n",
    "            if test_portion is None:\n",
    "                raise ValueError(\"Both train portion and test portion cannot be None.\")\n",
    "            self.train_portion = 1 - test_portion\n",
    "        self.batch_size = batch_size\n",
    "        self.df_id = self.generate_dataset(\n",
    "            total_cases, portion_negatives, df_transactions\n",
    "        )\n",
    "        self.train_portion = train_portion\n",
    "        self.train, self.val = self.split_dataset()\n",
    "        self.transform, self.target_transform = transform, target_transform\n",
    "\n",
    "    def generate_dataset(\n",
    "        self, total_cases: int, portion_negatives: float, df_transactions: pd.DataFrame\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Produce DataFrames for positive labels and generated negative samples\n",
    "\n",
    "        Args:\n",
    "            total_cases (int): Total number of transactions\n",
    "            portion_negatives (float): The portion of the `total_cases` that should be negative. Balanced 0/1 when 0.5\n",
    "            df_transactions (pd.DataFrame): Transactions to pull samples/generate samples from\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: _description_\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            0 <= portion_negatives <= 1\n",
    "        ), r\"portion negatives must be a float between 0%=0.0 and 100%=1.0!\"\n",
    "        n_positive = round(total_cases * (1 - portion_negatives))\n",
    "        n_negative = total_cases - n_positive\n",
    "\n",
    "        df_positive = df_transactions.sample(n=n_positive).reset_index(drop=True)\n",
    "        df_positive = df_positive[[\"customer_id\", \"article_id\"]]\n",
    "        df_positive[\"label\"] = 1\n",
    "\n",
    "        # Sampling negative labels:\n",
    "        #   We select a random combination of `customer_id`, `article_id`, and ensure that this is not a true transaction.\n",
    "        #   Then we make a 2-column dataframe on same form as `df_positive`\n",
    "\n",
    "        df_np = df_transactions[[\"customer_id\", \"article_id\"]].to_numpy()\n",
    "        neg_np = np.empty((n_negative, df_np.shape[1]), dtype=\"<U64\")\n",
    "        for i in range(n_negative):\n",
    "            legit = False\n",
    "            while not legit:\n",
    "                sample = [\n",
    "                    np.random.choice(df_np[:, col]) for col in range(df_np.shape[1])\n",
    "                ]\n",
    "                legit = not (\n",
    "                    (df_np[:, 0] == sample[0]) & (df_np[:, 1] == sample[1])\n",
    "                ).any()\n",
    "            neg_np[i, :] = sample\n",
    "        neg_np = np.column_stack((neg_np, [0] * neg_np.shape[0]))\n",
    "        df_negative = pd.DataFrame(neg_np, columns=df_positive.columns)\n",
    "        # Return a shuffled concatenation of the two dataframes\n",
    "        full_data = (\n",
    "            pd.concat((df_positive, df_negative)).sample(frac=1).reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        # Make label encodings of the IDs\n",
    "        le_cust = LabelEncoder()\n",
    "        le_art = LabelEncoder()\n",
    "        le_cust.fit(full_data[\"customer_id\"])\n",
    "        le_art.fit(full_data[\"article_id\"])\n",
    "        cust_encode = le_cust.transform(full_data[\"customer_id\"])\n",
    "        art_encode = le_art.transform(full_data[\"article_id\"])\n",
    "        return pd.DataFrame(\n",
    "            data={\n",
    "                \"customer_id\": cust_encode,\n",
    "                \"article_id\": art_encode,\n",
    "                \"label\": full_data[\"label\"].astype(np.uint8),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_id.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row, label = self.df_id.iloc[idx, :-1].values, self.df_id.iloc[idx, -1]\n",
    "        label = int(label)  # Stored as str initially\n",
    "        if self.transform:\n",
    "            row = self.transform(row)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return row, label\n",
    "\n",
    "    def split_dataset(self):\n",
    "        \"\"\"Split full data to train and validation Subset-objects\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Subset, Subset]: Train and validation subsets\n",
    "        \"\"\"\n",
    "        length = len(self)\n",
    "        train_size = int(length * self.train_portion)\n",
    "        valid_size = length - train_size\n",
    "        train, val = torch.utils.data.random_split(self, [train_size, valid_size])\n",
    "        return train, val\n",
    "\n",
    "    def get_data_from_subset(self, subset: torch.utils.data.Subset):\n",
    "        \"\"\"Not in use currently, but can retrieve data from Subset object directly\"\"\"\n",
    "        return subset.dataset.df_id.iloc[subset.indices]\n",
    "\n",
    "    def get_DataLoader(self, trainDL: bool = True):\n",
    "        subset = self.train if trainDL else self.val\n",
    "        return DataLoader(dataset=subset, batch_size=self.batch_size)\n",
    "\n",
    "def read_dataset_obj(src: str) -> Any:\n",
    "\n",
    "    with open(src, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load things:)\n",
    "dataset = read_dataset_obj(\"object_storage/dataset-2022.11.09.19.34.pckl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other model formulation\n",
    "class HM_neural(torch.nn.Module):\n",
    "    def __init__(self, num_customer, num_articles, embedding_size, bias_nodes: bool, n_activations: int) -> None:\n",
    "        super().__init__()\n",
    "        self.customer_embed = torch.nn.Embedding(\n",
    "            num_embeddings=num_customer, embedding_dim=embedding_size\n",
    "        )\n",
    "        self.article_embed = torch.nn.Embedding(\n",
    "            num_embeddings=num_articles, embedding_dim=embedding_size\n",
    "        )\n",
    "        if bias_nodes:\n",
    "            self.customer_bias = torch.nn.Embedding(num_customer, 1)\n",
    "            self.article_bias = torch.nn.Embedding(num_articles, 1)\n",
    "        else:\n",
    "            # They're added lienarly so this should give no effect\n",
    "            self.customer_bias = lambda row: 0\n",
    "            self.article_bias = lambda row: 0\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            # 3 features: age (customer), index_group and garment_group (article)\n",
    "            # n_activations deaults to e.g. 100?\n",
    "            torch.nn.Linear(num_articles*2 + num_customer, n_activations),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(n_activations, 1)\n",
    "        )\n",
    "    def forward(self, customer, article, garment_group, index_group, age):\n",
    "        embeddings = self.customer_embed(customer), self.article_embed(article)\n",
    "        biases = self.customer_bias(customer), self.article_bias(article)\n",
    "        article_info = index_group, garment_group\n",
    "        customer_info = age\n",
    "        x = torch.prod(embeddings) + torch.sum(biases)\n",
    "        x = x + self.layers(torch.cat(*article_info, customer_info))\n",
    "        return torch.sigmoid(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04a4c663c1c9b43728e49bf64ea34e4d1fe986a9327a4efda5dcdc8b739aba35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
